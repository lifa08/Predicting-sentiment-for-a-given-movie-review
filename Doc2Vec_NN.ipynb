{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import locale\n",
    "import glob\n",
    "import os.path\n",
    "import requests\n",
    "import tarfile\n",
    "import sys\n",
    "import codecs\n",
    "import smart_open\n",
    "import time\n",
    "from collections import namedtuple\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "import multiprocessing\n",
    "from random import shuffle\n",
    "import theano.tensor as tensor\n",
    "from theano import config\n",
    "import numpy\n",
    "import theano\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert text to lower-case and strip punctuation/symbols from words\n",
    "def normalize_text(text):\n",
    "    norm_text = text.lower()\n",
    "    # Replace breaks with spaces\n",
    "    norm_text = norm_text.replace('<br />', ' ')\n",
    "    # Pad punctuation with spaces on both sides\n",
    "    for char in [':', '\"', ',', '(', ')', '!', '?', ';', '*']:\n",
    "        norm_text = norm_text.replace(char, ' ')\n",
    "    norm_text = norm_text.replace('.', ' ' + '.' + ' ')\n",
    "    return norm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_files(dirname):\n",
    "    if sys.version > '3':\n",
    "        control_chars = [chr(0x85)]\n",
    "    else:\n",
    "        control_chars = [unichr(0x85)]\n",
    "\n",
    "    folders = ['train/pos', 'train/neg', 'test/pos', 'test/neg']\n",
    "    alldata = u''\n",
    "\n",
    "    for fol in folders:\n",
    "        temp = u''\n",
    "        output = fol.replace('/', '-') + '.txt'\n",
    "        txt_files = glob.glob(os.path.join(dirname, fol, '*.txt')) # get all text files\n",
    "        for txt in txt_files:\n",
    "            with smart_open.smart_open(txt, \"rb\") as t:\n",
    "                t_clean = t.read().decode(\"utf-8\")\n",
    "                for c in control_chars:\n",
    "                    t_clean = t_clean.replace(c, ' ')\n",
    "                temp += t_clean\n",
    "            temp += \"\\n\"\n",
    "        temp_norm = normalize_text(temp)\n",
    "        with smart_open.smart_open(os.path.join(dirname, output), \"wb\") as n:\n",
    "            n.write(temp_norm.encode(\"utf-8\"))\n",
    "        alldata += temp_norm\n",
    "\n",
    "    with smart_open.smart_open(os.path.join(dirname, 'alldata-id.txt'), 'wb') as f:\n",
    "        for idx, line in enumerate(alldata.splitlines()):\n",
    "            num_line = u\"_*{0} {1}\\n\".format(idx, line)\n",
    "            f.write(num_line.encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentences(alldata_filename):\n",
    "    SentimentDocument = namedtuple('SentimentDocument', 'words tags split sentiment')\n",
    "    alldocs = []  # Will hold all docs in original order\n",
    "\n",
    "    with open(alldata_filename, encoding='utf-8') as alldata:\n",
    "        for line_no, line in enumerate(alldata):\n",
    "            tokens = gensim.utils.to_unicode(line).split()\n",
    "            words = tokens[1:]\n",
    "            tags = [line_no]\n",
    "            split = ['train', 'test'][line_no//25000]  # 25k train, 25k test\n",
    "            sentiment = [1.0, 0.0, 1.0, 0.0][line_no//12500] #[12.5K pos, 12.5K neg]*2\n",
    "            alldocs.append(SentimentDocument(words, tags, split, sentiment))\n",
    "\n",
    "    train_docs = [doc for doc in alldocs if doc.split == 'train']\n",
    "    test_docs = [doc for doc in alldocs if doc.split == 'test']\n",
    "    doc_list = alldocs[:]  # For reshuffling per pass\n",
    "    return (train_docs, test_docs, doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_doc2vec(all_docs, num_epoch=10, size=128):\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "\n",
    "    model = Doc2Vec(dm=1, dm_concat=1, size=size,\n",
    "                window=5, negative=5, hs=0, min_count=1, workers=cores, sample=1e-4)\n",
    "    model.build_vocab(all_docs)\n",
    "\n",
    "    alpha, min_alpha = (0.025, 0.001)\n",
    "    alpha_delta = (alpha - min_alpha) / num_epoch\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        shuffle(all_docs)\n",
    "        model.alpha, model.min_alpha = alpha, alpha\n",
    "        model.train(all_docs, total_examples=len(all_docs), epochs=1)\n",
    "        alpha -= alpha_delta\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# neural network classifier\n",
    "class NN_classifier(object):\n",
    "    def __init__(self, rng, input, n_in, n_out, U=None, b=None):\n",
    "        self.input = input\n",
    "\n",
    "        if U is None:\n",
    "            U_values = 0.01 * rng.randn(n_in, n_out).astype(config.floatX)\n",
    "            U = theano.shared(value=U_values, name='U', borrow=True)\n",
    "\n",
    "        if b is None:\n",
    "            b_values = numpy.zeros((n_out,)).astype(config.floatX)\n",
    "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
    "\n",
    "        self.U = U\n",
    "        self.b = b\n",
    "\n",
    "        self.pred = tensor.nnet.softmax(tensor.dot(input, self.U) + self.b)\n",
    "\n",
    "        self.y_pred = self.pred.argmax(axis=1)\n",
    "\n",
    "        self.weight_decay = (self.U ** 2).sum()\n",
    "\n",
    "        self.params = [self.U, self.b]\n",
    "\n",
    "    def cost(self, y):\n",
    "        off = 1e-8\n",
    "        if self.pred.dtype == 'float16':\n",
    "            off = 1e-6\n",
    "        return -tensor.log(self.pred[tensor.arange(y.shape[0]), y] + off).mean()\n",
    "\n",
    "    def errors(self, y):\n",
    "        if y.ndim != self.y_pred.ndim:\n",
    "            raise TypeError(\n",
    "                'y should have the same shape as self.y_pred',\n",
    "                ('y', y.type, 'y_pred', self.y_pred.type)\n",
    "            )\n",
    "        if y.dtype.startswith('int'):\n",
    "            return tensor.mean(tensor.neq(self.y_pred, y))\n",
    "        else:\n",
    "            raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(model, train_set, test_set, valid_portion=0.1, test_size=0):\n",
    "    train_set_y, train_set_x = zip(*[(doc.sentiment, model.docvecs[doc.tags[0]])\n",
    "                                            for doc in train_set])\n",
    "\n",
    "    n_samples = len(train_set_x)\n",
    "    sidx = numpy.random.permutation(n_samples)\n",
    "    n_train = int(numpy.round(n_samples * (1. - valid_portion)))\n",
    "\n",
    "    valid_x = [train_set_x[s] for s in sidx[n_train:]]\n",
    "    valid_y = [train_set_y[s] for s in sidx[n_train:]]\n",
    "\n",
    "    train_x = [train_set_x[s] for s in sidx[:n_train]]\n",
    "    train_y = [train_set_y[s] for s in sidx[:n_train]]\n",
    "\n",
    "    test_set_y, test_set_x = zip(*[(doc.sentiment, model.docvecs[doc.tags[0]])\n",
    "                                            for doc in test_set])\n",
    "    \n",
    "    if test_size > 0:\n",
    "        idx = numpy.arange(len(test_set_x))\n",
    "        numpy.random.shuffle(idx)\n",
    "        idx = idx[:test_size]\n",
    "        test = ([test_set_x[n] for n in idx], [test_set_y[n] for n in idx])\n",
    "\n",
    "    def shared_dataset(data_x, data_y, borrow=True):\n",
    "        shared_x = theano.shared(numpy.asarray(data_x,\n",
    "                                               dtype=theano.config.floatX),\n",
    "                                 borrow=borrow)\n",
    "        shared_y = theano.shared(numpy.asarray(data_y,\n",
    "                                               dtype=theano.config.floatX),\n",
    "                                 borrow=borrow)\n",
    "        return shared_x, tensor.cast(shared_y, 'int32')\n",
    "\n",
    "    test_set_x, test_set_y = shared_dataset(test[0], test[1])\n",
    "    valid_set_x, valid_set_y = shared_dataset(valid_x, valid_y)\n",
    "    train_set_x, train_set_y = shared_dataset(train_x, train_y)\n",
    "\n",
    "    rval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y),\n",
    "            (test_set_x, test_set_y)]\n",
    "\n",
    "    return rval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NN_classifier_optimize(doc2vec_model, train_set, test_set,\n",
    "                           test_size=5000, batch_size=16, \n",
    "                           decay_c=0.001, learning_rate=0.001,\n",
    "                           valid_portion=0.1, max_epoch=100,\n",
    "                           patience=50, validFreq=-1):\n",
    "\n",
    "    train, valid, test = load_dataset(doc2vec_model, train_set, test_set,\n",
    "                                      test_size=test_size, valid_portion=valid_portion)\n",
    "\n",
    "    train_x, train_y = train\n",
    "    valid_x, valid_y = valid\n",
    "    test_x, test_y = test\n",
    "    print(train_x.shape.eval())\n",
    "\n",
    "    n_train_batches = train_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    n_valid_batches = valid_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    n_test_batches = test_x.get_value(borrow=True).shape[0] // batch_size\n",
    "\n",
    "    index = tensor.iscalar()\n",
    "    x = tensor.matrix('x')\n",
    "    y = tensor.ivector('y')\n",
    "\n",
    "    rng = numpy.random.RandomState(1234)\n",
    "    n_in = train_x.get_value(borrow=True).shape[1]\n",
    "    classifier = NN_classifier(rng= rng, input=x, n_in=n_in, n_out=2)\n",
    "\n",
    "    test_model = theano.function(\n",
    "        inputs = [index],\n",
    "        outputs = classifier.errors(y),\n",
    "        givens = {\n",
    "            x: test_x[index * batch_size:(index + 1) * batch_size],\n",
    "            y: test_y[index * batch_size:(index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    validate_model = theano.function(\n",
    "        inputs = [index],\n",
    "        outputs = classifier.errors(y),\n",
    "        givens = {\n",
    "            x: valid_x[index * batch_size:(index + 1) * batch_size],\n",
    "            y: valid_y[index * batch_size:(index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    cost = classifier.cost(y)\n",
    "    if decay_c > 0.:\n",
    "        decay_c = theano.shared(numpy.asarray(decay_c, dtype=config.floatX), name='decay_c')\n",
    "        classifier.weight_decay *= decay_c\n",
    "        cost += classifier.weight_decay\n",
    "\n",
    "    g_U = tensor.grad(cost=cost, wrt=classifier.U)\n",
    "    g_b = tensor.grad(cost=cost, wrt=classifier.b)\n",
    "\n",
    "    updates = [(classifier.U, classifier.U - learning_rate * g_U),\n",
    "               (classifier.b, classifier.b - learning_rate * g_b)]\n",
    "\n",
    "    train_model = theano.function(\n",
    "        inputs = [index],\n",
    "        outputs = cost,\n",
    "        updates = updates,\n",
    "        givens = {\n",
    "            x: train_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print('... training the model')\n",
    "    patience_increase = 2\n",
    "    improvement_threshold = 0.995\n",
    "\n",
    "    if validFreq == -1:\n",
    "        validFreq = n_train_batches\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    test_score = 0.\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    done_looping = False\n",
    "    epoch = 0\n",
    "    while (epoch < max_epoch) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "            if (iter + 1) % validFreq == 0:\n",
    "                validation_losses = [validate_model(i)\n",
    "                                     for i in range(n_valid_batches)]\n",
    "                this_validation_loss = numpy.mean(validation_losses)\n",
    "                print('epoch %i, minibatch %i/%i,validation error %f %%' %(epoch,\n",
    "                                                                           minibatch_index + 1,\n",
    "                                                                           n_train_batches,\n",
    "                                                                          this_validation_loss * 100.))\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "                    if this_validation_loss < best_validation_loss * improvement_threshold:\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "                    best_validation_loss = this_validation_loss\n",
    "\n",
    "                    test_losses = [test_model(i) for i in range(n_test_batches)]\n",
    "                    test_score = numpy.mean(test_losses)\n",
    "                    print(('     epoch %i, minibatch %i/%i, test error of'\n",
    "                                ' best model %f %%') %(epoch,\n",
    "                                                       minibatch_index + 1,\n",
    "                                                       n_train_batches,\n",
    "                                                       test_score * 100.))\n",
    "                if patience <= iter:\n",
    "                    one_looping = True\n",
    "                    break\n",
    "    end_time = timeit.default_timer()\n",
    "    print(('Optimization complete with best validation score of %f %%,'\n",
    "           'with test performance %f %%')%(best_validation_loss * 100.,\n",
    "                                           test_score * 100.))\n",
    "    print('The code run for %d epochs, with %f epochs/sec' % (\n",
    "            epoch, 1. * epoch / (end_time - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(reload=None, emb_size=128, doc2vec_epoch=20):\n",
    "    dirname = '../Miniproject_test/aclImdb'\n",
    "    alldata_file = os.path.join(dirname, 'alldata-id.txt')\n",
    "\n",
    "    if not os.path.isfile(alldata_file):\n",
    "        clean_files(dirname)\n",
    "\n",
    "    assert os.path.isfile(alldata_file), \"alldata-id.txt unavailable\"\n",
    "\n",
    "    train_set, test_set, all_docs = get_sentences(alldata_file)\n",
    "\n",
    "    if reload is not None:\n",
    "        model = Doc2Vec.load('./imdb.d2v')\n",
    "    else:\n",
    "        # train doc2vec\n",
    "        print('training doc2vec')\n",
    "        start_time = timeit.default_timer()\n",
    "        model = train_doc2vec(all_docs, num_epoch=doc2vec_epoch, size=emb_size)\n",
    "        model.save('./imdb.d2v')\n",
    "        end_time = timeit.default_timer()\n",
    "        print('doc2vec training took %f sec' %(end_time - start_time))\n",
    "\n",
    "    NN_classifier_optimize(doc2vec_model=model, train_set=train_set, test_set=test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training doc2vec\n",
      "doc2vec training took 612.669210 sec\n",
      "[22500   128]\n",
      "... training the model\n",
      "epoch 1, minibatch 1406/1406,validation error 43.950321 %\n",
      "     epoch 1, minibatch 1406/1406, test error of best model 43.129006 %\n",
      "epoch 2, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 3, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 4, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 5, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 6, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 7, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 8, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 9, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 10, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 11, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 12, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 13, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 14, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 15, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 16, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 17, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 18, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 19, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 20, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 21, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 22, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 23, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 24, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 25, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 26, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 27, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 28, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 29, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 30, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 31, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 32, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 33, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 34, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 35, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 36, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 37, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 38, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 39, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 40, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 41, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 42, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 43, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 44, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 45, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 46, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 47, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 48, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 49, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 50, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 51, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 52, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 53, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 54, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 55, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 56, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 57, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 58, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 59, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 60, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 61, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 62, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 63, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 64, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 65, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 66, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 67, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 68, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 69, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 70, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 71, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 72, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 73, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 74, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 75, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 76, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 77, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 78, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 79, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 80, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 81, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 82, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 83, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 84, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 85, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 86, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 87, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 88, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 89, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 90, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 91, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 92, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 93, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 94, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 95, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 96, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 97, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 98, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 99, minibatch 1406/1406,validation error 43.950321 %\n",
      "epoch 100, minibatch 1406/1406,validation error 43.950321 %\n",
      "Optimization complete with best validation score of 43.950321 %,with test performance 43.129006 %\n",
      "The code run for 100 epochs, with 79.245529 epochs/sec\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training doc2vec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-2436fc2ab63a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-124-985373dc9c4d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(reload)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training doc2vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_doc2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'doc2vec training took %f sec'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-86-de7bff964bfc>\u001b[0m in \u001b[0;36mtrain_doc2vec\u001b[0;34m(all_docs, num_epoch, size)\u001b[0m\n\u001b[1;32m      5\u001b[0m     model = Doc2Vec(dm=1, dm_concat=1, size=size,\n\u001b[1;32m      6\u001b[0m                 window=5, negative=5, hs=0, min_count=1, workers=cores, sample=1e-4)\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, keep_raw_vocab, trim_rule, progress_per, update)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# trim by min_count & precalculate downsampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# build tables & arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_vocab_from_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mfinalize_vocab\u001b[0;34m(self, update)\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0;31m# set initial input/projection and hidden weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mreset_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdm_tag_count\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"using concatenative %d-dimensional layer1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mreset_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m             \u001b[0;31m# construct deterministic seed from word AND seed argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseeded_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mseeded_vector\u001b[0;34m(self, seed_string)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \u001b[0;34m\"\"\"Create one 'random' vector (but deterministic by seed_string)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m         \u001b[0;31m# Note: built-in hash() may vary by Python version or even (in Py3.x) per launch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0monce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhashfxn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_string\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m0xffffffff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0monce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
