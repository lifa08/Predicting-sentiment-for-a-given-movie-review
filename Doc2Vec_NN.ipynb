{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: combine doc2vec with standard logistic regression classifier\n",
    "\n",
    "1. Train vector representations of documents with gensim.models.Doc2Vec\n",
    "\n",
    "2. Feed the document vectors to the standard logistic regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGUAAADLCAYAAAB3al1RAAAO/UlEQVR4nO2dX0wUV/vHn5v3cm/P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\nqTiKKJ/tAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"doc2vec.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import locale\n",
    "import glob\n",
    "import os.path\n",
    "import requests\n",
    "import tarfile\n",
    "import sys\n",
    "import codecs\n",
    "import smart_open\n",
    "import time\n",
    "from collections import namedtuple\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "import multiprocessing\n",
    "from random import shuffle\n",
    "import theano.tensor as tensor\n",
    "from theano import config\n",
    "import numpy\n",
    "import theano\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert text to lower-case and strip punctuation/symbols from words\n",
    "def normalize_text(text):\n",
    "    norm_text = text.lower()\n",
    "    # Replace breaks with spaces\n",
    "    norm_text = norm_text.replace('<br />', ' ')\n",
    "\n",
    "    # Pad punctuation with spaces on both sides\n",
    "    for char in [':', '\"', ',', ')', '!', '?', ';']:\n",
    "        norm_text = norm_text.replace(char, ' ' + char)\n",
    "\n",
    "    norm_text = norm_text.replace('(', '(' + ' ')\n",
    "    norm_text = norm_text.replace('.', ' ' + '.')\n",
    "    return norm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_files(dirname):\n",
    "    if sys.version > '3':\n",
    "        control_chars = [chr(0x85)]\n",
    "    else:\n",
    "        control_chars = [unichr(0x85)]\n",
    "\n",
    "    folders = ['train/pos', 'train/neg', 'test/pos', 'test/neg']\n",
    "    alldata = u''\n",
    "\n",
    "    for fol in folders:\n",
    "        temp = u''\n",
    "        output = fol.replace('/', '-') + '.txt'\n",
    "\n",
    "        txt_files = glob.glob(os.path.join(dirname, fol, '*.txt')) # get all text files\n",
    "        for txt in txt_files:\n",
    "            with smart_open.smart_open(txt, \"rb\") as t:\n",
    "                t_clean = t.read().decode(\"utf-8\")\n",
    "                for c in control_chars:\n",
    "                    t_clean = t_clean.replace(c, ' ')\n",
    "                temp += t_clean\n",
    "            temp += \"\\n\"\n",
    "        temp_norm = normalize_text(temp)\n",
    "\n",
    "        with smart_open.smart_open(os.path.join(dirname, output), \"wb\") as n:\n",
    "            n.write(temp_norm.encode(\"utf-8\"))\n",
    "        alldata += temp_norm\n",
    "\n",
    "    with smart_open.smart_open(os.path.join(dirname, 'alldata-id.txt'), 'wb') as f:\n",
    "        for idx, line in enumerate(alldata.splitlines()):\n",
    "            num_line = u\"_*{0} {1}\\n\".format(idx, line)\n",
    "            f.write(num_line.encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentences(alldata_filename):\n",
    "    SentimentDocument = namedtuple('SentimentDocument', 'words tags split sentiment')\n",
    "    alldocs = []  # Will hold all docs in original order\n",
    "\n",
    "    with open(alldata_filename, encoding='utf-8') as alldata:\n",
    "        for line_no, line in enumerate(alldata):\n",
    "            tokens = gensim.utils.to_unicode(line).split()\n",
    "            words = tokens[1:]\n",
    "            tags = [line_no]\n",
    "            split = ['train', 'test'][line_no//25000]  # 25k train, 25k test\n",
    "            sentiment = [1.0, 0.0, 1.0, 0.0][line_no//12500] #[12.5K pos, 12.5K neg]*2\n",
    "            alldocs.append(SentimentDocument(words, tags, split, sentiment))\n",
    "\n",
    "    train_docs = [doc for doc in alldocs if doc.split == 'train']\n",
    "    test_docs = [doc for doc in alldocs if doc.split == 'test']\n",
    "    doc_list = alldocs[:]  # For reshuffling per pass\n",
    "    return (train_docs, test_docs, doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_doc2vec(all_docs, num_epoch=10, size=128):\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "\n",
    "    model = Doc2Vec(dm=1, dm_concat=1, size=size,\n",
    "                window=5, negative=5, hs=0, min_count=1, workers=cores, sample=1e-4)\n",
    "    model.build_vocab(all_docs)\n",
    "\n",
    "    alpha, min_alpha = (0.025, 0.001)\n",
    "    alpha_delta = (alpha - min_alpha) / num_epoch\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        shuffle(all_docs)\n",
    "        model.alpha, model.min_alpha = alpha, min_alpha\n",
    "        model.train(all_docs, total_examples=len(all_docs), epochs=1)\n",
    "        alpha -= alpha_delta\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# neural network classifier\n",
    "class NN_classifier(object):\n",
    "    def __init__(self, rng, input, n_in, n_out, U=None, b=None):\n",
    "        self.input = input\n",
    "\n",
    "        if U is None:\n",
    "            U_values = 0.01 * rng.randn(n_in, n_out).astype(config.floatX)\n",
    "            U = theano.shared(value=U_values, name='U', borrow=True)\n",
    "\n",
    "        if b is None:\n",
    "            b_values = numpy.zeros((n_out,)).astype(config.floatX)\n",
    "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
    "\n",
    "        self.U = U\n",
    "        self.b = b\n",
    "\n",
    "        self.pred = tensor.nnet.softmax(tensor.dot(input, self.U) + self.b)\n",
    "\n",
    "        self.y_pred = self.pred.argmax(axis=1)\n",
    "\n",
    "        self.weight_decay = (self.U ** 2).sum()\n",
    "\n",
    "        self.params = [self.U, self.b]\n",
    "\n",
    "    def cost(self, y):\n",
    "        off = 1e-8\n",
    "        if self.pred.dtype == 'float16':\n",
    "            off = 1e-6\n",
    "        return -tensor.log(self.pred[tensor.arange(y.shape[0]), y] + off).mean()\n",
    "\n",
    "    def errors(self, y):\n",
    "        if y.ndim != self.y_pred.ndim:\n",
    "            raise TypeError(\n",
    "                'y should have the same shape as self.y_pred',\n",
    "                ('y', y.type, 'y_pred', self.y_pred.type)\n",
    "            )\n",
    "        if y.dtype.startswith('int'):\n",
    "            return tensor.mean(tensor.neq(self.y_pred, y))\n",
    "        else:\n",
    "            raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(model, train_set, test_set, valid_portion=0.1, test_size=0):\n",
    "    train_set_y, train_set_x = zip(*[(doc.sentiment, model.docvecs[doc.tags[0]])\n",
    "                                            for doc in train_set])\n",
    "\n",
    "    n_samples = len(train_set_x)\n",
    "    sidx = numpy.random.permutation(n_samples)\n",
    "    n_train = int(numpy.round(n_samples * (1. - valid_portion)))\n",
    "\n",
    "    valid_x = [train_set_x[s] for s in sidx[n_train:]]\n",
    "    valid_y = [train_set_y[s] for s in sidx[n_train:]]\n",
    "\n",
    "    train_x = [train_set_x[s] for s in sidx[:n_train]]\n",
    "    train_y = [train_set_y[s] for s in sidx[:n_train]]\n",
    "\n",
    "    test_set_y, test_set_x = zip(*[(doc.sentiment, model.docvecs[doc.tags[0]])\n",
    "                                            for doc in test_set])\n",
    "    \n",
    "    if test_size > 0:\n",
    "        idx = numpy.arange(len(test_set_x))\n",
    "        numpy.random.shuffle(idx)\n",
    "        idx = idx[:test_size]\n",
    "        test = ([test_set_x[n] for n in idx], [test_set_y[n] for n in idx])\n",
    "\n",
    "    def shared_dataset(data_x, data_y, borrow=True):\n",
    "        shared_x = theano.shared(numpy.asarray(data_x,\n",
    "                                               dtype=theano.config.floatX),\n",
    "                                 borrow=borrow)\n",
    "        shared_y = theano.shared(numpy.asarray(data_y,\n",
    "                                               dtype=theano.config.floatX),\n",
    "                                 borrow=borrow)\n",
    "        return shared_x, tensor.cast(shared_y, 'int32')\n",
    "\n",
    "    test_set_x, test_set_y = shared_dataset(test[0], test[1])\n",
    "    valid_set_x, valid_set_y = shared_dataset(valid_x, valid_y)\n",
    "    train_set_x, train_set_y = shared_dataset(train_x, train_y)\n",
    "\n",
    "    rval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y),\n",
    "            (test_set_x, test_set_y)]\n",
    "\n",
    "    return rval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NN_classifier_optimize(doc2vec_model, train_set, test_set,\n",
    "                           test_size=5000, batch_size=16, \n",
    "                           decay_c=0.001, learning_rate=0.001,\n",
    "                           valid_portion=0.1, max_epoch=100,\n",
    "                           patience=50, validFreq=-1):\n",
    "\n",
    "    train, valid, test = load_dataset(doc2vec_model, train_set, test_set,\n",
    "                                      test_size=test_size, valid_portion=valid_portion)\n",
    "\n",
    "    train_x, train_y = train\n",
    "    valid_x, valid_y = valid\n",
    "    test_x, test_y = test\n",
    "    print(train_x.shape.eval())\n",
    "\n",
    "    n_train_batches = train_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    n_valid_batches = valid_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    n_test_batches = test_x.get_value(borrow=True).shape[0] // batch_size\n",
    "\n",
    "    index = tensor.iscalar()\n",
    "    x = tensor.matrix('x')\n",
    "    y = tensor.ivector('y')\n",
    "\n",
    "    rng = numpy.random.RandomState(1234)\n",
    "    n_in = train_x.get_value(borrow=True).shape[1]\n",
    "    classifier = NN_classifier(rng= rng, input=x, n_in=n_in, n_out=2)\n",
    "\n",
    "    test_model = theano.function(\n",
    "        inputs = [index],\n",
    "        outputs = classifier.errors(y),\n",
    "        givens = {\n",
    "            x: test_x[index * batch_size:(index + 1) * batch_size],\n",
    "            y: test_y[index * batch_size:(index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    validate_model = theano.function(\n",
    "        inputs = [index],\n",
    "        outputs = classifier.errors(y),\n",
    "        givens = {\n",
    "            x: valid_x[index * batch_size:(index + 1) * batch_size],\n",
    "            y: valid_y[index * batch_size:(index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    cost = classifier.cost(y)\n",
    "    if decay_c > 0.:\n",
    "        decay_c = theano.shared(numpy.asarray(decay_c, dtype=config.floatX), name='decay_c')\n",
    "        classifier.weight_decay *= decay_c\n",
    "        cost += classifier.weight_decay\n",
    "\n",
    "    g_U = tensor.grad(cost=cost, wrt=classifier.U)\n",
    "    g_b = tensor.grad(cost=cost, wrt=classifier.b)\n",
    "\n",
    "    updates = [(classifier.U, classifier.U - learning_rate * g_U),\n",
    "               (classifier.b, classifier.b - learning_rate * g_b)]\n",
    "\n",
    "    train_model = theano.function(\n",
    "        inputs = [index],\n",
    "        outputs = cost,\n",
    "        updates = updates,\n",
    "        givens = {\n",
    "            x: train_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print('... training the model')\n",
    "    patience_increase = 2\n",
    "    improvement_threshold = 0.995\n",
    "\n",
    "    if validFreq == -1:\n",
    "        validFreq = n_train_batches\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    test_score = 0.\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    done_looping = False\n",
    "    epoch = 0\n",
    "    while (epoch < max_epoch) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "            if (iter + 1) % validFreq == 0:\n",
    "                validation_losses = [validate_model(i)\n",
    "                                     for i in range(n_valid_batches)]\n",
    "                this_validation_loss = numpy.mean(validation_losses)\n",
    "                print('epoch %i, minibatch %i/%i,validation error %f %%' %(epoch,\n",
    "                                                                           minibatch_index + 1,\n",
    "                                                                           n_train_batches,\n",
    "                                                                          this_validation_loss * 100.))\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "                    if this_validation_loss < best_validation_loss * improvement_threshold:\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "                    best_validation_loss = this_validation_loss\n",
    "\n",
    "                    test_losses = [test_model(i) for i in range(n_test_batches)]\n",
    "                    test_score = numpy.mean(test_losses)\n",
    "                    print(('     epoch %i, minibatch %i/%i, test error of'\n",
    "                                ' best model %f %%') %(epoch,\n",
    "                                                       minibatch_index + 1,\n",
    "                                                       n_train_batches,\n",
    "                                                       test_score * 100.))\n",
    "                if patience <= iter:\n",
    "                    done_looping = True\n",
    "                    break\n",
    "    end_time = timeit.default_timer()\n",
    "    print(('Optimization complete with best validation score of %f %%,'\n",
    "           'with test performance %f %%')%(best_validation_loss * 100.,\n",
    "                                           test_score * 100.))\n",
    "    print('The code run for %d epochs, with %f epochs/sec' % (\n",
    "            epoch, 1. * epoch / (end_time - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(reload=None, emb_size=128, doc2vec_epoch=20):\n",
    "    dirname = '/Users/lifa08/Local_documents/Machine_Learning/Miniproject_test/aclImdb'\n",
    "    alldata_file = os.path.join(dirname, 'alldata-id.txt')\n",
    "\n",
    "    if not os.path.isfile(alldata_file):\n",
    "        clean_files(dirname)\n",
    "\n",
    "    assert os.path.isfile(alldata_file), \"alldata-id.txt unavailable\"\n",
    "\n",
    "    train_set, test_set, all_docs = get_sentences(alldata_file)\n",
    "\n",
    "    if reload is not None:\n",
    "        model = Doc2Vec.load('./imdb.d2v')\n",
    "    else:\n",
    "        # train doc2vec\n",
    "        print('training doc2vec')\n",
    "        start_time = timeit.default_timer()\n",
    "        model = train_doc2vec(all_docs, num_epoch=doc2vec_epoch, size=emb_size)\n",
    "        model.save('./imdb.d2v')\n",
    "        end_time = timeit.default_timer()\n",
    "        print('doc2vec training took %f sec' %(end_time - start_time))\n",
    "\n",
    "    NN_classifier_optimize(doc2vec_model=model, train_set=train_set, test_set=test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training doc2vec\n",
      "doc2vec training took 516.507186 sec\n",
      "[22500   128]\n",
      "... training the model\n",
      "epoch 1, minibatch 1406/1406,validation error 47.195513 %\n",
      "     epoch 1, minibatch 1406/1406, test error of best model 46.794872 %\n",
      "epoch 2, minibatch 1406/1406,validation error 47.195513 %\n",
      "Optimization complete with best validation score of 47.195513 %,with test performance 46.794872 %\n",
      "The code run for 2 epochs, with 44.244200 epochs/sec\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training doc2vec\n",
      "doc2vec training took 562.559510 sec\n",
      "[22500   128]\n",
      "... training the model\n",
      "epoch 1, minibatch 1406/1406,validation error 48.237179 %\n",
      "     epoch 1, minibatch 1406/1406, test error of best model 49.018429 %\n",
      "epoch 2, minibatch 1406/1406,validation error 48.237179 %\n",
      "Optimization complete with best validation score of 48.237179 %,with test performance 49.018429 %\n",
      "The code run for 2 epochs, with 51.006320 epochs/sec\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
