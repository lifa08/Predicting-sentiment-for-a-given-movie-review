{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proprocess raw texts to digital indices and train them to word embeddings via gensim's word2vec\n",
    "\n",
    "**Libraries used:**\n",
    "1. nltk: used to tokenize text\n",
    "\n",
    "2. gensim: use its models.word2vec to produce word vectors.\n",
    "\n",
    "**Note: This is summerized in a more concise python file Wemb_gensim.py **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "import nltk\n",
    "import numpy\n",
    "import pickle as pkl\n",
    "import sys\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Strip punctuation/symbols from words\n",
    "def normalize_text(text):\n",
    "    norm_text = text.lower()\n",
    "    \n",
    "    # Replace breaks with spaces\n",
    "    norm_text = norm_text.replace('<br />', ' ')\n",
    "\n",
    "    # Pad punctuation with spaces on both sides\n",
    "    for char in [':', '\"', ',', '(', ')', '!', '?', ';', '*']:\n",
    "        norm_text = norm_text.replace(char, ' ')\n",
    "\n",
    "    norm_text = norm_text.replace('.', ' ' + '.')\n",
    "    return norm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_dataset(path):\n",
    "    if sys.version > '3':\n",
    "        control_chars = [chr(0x85)] # UTF-8\n",
    "    else:\n",
    "        control_chars = [unichr(0x85)] # UTF-16 which is often used in other language codes (e.g. Chinese)\n",
    "\n",
    "    dataset = []\n",
    "    currdir = os.getcwd()\n",
    "    os.chdir(path)\n",
    "    #i = 0\n",
    "    for ff in glob.glob(\"*.txt\"):\n",
    "        #i += 1\n",
    "        with open(ff, \"r\") as f:\n",
    "            line_txt = f.readline().strip()\n",
    "            line_norm = normalize_text(line_txt)\n",
    "            dataset.append(line_norm)\n",
    "        #if(i==100):\n",
    "            #break\n",
    "    os.chdir(currdir)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dict(path, Wemb_size=128, iter=10):\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "\n",
    "    sentences_pos = read_dataset(path+'/pos/')\n",
    "    sentences_neg = read_dataset(path+'/neg/')\n",
    "\n",
    "    sentences_train = sentences_pos + sentences_neg\n",
    "    tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences_train]\n",
    "\n",
    "    model = gensim.models.Word2Vec(tokenized_sentences, min_count=1,\n",
    "                                   size=Wemb_size, window=5,\n",
    "                                   workers=cores, iter=iter)\n",
    "\n",
    "    tok_sents_pos = tokenized_sentences[:len(sentences_pos)]\n",
    "    tok_sents_neg = tokenized_sentences[len(sentences_pos):]\n",
    "\n",
    "    return {'model': model, 'tok_sents_pos': tok_sents_pos, 'tok_sents_neg': tok_sents_neg}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert words in text sentences to their corresponding indices in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence2idx(tokenized_sentences, model):\n",
    "    idx = []\n",
    "    for tok_sen in tokenized_sentences:\n",
    "        idx_sent = numpy.zeros(len(tok_sen), dtype=numpy.int)\n",
    "        for i, word in enumerate(tok_sen):\n",
    "            if word in model.wv.vocab:\n",
    "                idx_sent[i] = model.wv.vocab[word].index\n",
    "            else:\n",
    "                idx_sent[i] = model.wv.vocab['.'].index\n",
    "        idx.append(idx_sent)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert words in the dictionary into embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, there is no need to create word embedding specifically, because it is included in the trained gensim model and can be acessed by `model.wv.syn0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_Wemb(model, Wemb_size=128):\n",
    "    Wemb = numpy.zeros((len(model.wv.vocab), Wemb_size))\n",
    "    for i in range(len(model.wv.vocab)):\n",
    "        embedding_vector = model.wv[model.wv.index2word[i]]\n",
    "        if embedding_vector is not None:\n",
    "            Wemb[i] = embedding_vector\n",
    "    return Wemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '/Users/lifa08/Local_documents/Machine_Learning/Miniproject_test/aclImdb/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-02 17:18:52,626 : INFO : collecting all words and their counts\n",
      "2018-07-02 17:18:52,627 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-07-02 17:18:53,139 : INFO : PROGRESS: at sentence #10000, processed 2518483 words, keeping 63047 word types\n",
      "2018-07-02 17:18:53,724 : INFO : PROGRESS: at sentence #20000, processed 5010758 words, keeping 90480 word types\n",
      "2018-07-02 17:18:54,034 : INFO : collected 101119 word types from a corpus of 6251170 raw words and 25000 sentences\n",
      "2018-07-02 17:18:54,035 : INFO : Loading a fresh vocabulary\n",
      "2018-07-02 17:18:54,576 : INFO : min_count=1 retains 101119 unique words (100% of original 101119, drops 0)\n",
      "2018-07-02 17:18:54,579 : INFO : min_count=1 leaves 6251170 word corpus (100% of original 6251170, drops 0)\n",
      "2018-07-02 17:18:54,899 : INFO : deleting the raw counts dictionary of 101119 items\n",
      "2018-07-02 17:18:54,904 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2018-07-02 17:18:54,905 : INFO : downsampling leaves estimated 4559750 word corpus (72.9% of prior 6251170)\n",
      "2018-07-02 17:18:54,906 : INFO : estimated required memory for 101119 words and 128 dimensions: 154105356 bytes\n",
      "2018-07-02 17:18:55,296 : INFO : resetting layer weights\n",
      "2018-07-02 17:18:57,293 : INFO : training model with 4 workers on 101119 vocabulary and 128 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-07-02 17:18:58,303 : INFO : PROGRESS: at 1.67% examples, 765104 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:18:59,306 : INFO : PROGRESS: at 3.44% examples, 784853 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-02 17:19:00,308 : INFO : PROGRESS: at 5.11% examples, 780562 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:01,317 : INFO : PROGRESS: at 6.86% examples, 782096 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:02,321 : INFO : PROGRESS: at 8.68% examples, 789728 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:03,324 : INFO : PROGRESS: at 10.38% examples, 785236 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:04,332 : INFO : PROGRESS: at 12.14% examples, 787592 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:05,337 : INFO : PROGRESS: at 13.92% examples, 790878 words/s, in_qsize 8, out_qsize 1\n",
      "2018-07-02 17:19:06,348 : INFO : PROGRESS: at 15.69% examples, 792686 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:07,353 : INFO : PROGRESS: at 17.48% examples, 793303 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:08,354 : INFO : PROGRESS: at 19.14% examples, 790051 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:09,363 : INFO : PROGRESS: at 20.90% examples, 790253 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:10,365 : INFO : PROGRESS: at 22.69% examples, 792671 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:11,367 : INFO : PROGRESS: at 24.41% examples, 792311 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:12,391 : INFO : PROGRESS: at 26.25% examples, 793974 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:13,394 : INFO : PROGRESS: at 28.13% examples, 797487 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:14,400 : INFO : PROGRESS: at 29.93% examples, 797832 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:15,406 : INFO : PROGRESS: at 31.30% examples, 788209 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:16,409 : INFO : PROGRESS: at 32.98% examples, 787696 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:17,418 : INFO : PROGRESS: at 34.72% examples, 787767 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:18,425 : INFO : PROGRESS: at 36.49% examples, 788150 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:19,441 : INFO : PROGRESS: at 37.84% examples, 779437 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:20,464 : INFO : PROGRESS: at 39.60% examples, 779289 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-02 17:19:21,468 : INFO : PROGRESS: at 41.26% examples, 778567 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:22,479 : INFO : PROGRESS: at 43.00% examples, 779186 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:23,486 : INFO : PROGRESS: at 44.85% examples, 781532 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:24,489 : INFO : PROGRESS: at 46.56% examples, 781098 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-02 17:19:25,506 : INFO : PROGRESS: at 47.98% examples, 775782 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:26,536 : INFO : PROGRESS: at 49.60% examples, 773417 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-02 17:19:27,539 : INFO : PROGRESS: at 51.15% examples, 771401 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:28,547 : INFO : PROGRESS: at 52.56% examples, 767130 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:29,571 : INFO : PROGRESS: at 53.44% examples, 755238 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:30,585 : INFO : PROGRESS: at 55.03% examples, 754416 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:31,590 : INFO : PROGRESS: at 56.88% examples, 756698 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:32,598 : INFO : PROGRESS: at 58.74% examples, 759005 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-02 17:19:33,599 : INFO : PROGRESS: at 60.60% examples, 761334 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-02 17:19:34,605 : INFO : PROGRESS: at 62.28% examples, 761305 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-02 17:19:35,657 : INFO : PROGRESS: at 63.30% examples, 752575 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-02 17:19:36,668 : INFO : PROGRESS: at 64.26% examples, 744577 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:37,675 : INFO : PROGRESS: at 65.00% examples, 734496 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:38,741 : INFO : PROGRESS: at 65.84% examples, 724895 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:39,744 : INFO : PROGRESS: at 67.43% examples, 724502 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-02 17:19:40,745 : INFO : PROGRESS: at 69.15% examples, 725746 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:41,746 : INFO : PROGRESS: at 70.90% examples, 727391 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:42,746 : INFO : PROGRESS: at 72.54% examples, 727928 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:43,751 : INFO : PROGRESS: at 74.24% examples, 729023 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:44,754 : INFO : PROGRESS: at 75.92% examples, 729864 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:45,777 : INFO : PROGRESS: at 77.75% examples, 731338 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:46,780 : INFO : PROGRESS: at 79.38% examples, 731379 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:47,782 : INFO : PROGRESS: at 81.09% examples, 732443 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:48,783 : INFO : PROGRESS: at 82.78% examples, 733366 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:49,784 : INFO : PROGRESS: at 84.45% examples, 733996 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:50,787 : INFO : PROGRESS: at 86.16% examples, 734653 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-02 17:19:51,792 : INFO : PROGRESS: at 87.80% examples, 734672 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:52,832 : INFO : PROGRESS: at 88.95% examples, 730435 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:53,852 : INFO : PROGRESS: at 89.46% examples, 721177 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-02 17:19:54,892 : INFO : PROGRESS: at 90.14% examples, 713506 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-02 17:19:55,902 : INFO : PROGRESS: at 91.30% examples, 710359 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:56,903 : INFO : PROGRESS: at 93.14% examples, 712613 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:57,903 : INFO : PROGRESS: at 94.73% examples, 713024 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:58,913 : INFO : PROGRESS: at 96.46% examples, 713997 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:19:59,917 : INFO : PROGRESS: at 98.15% examples, 714781 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:20:00,991 : INFO : PROGRESS: at 99.64% examples, 713287 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 17:20:01,202 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-02 17:20:01,216 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-02 17:20:01,218 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-02 17:20:01,229 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-02 17:20:01,231 : INFO : training on 62511700 raw words (45593202 effective words) took 63.9s, 713137 effective words/s\n"
     ]
    }
   ],
   "source": [
    "result = build_dict(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents_pos = result['tok_sents_pos']\n",
    "model = result['model']\n",
    "train_x_pos = sentence2idx(sents_pos, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-02 17:20:10,797 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('decent', 0.7537664771080017),\n",
       " ('great', 0.6990833282470703),\n",
       " ('bad', 0.6676902174949646),\n",
       " ('cool', 0.6532009840011597),\n",
       " ('lousy', 0.6135501265525818),\n",
       " ('nice', 0.6117060780525208),\n",
       " ('solid', 0.6004713773727417),\n",
       " ('fine', 0.591917872428894),\n",
       " ('ok', 0.5889630317687988),\n",
       " ('terrific', 0.5840588808059692)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  792,    16,    29,     4,     0,   118,  1764,  7121,    10,\n",
      "          19,   983,     5,    28, 20422,     5,     7,    12,  2375,\n",
      "        1807,   128,  2230,     5,     3,  6987,   300,     1,  2584,\n",
      "        2314,     0,    19,    36,   484,  5942,    12,  3371,     2,\n",
      "          39,    12,     3,  1003,   175,    21,    50,   782,     1]), array([   78,     1,     1, 49001,     9,     0,   197,   627,   131,\n",
      "           8,  3345,     2,     9,   250,     0,    17,   179,   710,\n",
      "           5,   107,     1,   190,    32, 12560,     5,    99, 10283,\n",
      "        1478,     2,  3369,     1,  1642,    71,   147,   103,   627,\n",
      "         168,  4411,   204,   102,    32,  1747,     0,    90,    29,\n",
      "           9,   375,  2383, 26621,   151,    63,   537,   305,     6,\n",
      "        2200,     4,  9892,     0,   861,   137,     8,  6074,   397,\n",
      "          55,    32,   955,   155,    29,     4,   132, 19296, 12672,\n",
      "           4,     3,    17, 10283,  5509,    61,    23,    28,  1148,\n",
      "           3,    86,    17,     8,   189,     7,    61,    23,    28,\n",
      "          56,  1148,    34,  8097,     4,     3,    86,    17,    15,\n",
      "           7,   445,    59,     5,    51,    75,    11,     1,    14,\n",
      "           9,  3301,     5,   850,   150,     7,     9,  1901,    11,\n",
      "          57,   502,   627,     5,  7401,     2,     9,  3195, 29310,\n",
      "          11,     0,   115,   240,    36,     0,  1732,    55,  9426,\n",
      "         144,     0,   251,     2,   499,    71,     1,    72,    32,\n",
      "          65,   104,    11,  5801,     0,    81,    35,    66, 16752,\n",
      "           8,     0,    84,   103,    59,    14,   150,    32,    70,\n",
      "         177,    36,     0,  1313,  1210,    55,    99,     0,    17,\n",
      "          16,     0,  2532,   169, 12557,   101,   125,  1209,     9,\n",
      "          55,   113,  1414,  2636,    10,    17,     9,   375,    39,\n",
      "          24,  9625,   734,     5,   163,   182,    11,  3560,    78,     1])]\n"
     ]
    }
   ],
   "source": [
    "sents_neg = result['tok_sents_neg']\n",
    "train_x_neg = sentence2idx(sents_neg, model)\n",
    "print(train_x_neg[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = train_x_pos + train_x_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = [1] * len(train_x_pos) + [0] * len(train_x_neg)\n",
    "# print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_path = '/Users/lifa08/Local_documents/Machine_Learning/Miniproject_test/aclImdb/test'\n",
    "test_sents_pos = read_dataset(test_path+'/pos/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok_test_sents_pos = [nltk.word_tokenize(sent) for sent in test_sents_pos]\n",
    "test_x_pos = sentence2idx(tok_test_sents_pos, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sents_neg = read_dataset(test_path+'/neg/')\n",
    "tok_test_sents_neg = [nltk.word_tokenize(sent) for sent in test_sents_neg]\n",
    "test_x_neg = sentence2idx(tok_test_sents_neg, model)\n",
    "test_x = test_x_pos + test_x_neg\n",
    "test_y = [1] * len(test_x_pos) + [0] * len(test_x_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save converted sentence indices to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('/Users/lifa08/Local_documents/Machine_Learning/miniproject/gensim/gensim_imdb.pkl', 'wb')\n",
    "pkl.dump((train_x, train_y), f, -1)\n",
    "pkl.dump((test_x, test_y), f, -1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('/Users/lifa08/Local_documents/Machine_Learning/miniproject/gensim/gensim_imdb.pkl', 'rb')\n",
    "train_set = pkl.load(f)\n",
    "test_set = pkl.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save word embeddings to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Wemb = create_Wemb(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('/Users/lifa08/Local_documents/Machine_Learning/miniproject/gensim/gensim_imdb_Wemb.pkl', 'wb')\n",
    "pkl.dump(Wemb, f, -1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('/Users/lifa08/Local_documents/Machine_Learning/miniproject/gensim/gensim_imdb_Wemb.pkl', 'rb')\n",
    "Wemb_file = pkl.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.98381102e-01   4.73410547e-01   1.33979070e+00 ...,  -1.87317252e+00\n",
      "    2.34784722e+00   8.82582009e-01]\n",
      " [ -6.02306604e-01  -3.29882771e-01   1.81562877e+00 ...,  -1.46761751e+00\n",
      "    1.23798764e+00   8.82097363e-01]\n",
      " [ -6.25977695e-01  -6.34995878e-01   1.92042744e+00 ...,  -8.90727460e-01\n",
      "    3.45125347e-02   8.86263549e-01]\n",
      " ..., \n",
      " [  5.45666972e-03  -1.41647756e-02   5.51845878e-03 ...,   4.92366478e-02\n",
      "   -1.74962711e-02   1.49816228e-02]\n",
      " [ -1.06746666e-02  -2.27900594e-03   1.81603748e-02 ...,  -3.55963185e-02\n",
      "    2.40580682e-02   1.41165266e-03]\n",
      " [  5.12123890e-02   3.75586897e-02  -4.52179313e-02 ...,   3.36575974e-03\n",
      "   -4.37835837e-03   4.71857414e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(Wemb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.98381102e-01   4.73410547e-01   1.33979070e+00 ...,  -1.87317252e+00\n",
      "    2.34784722e+00   8.82582009e-01]\n",
      " [ -6.02306604e-01  -3.29882771e-01   1.81562877e+00 ...,  -1.46761751e+00\n",
      "    1.23798764e+00   8.82097363e-01]\n",
      " [ -6.25977695e-01  -6.34995878e-01   1.92042744e+00 ...,  -8.90727460e-01\n",
      "    3.45125347e-02   8.86263549e-01]\n",
      " ..., \n",
      " [  5.45666972e-03  -1.41647756e-02   5.51845878e-03 ...,   4.92366478e-02\n",
      "   -1.74962711e-02   1.49816228e-02]\n",
      " [ -1.06746666e-02  -2.27900594e-03   1.81603748e-02 ...,  -3.55963185e-02\n",
      "    2.40580682e-02   1.41165266e-03]\n",
      " [  5.12123890e-02   3.75586897e-02  -4.52179313e-02 ...,   3.36575974e-03\n",
      "   -4.37835837e-03   4.71857414e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(Wemb_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include all above into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_gensim_w2vec(path):\n",
    "    # path = '/Users/lifa08/Local_documents/Machine_Learning/Miniproject_test/aclImdb/'\n",
    "    result = build_dict(path+'train')\n",
    "    model = result['model']\n",
    "\n",
    "    sents_pos = result['tok_sents_pos']\n",
    "    train_x_pos = sentence2idx(sents_pos, model)\n",
    "\n",
    "    sents_neg = result['tok_sents_neg']\n",
    "    train_x_neg = sentence2idx(sents_neg, model)\n",
    "    train_x = train_x_pos + train_x_neg\n",
    "    train_y = [1] * len(train_x_pos) + [0] * len(train_x_neg)\n",
    "\n",
    "    test_sents_pos = read_dataset(path+'test/pos/')\n",
    "    tok_test_sents_pos = [nltk.word_tokenize(sent) for sent in test_sents_pos]\n",
    "    test_x_pos = sentence2idx(tok_test_sents_pos, model)\n",
    "\n",
    "    test_sents_neg = read_dataset(path+'test/neg/')\n",
    "    tok_test_sents_neg = [nltk.word_tokenize(sent) for sent in test_sents_neg]\n",
    "    test_x_neg = sentence2idx(tok_test_sents_neg, model)\n",
    "    test_x = test_x_pos + test_x_neg\n",
    "    test_y = [1] * len(test_x_pos) + [0] * len(test_x_neg)\n",
    "\n",
    "    f = open('/Users/lifa08/Local_documents/Machine_Learning/miniproject/gensim/gensim_imdb.pkl', 'wb')\n",
    "    pkl.dump((train_x, train_y), f, -1)\n",
    "    pkl.dump((test_x, test_y), f, -1)\n",
    "    f.close()\n",
    "\n",
    "    Wemb = create_Wemb(model)\n",
    "    f = open('/Users/lifa08/Local_documents/Machine_Learning/miniproject/gensim/gensim_imdb_Wemb.pkl', 'wb')\n",
    "    pkl.dump(Wemb, f, -1)\n",
    "    f.close()\n",
    "\n",
    "    model.save('/Users/lifa08/Local_documents/Machine_Learning/miniproject/gensim/imdb_gensim_w2vmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-02 18:09:32,880 : INFO : collecting all words and their counts\n",
      "2018-07-02 18:09:32,882 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-07-02 18:09:33,363 : INFO : PROGRESS: at sentence #10000, processed 2518483 words, keeping 63047 word types\n",
      "2018-07-02 18:09:33,897 : INFO : PROGRESS: at sentence #20000, processed 5010758 words, keeping 90480 word types\n",
      "2018-07-02 18:09:34,198 : INFO : collected 101119 word types from a corpus of 6251170 raw words and 25000 sentences\n",
      "2018-07-02 18:09:34,199 : INFO : Loading a fresh vocabulary\n",
      "2018-07-02 18:09:35,913 : INFO : min_count=1 retains 101119 unique words (100% of original 101119, drops 0)\n",
      "2018-07-02 18:09:35,914 : INFO : min_count=1 leaves 6251170 word corpus (100% of original 6251170, drops 0)\n",
      "2018-07-02 18:09:36,197 : INFO : deleting the raw counts dictionary of 101119 items\n",
      "2018-07-02 18:09:36,201 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2018-07-02 18:09:36,202 : INFO : downsampling leaves estimated 4559750 word corpus (72.9% of prior 6251170)\n",
      "2018-07-02 18:09:36,204 : INFO : estimated required memory for 101119 words and 128 dimensions: 154105356 bytes\n",
      "2018-07-02 18:09:36,543 : INFO : resetting layer weights\n",
      "2018-07-02 18:09:37,887 : INFO : training model with 4 workers on 101119 vocabulary and 128 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-07-02 18:09:38,892 : INFO : PROGRESS: at 1.75% examples, 804341 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:09:39,897 : INFO : PROGRESS: at 3.52% examples, 803714 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:09:40,910 : INFO : PROGRESS: at 5.36% examples, 816682 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:09:41,917 : INFO : PROGRESS: at 7.19% examples, 816557 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:09:42,920 : INFO : PROGRESS: at 9.00% examples, 817564 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-02 18:09:43,923 : INFO : PROGRESS: at 10.84% examples, 820106 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:09:44,924 : INFO : PROGRESS: at 12.59% examples, 817519 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:09:45,930 : INFO : PROGRESS: at 14.53% examples, 826782 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:09:46,931 : INFO : PROGRESS: at 15.60% examples, 788907 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-02 18:09:47,932 : INFO : PROGRESS: at 17.34% examples, 788078 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:09:48,936 : INFO : PROGRESS: at 19.18% examples, 792275 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:09:49,936 : INFO : PROGRESS: at 20.98% examples, 794703 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:09:50,947 : INFO : PROGRESS: at 22.64% examples, 791892 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:09:51,959 : INFO : PROGRESS: at 24.33% examples, 790026 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-02 18:09:52,970 : INFO : PROGRESS: at 26.22% examples, 793858 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-02 18:09:53,977 : INFO : PROGRESS: at 28.04% examples, 795374 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:09:54,983 : INFO : PROGRESS: at 30.06% examples, 801773 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:09:55,984 : INFO : PROGRESS: at 31.19% examples, 786237 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:09:56,991 : INFO : PROGRESS: at 31.71% examples, 757444 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:09:57,992 : INFO : PROGRESS: at 32.50% examples, 737602 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:09:58,992 : INFO : PROGRESS: at 34.23% examples, 740373 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:00,036 : INFO : PROGRESS: at 34.84% examples, 718086 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-02 18:10:01,050 : INFO : PROGRESS: at 35.28% examples, 695612 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:02,072 : INFO : PROGRESS: at 35.80% examples, 675919 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:03,072 : INFO : PROGRESS: at 36.72% examples, 665565 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-02 18:10:04,077 : INFO : PROGRESS: at 38.34% examples, 668193 words/s, in_qsize 8, out_qsize 1\n",
      "2018-07-02 18:10:05,081 : INFO : PROGRESS: at 40.10% examples, 672503 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:06,082 : INFO : PROGRESS: at 41.82% examples, 676743 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:07,089 : INFO : PROGRESS: at 43.55% examples, 680422 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:08,097 : INFO : PROGRESS: at 44.89% examples, 678370 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:09,106 : INFO : PROGRESS: at 46.10% examples, 674070 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:10,111 : INFO : PROGRESS: at 47.58% examples, 673536 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-02 18:10:11,122 : INFO : PROGRESS: at 49.70% examples, 682113 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:12,125 : INFO : PROGRESS: at 50.68% examples, 675264 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:13,141 : INFO : PROGRESS: at 52.12% examples, 674461 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:14,147 : INFO : PROGRESS: at 53.47% examples, 672768 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:15,158 : INFO : PROGRESS: at 54.30% examples, 664911 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:16,167 : INFO : PROGRESS: at 56.19% examples, 669777 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:17,176 : INFO : PROGRESS: at 56.57% examples, 656972 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:18,178 : INFO : PROGRESS: at 57.69% examples, 653101 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:19,194 : INFO : PROGRESS: at 58.89% examples, 650356 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:20,196 : INFO : PROGRESS: at 60.26% examples, 649499 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:21,206 : INFO : PROGRESS: at 61.85% examples, 651331 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:22,215 : INFO : PROGRESS: at 63.44% examples, 652850 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:23,223 : INFO : PROGRESS: at 65.10% examples, 655241 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:24,226 : INFO : PROGRESS: at 66.72% examples, 656947 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:25,237 : INFO : PROGRESS: at 68.42% examples, 659266 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:26,245 : INFO : PROGRESS: at 70.19% examples, 661799 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:27,246 : INFO : PROGRESS: at 71.85% examples, 664010 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:28,258 : INFO : PROGRESS: at 73.50% examples, 665611 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:29,266 : INFO : PROGRESS: at 75.11% examples, 667069 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-02 18:10:30,271 : INFO : PROGRESS: at 76.72% examples, 668182 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:31,272 : INFO : PROGRESS: at 78.39% examples, 669891 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:32,274 : INFO : PROGRESS: at 80.07% examples, 671380 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:33,274 : INFO : PROGRESS: at 81.77% examples, 673429 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:34,274 : INFO : PROGRESS: at 83.44% examples, 674975 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:35,285 : INFO : PROGRESS: at 85.03% examples, 675945 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:36,286 : INFO : PROGRESS: at 86.65% examples, 676977 words/s, in_qsize 7, out_qsize 1\n",
      "2018-07-02 18:10:37,291 : INFO : PROGRESS: at 88.26% examples, 677837 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:38,296 : INFO : PROGRESS: at 89.83% examples, 678174 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:39,297 : INFO : PROGRESS: at 91.47% examples, 679341 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:40,304 : INFO : PROGRESS: at 93.14% examples, 680674 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:41,305 : INFO : PROGRESS: at 94.82% examples, 682150 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:42,306 : INFO : PROGRESS: at 96.49% examples, 683326 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:43,325 : INFO : PROGRESS: at 98.18% examples, 684394 words/s, in_qsize 8, out_qsize 1\n",
      "2018-07-02 18:10:44,329 : INFO : PROGRESS: at 99.86% examples, 685471 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-02 18:10:44,371 : INFO : worker thread finished; awaiting finish of 3 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-02 18:10:44,390 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-02 18:10:44,392 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-02 18:10:44,404 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-02 18:10:44,405 : INFO : training on 62511700 raw words (45599438 effective words) took 66.5s, 685607 effective words/s\n",
      "2018-07-02 18:12:07,201 : INFO : saving Word2Vec object under /Users/lifa08/Local_documents/Machine_Learning/miniproject/gensim/imdb_gensim_w2vmodel, separately None\n",
      "2018-07-02 18:12:07,202 : INFO : storing np array 'syn0' to /Users/lifa08/Local_documents/Machine_Learning/miniproject/gensim/imdb_gensim_w2vmodel.wv.syn0.npy\n",
      "2018-07-02 18:12:07,400 : INFO : not storing attribute syn0norm\n",
      "2018-07-02 18:12:07,404 : INFO : storing np array 'syn1neg' to /Users/lifa08/Local_documents/Machine_Learning/miniproject/gensim/imdb_gensim_w2vmodel.syn1neg.npy\n",
      "2018-07-02 18:12:07,578 : INFO : not storing attribute cum_table\n",
      "2018-07-02 18:12:07,832 : INFO : saved /Users/lifa08/Local_documents/Machine_Learning/miniproject/gensim/imdb_gensim_w2vmodel\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/lifa08/Local_documents/Machine_Learning/Miniproject_test/aclImdb/'\n",
    "train_gensim_w2vec(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if embeddings from gensim model equal to the word embeddings stored in the file to make sure the correctness of storing word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_idxwemb_wordemb(sentence_idx):\n",
    "    model_path = '/Users/lifa08/Local_documents/Machine_Learning/miniproject/gensim/imdb_gensim_w2vmodel'\n",
    "    model = gensim.models.Word2Vec.load(model_path)\n",
    "    sentence_words = []\n",
    "    for idx, x in enumerate(sentence_idx):\n",
    "        sentence_words.append(model.wv[model.wv.index2word[x]])\n",
    "\n",
    "    f = open('/Users/lifa08/Local_documents/Machine_Learning/miniproject/gensim/gensim_imdb_Wemb.pkl', 'rb')\n",
    "    Wemb = pkl.load(f)\n",
    "    f.close()\n",
    "    emb_x = Wemb[sentence_idx]\n",
    "\n",
    "    return (numpy.matrix(sentence_words)==numpy.matrix(emb_x)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-02 18:24:33,180 : INFO : loading Word2Vec object from /Users/lifa08/Local_documents/Machine_Learning/miniproject/gensim/imdb_gensim_w2vmodel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  792    16    29     4     0   118  1764  7121    10    19   983     5\n",
      "    28 20422     5     7    12  2375  1807   128  2230     5     3  6987\n",
      "   300     1  2584  2314     0    19    36   484  5942    12  3371     2\n",
      "    39    12     3  1003   175    21    50   782     1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-02 18:24:33,418 : INFO : loading wv recursively from /Users/lifa08/Local_documents/Machine_Learning/miniproject/gensim/imdb_gensim_w2vmodel.wv.* with mmap=None\n",
      "2018-07-02 18:24:33,420 : INFO : loading syn0 from /Users/lifa08/Local_documents/Machine_Learning/miniproject/gensim/imdb_gensim_w2vmodel.wv.syn0.npy with mmap=None\n",
      "2018-07-02 18:24:33,458 : INFO : setting ignored attribute syn0norm to None\n",
      "2018-07-02 18:24:33,461 : INFO : loading syn1neg from /Users/lifa08/Local_documents/Machine_Learning/miniproject/gensim/imdb_gensim_w2vmodel.syn1neg.npy with mmap=None\n",
      "2018-07-02 18:24:33,507 : INFO : setting ignored attribute cum_table to None\n",
      "2018-07-02 18:24:33,511 : INFO : loaded /Users/lifa08/Local_documents/Machine_Learning/miniproject/gensim/imdb_gensim_w2vmodel\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_x_neg[0])\n",
    "compare_idxwemb_wordemb(train_x_neg[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
