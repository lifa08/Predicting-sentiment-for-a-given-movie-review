{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "import nltk\n",
    "import numpy\n",
    "import pickle as pkl\n",
    "import sys\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Strip punctuation/symbols from words\n",
    "def normalize_text(text):\n",
    "    norm_text = text.lower()\n",
    "    \n",
    "    # Replace breaks with spaces\n",
    "    norm_text = norm_text.replace('<br />', ' ')\n",
    "\n",
    "    # Pad punctuation with spaces on both sides\n",
    "    for char in [':', '\"', ',', '(', ')', '!', '?', ';', '*']:\n",
    "        norm_text = norm_text.replace(char, ' ')\n",
    "\n",
    "    norm_text = norm_text.replace('.', ' ' + '.')\n",
    "    return norm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_dataset(path):\n",
    "    if sys.version > '3':\n",
    "        control_chars = [chr(0x85)]\n",
    "    else:\n",
    "        control_chars = [unichr(0x85)]\n",
    "\n",
    "    dataset = []\n",
    "    currdir = os.getcwd()\n",
    "    os.chdir(path)\n",
    "    #i = 0\n",
    "    for ff in glob.glob(\"*.txt\"):\n",
    "        #i += 1\n",
    "        with open(ff, \"r\") as f:\n",
    "            line_txt = f.readline().strip()\n",
    "            line_norm = normalize_text(line_txt)\n",
    "            dataset.append(line_norm)\n",
    "        #if(i==100):\n",
    "            #break\n",
    "    os.chdir(currdir)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dict(path, Wemb_size=128, iter=10):\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "\n",
    "    sentences_pos = read_dataset(path+'/pos/')\n",
    "    sentences_neg = read_dataset(path+'/neg/')\n",
    "    \n",
    "    sentences_train = sentences_pos + sentences_neg\n",
    "    tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences_train]\n",
    "\n",
    "    model = gensim.models.Word2Vec(tokenized_sentences, min_count=1,\n",
    "                                   size=Wemb_size, window=5,\n",
    "                                   workers=cores, iter=iter)\n",
    "\n",
    "    tok_sents_pos = tokenized_sentences[:len(sentences_pos)]\n",
    "    tok_sents_neg = tokenized_sentences[len(sentences_pos):]\n",
    "\n",
    "    return {'model': model, 'tok_sents_pos': tok_sents_pos, 'tok_sents_neg': tok_sents_neg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence2idx(tokenized_sentences, model):\n",
    "    idx = []\n",
    "    for tok_sen in tokenized_sentences:\n",
    "        idx_sent = numpy.zeros(len(tok_sen), dtype=numpy.int)\n",
    "        for i, word in enumerate(tok_sen):\n",
    "            if word in model.wv.vocab:\n",
    "                idx_sent[i] = model.wv.vocab[word].index\n",
    "            else:\n",
    "                idx_sent[i] = model.wv.vocab['.'].index\n",
    "        idx.append(idx_sent)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_Wemb(model, Wemb_size=128):\n",
    "    Wemb = numpy.zeros((len(model.wv.vocab), Wemb_size))\n",
    "    for i in range(len(model.wv.vocab)):\n",
    "        embedding_vector = model.wv[model.wv.index2word[i]]\n",
    "        if embedding_vector is not None:\n",
    "            Wemb[i] = embedding_vector\n",
    "    return Wemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '/Users/lifa08/Documents/Lifa/Machine_Learning/Miniproject_test/aclImdb/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-25 15:00:18,290 : INFO : collecting all words and their counts\n",
      "2018-01-25 15:00:18,291 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-01-25 15:00:18,842 : INFO : PROGRESS: at sentence #10000, processed 2518483 words, keeping 63047 word types\n",
      "2018-01-25 15:00:19,370 : INFO : PROGRESS: at sentence #20000, processed 5010758 words, keeping 90480 word types\n",
      "2018-01-25 15:00:19,624 : INFO : collected 101119 word types from a corpus of 6251170 raw words and 25000 sentences\n",
      "2018-01-25 15:00:19,625 : INFO : Loading a fresh vocabulary\n",
      "2018-01-25 15:00:21,236 : INFO : min_count=1 retains 101119 unique words (100% of original 101119, drops 0)\n",
      "2018-01-25 15:00:21,237 : INFO : min_count=1 leaves 6251170 word corpus (100% of original 6251170, drops 0)\n",
      "2018-01-25 15:00:21,562 : INFO : deleting the raw counts dictionary of 101119 items\n",
      "2018-01-25 15:00:21,574 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2018-01-25 15:00:21,580 : INFO : downsampling leaves estimated 4559750 word corpus (72.9% of prior 6251170)\n",
      "2018-01-25 15:00:21,585 : INFO : estimated required memory for 101119 words and 128 dimensions: 154105356 bytes\n",
      "2018-01-25 15:00:22,060 : INFO : resetting layer weights\n",
      "2018-01-25 15:00:23,558 : INFO : training model with 4 workers on 101119 vocabulary and 128 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-01-25 15:00:24,572 : INFO : PROGRESS: at 1.63% examples, 740744 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:00:25,584 : INFO : PROGRESS: at 3.52% examples, 797438 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:00:26,586 : INFO : PROGRESS: at 5.10% examples, 774994 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:00:27,591 : INFO : PROGRESS: at 6.99% examples, 794613 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:00:28,592 : INFO : PROGRESS: at 8.93% examples, 811694 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:00:29,597 : INFO : PROGRESS: at 10.80% examples, 817444 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:00:30,604 : INFO : PROGRESS: at 12.28% examples, 795995 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:00:31,618 : INFO : PROGRESS: at 13.80% examples, 782351 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:00:32,627 : INFO : PROGRESS: at 15.42% examples, 778189 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:00:33,704 : INFO : PROGRESS: at 16.27% examples, 732964 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:00:34,705 : INFO : PROGRESS: at 17.63% examples, 721867 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:00:35,707 : INFO : PROGRESS: at 18.85% examples, 708726 words/s, in_qsize 6, out_qsize 1\n",
      "2018-01-25 15:00:36,754 : INFO : PROGRESS: at 19.94% examples, 689381 words/s, in_qsize 6, out_qsize 1\n",
      "2018-01-25 15:00:37,755 : INFO : PROGRESS: at 20.92% examples, 672396 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-25 15:00:38,776 : INFO : PROGRESS: at 21.89% examples, 656451 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-25 15:00:39,781 : INFO : PROGRESS: at 22.87% examples, 644169 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:00:40,782 : INFO : PROGRESS: at 24.12% examples, 639607 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:00:41,783 : INFO : PROGRESS: at 25.51% examples, 639811 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:00:42,792 : INFO : PROGRESS: at 27.01% examples, 641220 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:00:43,798 : INFO : PROGRESS: at 28.30% examples, 638387 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:00:44,802 : INFO : PROGRESS: at 29.89% examples, 641869 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:00:45,863 : INFO : PROGRESS: at 31.36% examples, 641895 words/s, in_qsize 7, out_qsize 2\n",
      "2018-01-25 15:00:46,884 : INFO : PROGRESS: at 32.60% examples, 637974 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-25 15:00:47,903 : INFO : PROGRESS: at 33.37% examples, 625708 words/s, in_qsize 7, out_qsize 1\n",
      "2018-01-25 15:00:48,903 : INFO : PROGRESS: at 35.20% examples, 634356 words/s, in_qsize 6, out_qsize 1\n",
      "2018-01-25 15:00:49,911 : INFO : PROGRESS: at 37.02% examples, 641307 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:00:50,912 : INFO : PROGRESS: at 38.60% examples, 644001 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:00:51,913 : INFO : PROGRESS: at 40.29% examples, 648022 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:00:52,966 : INFO : PROGRESS: at 40.90% examples, 634518 words/s, in_qsize 8, out_qsize 1\n",
      "2018-01-25 15:00:53,976 : INFO : PROGRESS: at 42.04% examples, 630375 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:00:54,977 : INFO : PROGRESS: at 43.44% examples, 630864 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:00:55,982 : INFO : PROGRESS: at 45.03% examples, 634064 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:00:57,005 : INFO : PROGRESS: at 45.94% examples, 627017 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-25 15:00:58,007 : INFO : PROGRESS: at 47.12% examples, 624194 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:00:59,009 : INFO : PROGRESS: at 48.73% examples, 627147 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:01:00,010 : INFO : PROGRESS: at 50.68% examples, 634242 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:01:01,016 : INFO : PROGRESS: at 52.05% examples, 633803 words/s, in_qsize 8, out_qsize 1\n",
      "2018-01-25 15:01:02,031 : INFO : PROGRESS: at 53.72% examples, 637022 words/s, in_qsize 6, out_qsize 1\n",
      "2018-01-25 15:01:03,127 : INFO : PROGRESS: at 55.10% examples, 635496 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:01:04,131 : INFO : PROGRESS: at 55.70% examples, 626648 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:01:05,134 : INFO : PROGRESS: at 56.88% examples, 624242 words/s, in_qsize 6, out_qsize 1\n",
      "2018-01-25 15:01:06,137 : INFO : PROGRESS: at 58.40% examples, 625887 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:01:07,170 : INFO : PROGRESS: at 59.74% examples, 624676 words/s, in_qsize 6, out_qsize 1\n",
      "2018-01-25 15:01:08,177 : INFO : PROGRESS: at 61.22% examples, 625783 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:01:09,188 : INFO : PROGRESS: at 62.66% examples, 626522 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:01:10,190 : INFO : PROGRESS: at 64.13% examples, 627518 words/s, in_qsize 6, out_qsize 1\n",
      "2018-01-25 15:01:11,193 : INFO : PROGRESS: at 65.60% examples, 628416 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:01:12,200 : INFO : PROGRESS: at 67.01% examples, 628494 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-25 15:01:13,331 : INFO : PROGRESS: at 67.54% examples, 618985 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:01:14,338 : INFO : PROGRESS: at 69.00% examples, 619789 words/s, in_qsize 7, out_qsize 1\n",
      "2018-01-25 15:01:15,338 : INFO : PROGRESS: at 70.92% examples, 624655 words/s, in_qsize 8, out_qsize 1\n",
      "2018-01-25 15:01:16,342 : INFO : PROGRESS: at 72.77% examples, 628931 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:01:17,346 : INFO : PROGRESS: at 74.27% examples, 630013 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:01:18,359 : INFO : PROGRESS: at 76.17% examples, 634137 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-25 15:01:19,369 : INFO : PROGRESS: at 77.98% examples, 637301 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-25 15:01:20,376 : INFO : PROGRESS: at 79.64% examples, 639235 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:01:21,386 : INFO : PROGRESS: at 81.25% examples, 640803 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-25 15:01:22,396 : INFO : PROGRESS: at 82.94% examples, 643075 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:01:23,414 : INFO : PROGRESS: at 83.56% examples, 636799 words/s, in_qsize 8, out_qsize 1\n",
      "2018-01-25 15:01:24,428 : INFO : PROGRESS: at 84.72% examples, 635024 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:01:25,429 : INFO : PROGRESS: at 86.27% examples, 636073 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:01:26,435 : INFO : PROGRESS: at 87.78% examples, 636727 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:01:27,454 : INFO : PROGRESS: at 88.99% examples, 635188 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:01:28,455 : INFO : PROGRESS: at 90.29% examples, 634426 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:01:29,456 : INFO : PROGRESS: at 91.50% examples, 633256 words/s, in_qsize 6, out_qsize 1\n",
      "2018-01-25 15:01:30,460 : INFO : PROGRESS: at 92.92% examples, 633614 words/s, in_qsize 8, out_qsize 1\n",
      "2018-01-25 15:01:31,461 : INFO : PROGRESS: at 94.17% examples, 632615 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-25 15:01:32,468 : INFO : PROGRESS: at 95.25% examples, 630644 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:01:33,490 : INFO : PROGRESS: at 95.85% examples, 625401 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-25 15:01:34,501 : INFO : PROGRESS: at 96.96% examples, 623446 words/s, in_qsize 8, out_qsize 1\n",
      "2018-01-25 15:01:35,502 : INFO : PROGRESS: at 98.34% examples, 623549 words/s, in_qsize 6, out_qsize 1\n",
      "2018-01-25 15:01:36,506 : INFO : PROGRESS: at 99.94% examples, 624764 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-25 15:01:36,517 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-01-25 15:01:36,524 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-25 15:01:36,532 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-25 15:01:36,538 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-25 15:01:36,539 : INFO : training on 62511700 raw words (45597626 effective words) took 73.0s, 624833 effective words/s\n"
     ]
    }
   ],
   "source": [
    "result = build_dict(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents_pos = result['tok_sents_pos']\n",
    "model = result['model']\n",
    "train_x_pos = sentence2idx(sents_pos, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-25 15:02:16,472 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('decent', 0.7660684585571289),\n",
       " ('great', 0.7152284383773804),\n",
       " ('bad', 0.6988692283630371),\n",
       " ('cool', 0.6371133327484131),\n",
       " ('nice', 0.6201329231262207),\n",
       " ('solid', 0.603621244430542),\n",
       " ('terrific', 0.599502444267273),\n",
       " ('fine', 0.5884516835212708),\n",
       " ('mediocre', 0.5779147148132324),\n",
       " ('interesting', 0.5721300840377808)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents_neg = result['tok_sents_neg']\n",
    "train_x_neg = sentence2idx(sents_neg, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = train_x_pos + train_x_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = [1] * len(train_x_pos) + [0] * len(train_x_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_path = '/Users/lifa08/Documents/Lifa/Machine_Learning/Miniproject_test/aclImdb/test'\n",
    "test_sents_pos = read_dataset(test_path+'/pos/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok_test_sents_pos = [nltk.word_tokenize(sent) for sent in test_sents_pos]\n",
    "test_x_pos = sentence2idx(tok_test_sents_pos, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sents_neg = read_dataset(test_path+'/neg/')\n",
    "tok_test_sents_neg = [nltk.word_tokenize(sent) for sent in test_sents_neg]\n",
    "test_x_neg = sentence2idx(tok_test_sents_neg, model)\n",
    "test_x = test_x_pos + test_x_neg\n",
    "test_y = [1] * len(test_x_pos) + [0] * len(test_x_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('/Users/lifa08/Documents/Lifa/Machine_Learning/miniproject/gensim/gensim_imdb.pkl', 'wb')\n",
    "pkl.dump((train_x, train_y), f, -1)\n",
    "pkl.dump((test_x, test_y), f, -1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('/Users/lifa08/Documents/Lifa/Machine_Learning/miniproject/gensim/gensim_imdb.pkl', 'rb')\n",
    "train_set = pkl.load(f)\n",
    "test_set = pkl.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Wemb = create_Wemb(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('/Users/lifa08/Documents/Lifa/Machine_Learning/miniproject/gensim/gensim_imdb_Wemb.pkl', 'wb')\n",
    "pkl.dump(Wemb, f, -1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('/Users/lifa08/Documents/Lifa/Machine_Learning/miniproject/gensim/gensim_imdb_Wemb.pkl', 'rb')\n",
    "Wemb_file = pkl.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.09758687  1.05043387  0.39202166 ..., -0.38976276 -0.46508232\n",
      "  -0.3893936 ]\n",
      " [ 0.19986108 -0.11645149  0.10571738 ...,  0.00534131 -0.48626673\n",
      "  -0.71969342]\n",
      " [-0.19283009  0.90497458 -0.1725197  ...,  0.14764518  0.19503661\n",
      "  -0.82979882]\n",
      " ..., \n",
      " [-0.02627246  0.01910434  0.01541281 ...,  0.01297759 -0.03586807\n",
      "   0.00416359]\n",
      " [ 0.02873666 -0.04096749  0.00623717 ..., -0.02622253  0.01189386\n",
      "   0.00886645]\n",
      " [-0.03427199 -0.00707456  0.02952876 ..., -0.04402973 -0.01717805\n",
      "   0.01733459]]\n"
     ]
    }
   ],
   "source": [
    "print(Wemb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.09758687  1.05043387  0.39202166 ..., -0.38976276 -0.46508232\n",
      "  -0.3893936 ]\n",
      " [ 0.19986108 -0.11645149  0.10571738 ...,  0.00534131 -0.48626673\n",
      "  -0.71969342]\n",
      " [-0.19283009  0.90497458 -0.1725197  ...,  0.14764518  0.19503661\n",
      "  -0.82979882]\n",
      " ..., \n",
      " [-0.02627246  0.01910434  0.01541281 ...,  0.01297759 -0.03586807\n",
      "   0.00416359]\n",
      " [ 0.02873666 -0.04096749  0.00623717 ..., -0.02622253  0.01189386\n",
      "   0.00886645]\n",
      " [-0.03427199 -0.00707456  0.02952876 ..., -0.04402973 -0.01717805\n",
      "   0.01733459]]\n"
     ]
    }
   ],
   "source": [
    "print(Wemb_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_gensim_w2vec(path):\n",
    "    # path = '/Users/lifa08/Documents/Lifa/Machine_Learning/Miniproject_test/aclImdb/'\n",
    "    result = build_dict(path+'train')\n",
    "    model = result['model']\n",
    "\n",
    "    sents_pos = result['tok_sents_pos']\n",
    "    train_x_pos = sentence2idx(sents_pos, model)\n",
    "\n",
    "    sents_neg = result['tok_sents_neg']\n",
    "    train_x_neg = sentence2idx(sents_neg, model)\n",
    "    train_x = train_x_pos + train_x_neg\n",
    "    train_y = [1] * len(train_x_pos) + [0] * len(train_x_neg)\n",
    "\n",
    "    test_sents_pos = read_dataset(path+'test/pos/')\n",
    "    tok_test_sents_pos = [nltk.word_tokenize(sent) for sent in test_sents_pos]\n",
    "    test_x_pos = sentence2idx(tok_test_sents_pos, model)\n",
    "\n",
    "    test_sents_neg = read_dataset(path+'test/neg/')\n",
    "    tok_test_sents_neg = [nltk.word_tokenize(sent) for sent in test_sents_neg]\n",
    "    test_x_neg = sentence2idx(tok_test_sents_neg, model)\n",
    "    test_x = test_x_pos + test_x_neg\n",
    "    test_y = [1] * len(test_x_pos) + [0] * len(test_x_neg)\n",
    "\n",
    "    f = open('gensim/gensim_imdb.pkl', 'wb')\n",
    "    pkl.dump((train_x, train_y), f, -1)\n",
    "    pkl.dump((test_x, test_y), f, -1)\n",
    "    f.close()\n",
    "\n",
    "    Wemb = create_Wemb(model)\n",
    "    f = open('gensim/gensim_imdb_Wemb.pkl', 'wb')\n",
    "    pkl.dump(Wemb, f, -1)\n",
    "    f.close()\n",
    "\n",
    "    model.save('gensim/imdb_gensim_w2vmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lifa08/Documents/Lifa/Machine_Learning/Miniproject_test/aclImdb/train/pos\n"
     ]
    }
   ],
   "source": [
    "currdir = os.getcwd()\n",
    "print(currdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-20 22:30:30,011 : INFO : collecting all words and their counts\n",
      "2018-01-20 22:30:30,013 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-01-20 22:30:30,017 : INFO : collected 286 word types from a corpus of 532 raw words and 4 sentences\n",
      "2018-01-20 22:30:30,020 : INFO : Loading a fresh vocabulary\n",
      "2018-01-20 22:30:30,023 : INFO : min_count=1 retains 286 unique words (100% of original 286, drops 0)\n",
      "2018-01-20 22:30:30,024 : INFO : min_count=1 leaves 532 word corpus (100% of original 532, drops 0)\n",
      "2018-01-20 22:30:30,027 : INFO : deleting the raw counts dictionary of 286 items\n",
      "2018-01-20 22:30:30,029 : INFO : sample=0.001 downsamples 64 most-common words\n",
      "2018-01-20 22:30:30,031 : INFO : downsampling leaves estimated 351 word corpus (66.0% of prior 532)\n",
      "2018-01-20 22:30:30,032 : INFO : estimated required memory for 286 words and 128 dimensions: 435864 bytes\n",
      "2018-01-20 22:30:30,035 : INFO : resetting layer weights\n",
      "2018-01-20 22:30:30,041 : INFO : training model with 4 workers on 286 vocabulary and 128 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-01-20 22:30:30,045 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-01-20 22:30:30,046 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-20 22:30:30,049 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-20 22:30:30,051 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-20 22:30:30,054 : INFO : training on 2660 raw words (1725 effective words) took 0.0s, 176909 effective words/s\n",
      "2018-01-20 22:30:30,057 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-01-20 22:30:30,176 : INFO : saving Word2Vec object under gensim/imdb_gensim_vmodel, separately None\n",
      "2018-01-20 22:30:30,180 : INFO : not storing attribute syn0norm\n",
      "2018-01-20 22:30:30,185 : INFO : not storing attribute cum_table\n",
      "2018-01-20 22:30:30,192 : INFO : saved gensim/imdb_gensim_vmodel\n"
     ]
    }
   ],
   "source": [
    "train_gensim_w2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_idxwemb_wordemb(sentence):\n",
    "    model = gensim.models.Word2Vec.load('gensim/imdb_gensim_vmodel')\n",
    "    sentence_words = []\n",
    "    for idx, x in enumerate(sentence):\n",
    "        sentence_words.append(model.wv[model.wv.index2word[x]])\n",
    "\n",
    "    f = open('gensim/gensim_imdb_Wemb.pkl', 'rb')\n",
    "    Wemb = pkl.load(f)\n",
    "    f.close()\n",
    "    emb_x = Wemb[sentence]\n",
    "    \n",
    "    return (numpy.matrix(sentence_words)==numpy.matrix(emb_x)).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
