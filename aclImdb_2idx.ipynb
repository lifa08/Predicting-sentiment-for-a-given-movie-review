{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dictionary from the textual aclImdb dataset and convert the textual dataset to numerical dataset containing the ids of the words in the textual dataset.\n",
    "Borrowed from http://deeplearning.net/tutorial/code/imdb_preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pickle as pkl\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from subprocess import Popen, PIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(sentences):\n",
    "    print('Tokenizing..')\n",
    "    text = \"\\n\".join(sentences)\n",
    "\n",
    "    tokenizer = Popen(tokenizer_cmd, stdin=PIPE, stdout=PIPE)\n",
    "    tok_text, _ = tokenizer.communicate(bytes(text,'utf-8'))\n",
    "    toks = tok_text.decode('utf-8').split('\\n')[:-1]\n",
    "    return toks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer script is downloaded from https://github.com/moses-smt/mosesdecoder/tree/master/scripts/tokenizer.\n",
    "\n",
    "**Note:** need to downloaded relevant nonbreaking_prefixes from https://github.com/moses-smt/mosesdecoder/tree/master/scripts/share/nonbreaking_prefixes. If only english is needed, downloading nonbreaking_prefix.en is enough.\n",
    "\n",
    "**Mose tokenizer add a space in front of marks (e.g. '.', '!')**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing..\n",
      "['For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem . Imagine a movie where Joe Piscopo is actually funny ! Maureen Stapleton is a scene stealer .', 'The Moroni character is an absolute scream . Watch for Alan &quot; The Skipper &quot; Hale jr. as a police Sgt.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer_cmd = ['./script/tokenizer.perl', '-l', 'en', '-q', '-' ] \n",
    "text = ['For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. \\\n",
    "Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer.',\n",
    "'The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt.']\n",
    "testOutput = tokenize(text)\n",
    "print(testOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(path):\n",
    "    \"\"\"Build dictionary from the training texts.\"\"\"\n",
    "\n",
    "    texts = []\n",
    "    currdir = os.getcwd()\n",
    "    os.chdir('%s/pos/' % path)\n",
    "    for ff in glob.glob(\"*.txt\"):\n",
    "        with open(ff, 'r') as f:\n",
    "            texts.append(f.readline().strip())\n",
    "    os.chdir('%s/neg/' % path)\n",
    "    for ff in glob.glob(\"*.txt\"):\n",
    "        with open(ff, 'r') as f:\n",
    "            texts.append(f.readline().strip())\n",
    "    os.chdir(currdir)\n",
    "\n",
    "    sentences = tokenize(texts)\n",
    "\n",
    "    print('Building dictionary..')\n",
    "    wordcount = dict()\n",
    "    for ss in sentences:\n",
    "        words = ss.strip().lower().split()\n",
    "        for w in words:\n",
    "            if w not in wordcount:\n",
    "                wordcount[w] = 1\n",
    "            else:\n",
    "                wordcount[w] += 1\n",
    "\n",
    "    counts = list(wordcount.values())\n",
    "    keys = list(wordcount.keys())\n",
    "\n",
    "    sorted_idx = numpy.argsort(counts)[::-1]\n",
    "\n",
    "    worddict = dict()\n",
    "\n",
    "    for idx, ss in enumerate(sorted_idx):\n",
    "        worddict[keys[ss]] = idx+2  # leave 0 and 1 (UNK)\n",
    " \n",
    "    print(numpy.sum(counts), ' total words ', len(keys), ' unique words')\n",
    "\n",
    "    return worddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing..\n",
      "Building dictionary..\n",
      "7113750  total words  101743  unique words\n"
     ]
    }
   ],
   "source": [
    "data_path = '/Users/lifa08/Local_documents/Machine_Learning/Miniproject_test/aclImdb/'\n",
    "dictionary = build_dict(os.path.join(data_path, 'train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_to_id(path, dictionary):\n",
    "    \"\"\" Convert text words into their corresponding ids in the dictionary\"\"\"\n",
    "    texts = []\n",
    "    currdir = os.getcwd()\n",
    "    os.chdir(path)\n",
    "    for ff in glob.glob(\"*.txt\"):\n",
    "        with open(ff, 'r') as f:\n",
    "            texts.append(f.readline().strip())\n",
    "    os.chdir(currdir)\n",
    "    \"\"\"\n",
    "    seqs_text = []\n",
    "    sentenceCount = 0\n",
    "    textCount = 0\n",
    "    for text in texts:\n",
    "        #print(text)\n",
    "        words = tokenize_sentences(text)\n",
    "        seqs_sentence = []\n",
    "        for idx, ss in enumerate(words):\n",
    "            word = ss.strip().lower()\n",
    "            wordfound = False\n",
    "            for (k, v) in dictionary.items():\n",
    "                if(k == word):\n",
    "                    seqs_sentence.append(v)  \n",
    "                    wordfound = True\n",
    "                    break\n",
    "            if(wordfound == False):\n",
    "                seqs_sentence.append(1) \n",
    "            #sentenceCount += 1\n",
    "        seqs_text.append(seqs_sentence)\n",
    "        #textCount +=1\n",
    "        #if(textCount >= 2): break\n",
    "    print(seqs_text)\n",
    "    return seqs_text\n",
    "    \"\"\"\n",
    "    sentences = tokenize(texts)\n",
    "\n",
    "    seqs = [None] * len(sentences)\n",
    "    for idx, ss in enumerate(sentences):\n",
    "        words = ss.strip().lower().split()\n",
    "        seqs[idx] = [dictionary[w] if w in dictionary else 1 for w in words]\n",
    "\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing..\n",
      "(12500,)\n",
      "[23, 6, 25, 18, 225, 70, 1179, 50, 263, 35, 6, 189, 7, 902, 4332, 3514, 23, 17, 1520, 4, 834, 6, 25, 132, 882, 14843, 10, 178, 180, 40, 7230, 14931, 10, 6, 151, 20385, 4, 2, 33234, 120, 10, 45, 1531, 1967, 4, 121, 23, 1537, 19, 2, 18972, 19, 8530, 4407, 22, 6, 570, 7976]\n"
     ]
    }
   ],
   "source": [
    "train_x_pos = word_to_id(data_path+'train/pos', dictionary)\n",
    "print(numpy.array(train_x_pos).shape)\n",
    "print(train_x_pos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_ids(path):\n",
    "    dictionary = build_dict(os.path.join(path, 'train'))\n",
    "\n",
    "    train_x_pos = word_to_id(path+'train/pos', dictionary)\n",
    "    train_x_neg = word_to_id(path+'train/neg', dictionary)\n",
    "    train_x = train_x_pos + train_x_neg\n",
    "    train_y = [1] * len(train_x_pos) + [0] * len(train_x_neg)\n",
    "\n",
    "    test_x_pos = word_to_id(path+'test/pos', dictionary)\n",
    "    test_x_neg = word_to_id(path+'test/neg', dictionary)\n",
    "    test_x = test_x_pos + test_x_neg\n",
    "    test_y = [1] * len(test_x_pos) + [0] * len(test_x_neg)\n",
    "\n",
    "    f = open('imdb.pkl', 'wb')\n",
    "    pkl.dump((train_x, train_y), f, -1)\n",
    "    pkl.dump((test_x, test_y), f, -1)\n",
    "    f.close()\n",
    "\n",
    "    f = open('imdb.dict.pkl', 'wb')\n",
    "    pkl.dump(dictionary, f, -1)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing..\n",
      "Building dictionary..\n",
      "7113750  total words  101743  unique words\n",
      "Tokenizing..\n",
      "Tokenizing..\n",
      "Tokenizing..\n",
      "Tokenizing..\n"
     ]
    }
   ],
   "source": [
    "text_to_ids(data_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
