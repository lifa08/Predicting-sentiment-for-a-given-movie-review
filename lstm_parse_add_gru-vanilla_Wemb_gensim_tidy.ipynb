{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: integrate the word2vect to the first method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjYAAAIXCAYAAACPRhSmAAAgAElEQVR4nO3dfWxUd57v+e/Onb47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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"LSTM_Wordembedding.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import six.moves.cPickle as pickle\n",
    "\n",
    "from collections import OrderedDict\n",
    "import sys\n",
    "import time\n",
    "import numpy\n",
    "\n",
    "import os\n",
    "import theano\n",
    "from theano import config\n",
    "import theano.tensor as tensor\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "import imdb\n",
    "import Wemb_gensim\n",
    "\n",
    "import pickle as pkl\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets = {'imdb': (imdb.load_data, imdb.prepare_data)}\n",
    "SEED = 123\n",
    "numpy.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adadelta(lr, tparams, grads, x, mask, y, cost):\n",
    "    \"\"\"\n",
    "    An adaptive learning rate optimizer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : Theano SharedVariable\n",
    "        Initial learning rate\n",
    "    tpramas: Theano SharedVariable\n",
    "        Model parameters\n",
    "    grads: Theano variable\n",
    "        Gradients of cost w.r.t to parameres\n",
    "    x: Theano variable\n",
    "        Model inputs\n",
    "    mask: Theano variable\n",
    "        Sequence mask\n",
    "    y: Theano variable\n",
    "        Targets\n",
    "    cost: Theano variable\n",
    "        Objective fucntion to minimize\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    For more information, see [ADADELTA]_.\n",
    "\n",
    "    .. [ADADELTA] Matthew D. Zeiler, *ADADELTA: An Adaptive Learning\n",
    "       Rate Method*, arXiv:1212.5701.\n",
    "    \"\"\"\n",
    "\n",
    "    zipped_grads = [theano.shared(p.get_value() * numpy_floatX(0.),\n",
    "                                  name='%s_grad' % k)\n",
    "                    for k, p in tparams.items()]\n",
    "    running_up2 = [theano.shared(p.get_value() * numpy_floatX(0.),\n",
    "                                 name='%s_rup2' % k)\n",
    "                   for k, p in tparams.items()]\n",
    "    running_grads2 = [theano.shared(p.get_value() * numpy_floatX(0.),\n",
    "                                    name='%s_rgrad2' % k)\n",
    "                      for k, p in tparams.items()]\n",
    "\n",
    "    zgup = [(zg, g) for zg, g in zip(zipped_grads, grads)]\n",
    "    rg2up = [(rg2, 0.95 * rg2 + 0.05 * (g ** 2))\n",
    "             for rg2, g in zip(running_grads2, grads)]\n",
    "\n",
    "    f_grad_shared = theano.function([x, mask, y], cost, updates=zgup + rg2up,\n",
    "                                    name='adadelta_f_grad_shared')\n",
    "\n",
    "    updir = [-tensor.sqrt(ru2 + 1e-6) / tensor.sqrt(rg2 + 1e-6) * zg\n",
    "             for zg, ru2, rg2 in zip(zipped_grads,\n",
    "                                     running_up2,\n",
    "                                     running_grads2)]\n",
    "    ru2up = [(ru2, 0.95 * ru2 + 0.05 * (ud ** 2))\n",
    "             for ru2, ud in zip(running_up2, updir)]\n",
    "    param_up = [(p, p + ud) for p, ud in zip(tparams.values(), updir)]\n",
    "\n",
    "    f_update = theano.function([lr], [], updates=ru2up + param_up,\n",
    "                               on_unused_input='ignore',\n",
    "                               name='adadelta_f_update')\n",
    "\n",
    "    return f_grad_shared, f_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ortho_weight(ndim):\n",
    "    W = numpy.random.randn(ndim, ndim)\n",
    "    u, s, v = numpy.linalg.svd(W)\n",
    "    return u.astype(config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _p(pp, name):\n",
    "    return '%s_%s' % (pp, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the parameters of lstm layer\n",
    "def param_init_lstm(options, params, prefix='lstm'):\n",
    "    \"\"\"\n",
    "    Init the LSTM parameter:\n",
    "\n",
    "    :see: init_params\n",
    "    \"\"\"\n",
    "    W = numpy.concatenate([ortho_weight(options['dim_proj']),\n",
    "                           ortho_weight(options['dim_proj']),\n",
    "                           ortho_weight(options['dim_proj']),\n",
    "                           ortho_weight(options['dim_proj'])], axis=1)\n",
    "    params[_p(prefix, 'W')] = W # _p is a function to concate prefix and W --> lstm_W\n",
    "    U = numpy.concatenate([ortho_weight(options['dim_proj']),\n",
    "                           ortho_weight(options['dim_proj']),\n",
    "                           ortho_weight(options['dim_proj']),\n",
    "                           ortho_weight(options['dim_proj'])], axis=1)\n",
    "    params[_p(prefix, 'U')] = U\n",
    "    b = numpy.zeros((4 * options['dim_proj'],))\n",
    "    params[_p(prefix, 'b')] = b.astype(config.floatX)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# state_below is passed in the emb --> the weights in the first layer ? why not X\n",
    "# because emb is the embedding of words which represents words\n",
    "def lstm_layer(tparams, state_below, options, prefix='lstm', mask=None):\n",
    "    nsteps = state_below.shape[0] # time steps namely, the number of words in a sentence\n",
    "    if state_below.ndim == 3:\n",
    "        n_samples = state_below.shape[1] # the number of sentences in a minibatch\n",
    "    else:\n",
    "        n_samples = 1 # in the case of no minibach applied, then train with one sentence at a time\n",
    "\n",
    "    assert mask is not None # sequence mask must be provided\n",
    "\n",
    "    def _slice(_x, n, dim):\n",
    "        if _x.ndim == 3:\n",
    "            return _x[:, :, n * dim:(n + 1) * dim]\n",
    "        return _x[:, n * dim:(n + 1) * dim]\n",
    "\n",
    "    # m_: mask, x_: state_below which is emb * lstm_W + lstm_b\n",
    "    def _step(m_, x_, h_, c_):\n",
    "        preact = tensor.dot(h_, tparams[_p(prefix, 'U')]) # lstm_U*h_t-1\n",
    "        preact += x_ # Wemb * lstm_W + lstm_b + lstm_U*h_t-1\n",
    "\n",
    "        i = tensor.nnet.sigmoid(_slice(preact, 0, options['dim_proj']))\n",
    "        f = tensor.nnet.sigmoid(_slice(preact, 1, options['dim_proj']))\n",
    "        o = tensor.nnet.sigmoid(_slice(preact, 2, options['dim_proj']))\n",
    "        c = tensor.tanh(_slice(preact, 3, options['dim_proj']))\n",
    "\n",
    "        c = f * c_ + i * c # c_ means previous state refer to s[t]= f*s[t-1] + i*s[t]\n",
    "        c = m_[:, None] * c + (1. - m_)[:, None] * c_ # c is of shape (minibatch maxlen, number of sentences\n",
    "                                                    # in a minibatch, word embbdeding size) = (98, 16, 4)\n",
    "        # c = theano.printing.Print('c')(c)\n",
    "\n",
    "        h = o * tensor.tanh(c)\n",
    "        h = m_[:, None] * h + (1. - m_)[:, None] * h_\n",
    "\n",
    "        return h, c\n",
    "\n",
    "    # emb * lstm_W + lstm_b\n",
    "    state_below = (tensor.dot(state_below, tparams[_p(prefix, 'W')]) +\n",
    "                   tparams[_p(prefix, 'b')])\n",
    "\n",
    "    dim_proj = options['dim_proj']\n",
    "    # scan function, sequence is the input x, nonsequence is ussually not iterated such as w and b\n",
    "    rval, updates = theano.scan(_step,\n",
    "                                sequences=[mask, state_below],\n",
    "                                outputs_info=[tensor.alloc(numpy_floatX(0.),\n",
    "                                                           n_samples,\n",
    "                                                           dim_proj),\n",
    "                                              tensor.alloc(numpy_floatX(0.),\n",
    "                                                           n_samples,\n",
    "                                                           dim_proj)],\n",
    "                                name=_p(prefix, '_layers'),\n",
    "                                n_steps=nsteps) # repeat as the number of words in a sentence\n",
    "    return rval[0] # rval = h which is of shape (n_samples, dim_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gru_layer(tparams, state_below, options, prefix='gru', mask=None):\n",
    "    nsteps = state_below.shape[0]\n",
    "    if state_below.ndim == 3:\n",
    "        n_samples = state_below.shape[1]\n",
    "    else:\n",
    "        n_samples = 1\n",
    "\n",
    "    assert mask is not None\n",
    "\n",
    "    def _slice(_x, n, dim):\n",
    "        if _x.ndim == 3:\n",
    "            return _x[:, :, n * dim:(n + 1) * dim]\n",
    "        return _x[:, n * dim:(n + 1) * dim]\n",
    "    \n",
    "    def x_seperate(_x, dim):\n",
    "        if _x.ndim == 3:\n",
    "            return _x[:, :, 0: 2 * dim], _x[:, :, 2 * dim:]\n",
    "        return _x[:, 0: 2 * dim], _x[:, 2 * dim:]\n",
    "\n",
    "    def _step(m_, x_, h_):\n",
    "        _x12, _x3 = x_seperate(x_, model_options['dim_proj'])\n",
    "\n",
    "        preact = tensor.dot(h_, tparams[_p(prefix, 'U')])\n",
    "        preact += _x12\n",
    "\n",
    "        m = tensor.nnet.sigmoid(preact)\n",
    "        r = _slice(m, 0, model_options['dim_proj'])\n",
    "        u = _slice(m, 1, model_options['dim_proj'])\n",
    "\n",
    "        _h = tensor.tanh(_x3 + tensor.dot(r * h_, tparams[_p(prefix, 'W_hh')]))\n",
    "        h = u * h_ + (1.0 - u) * _h\n",
    "\n",
    "        h = m_[:, None] * h + (1. - m_)[:, None] * h_\n",
    "\n",
    "        return h\n",
    "\n",
    "    state_below = (tensor.dot(state_below, tparams[_p(prefix, 'W')]) +\n",
    "                   tparams[_p(prefix, 'b')])\n",
    "\n",
    "    dim_proj = options['dim_proj']\n",
    "    initial_hidden_vector = tensor.alloc(numpy_floatX(0.), n_samples, dim_proj)\n",
    "\n",
    "    rval, updates = theano.scan(_step,\n",
    "                                sequences=[mask, state_below],\n",
    "                                outputs_info=[initial_hidden_vector],\n",
    "                                name=_p(prefix, '_layers'),\n",
    "                                n_steps=nsteps)\n",
    "    return rval[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the parameters of gru layer\n",
    "def param_init_gru(options, params, prefix='gru'):\n",
    "    \"\"\"\n",
    "    Init the GRU parameter:\n",
    "\n",
    "    :see: init_params\n",
    "    \"\"\"\n",
    "    W = numpy.concatenate([ortho_weight(options['dim_proj']),\n",
    "                           ortho_weight(options['dim_proj']),\n",
    "                           ortho_weight(options['dim_proj'])], axis=1)\n",
    "    params[_p(prefix, 'W')] = W\n",
    "        \n",
    "    U = numpy.concatenate([ortho_weight(options['dim_proj']),\n",
    "                           ortho_weight(options['dim_proj'])], axis=1)\n",
    "    params[_p(prefix, 'U')] = U\n",
    "\n",
    "    W_hh = ortho_weight(options['dim_proj'])\n",
    "    params[_p(prefix, 'W_hh')] = W_hh\n",
    "\n",
    "    b = numpy.zeros((3 * options['dim_proj'],))\n",
    "    params[_p(prefix, 'b')] = b.astype(config.floatX)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vanilla_layer(tparams, state_below, options, prefix='vanilla', mask=None):\n",
    "    nsteps = state_below.shape[0]\n",
    "    if state_below.ndim == 3:\n",
    "        n_samples = state_below.shape[1]\n",
    "    else:\n",
    "        n_samples = 1\n",
    "\n",
    "    assert mask is not None\n",
    "\n",
    "    def _step(m_, x_, h_):\n",
    "        a = x_ + tensor.dot(h_, tparams[_p(prefix, 'W')])\n",
    "        h = tensor.tanh(a)\n",
    "        h = m_[:, None] * h + (1. - m_)[:, None] * h_\n",
    "        return h\n",
    "\n",
    "    state_below = (tensor.dot(state_below, tparams[_p(prefix, 'U')]) +\n",
    "                   tparams[_p(prefix, 'b')])\n",
    "\n",
    "    dim_proj = options['dim_proj']\n",
    "    initial_hidden_vector = tensor.alloc(numpy_floatX(0.), n_samples, dim_proj)\n",
    "\n",
    "    rval, updates = theano.scan(_step,\n",
    "                                sequences=[mask, state_below],\n",
    "                                outputs_info=[initial_hidden_vector],\n",
    "                                name=_p(prefix, '_layers'),\n",
    "                                n_steps=nsteps)\n",
    "    return rval[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def param_init_vanilla(options, params, prefix='vanilla'):\n",
    "    \"\"\"\n",
    "    Init the vanilla RNN parameter:\n",
    "\n",
    "    :see: init_params\n",
    "    \"\"\"\n",
    "    ndim = options['dim_proj']\n",
    "    W_bound = numpy.sqrt(6. / (ndim + ndim))\n",
    "\n",
    "    U = numpy.asarray(numpy.random.uniform(low=-W_bound, high=W_bound, size=(ndim, ndim)))\n",
    "    params[_p(prefix, 'U')] = U.astype(config.floatX)\n",
    "    \n",
    "    W = numpy.asarray(numpy.random.uniform(low=-W_bound, high=W_bound, size=(ndim, ndim)))\n",
    "    params[_p(prefix, 'W')] = W.astype(config.floatX)\n",
    "\n",
    "    b = numpy.zeros(options['dim_proj'],)\n",
    "    params[_p(prefix, 'b')] = b.astype(config.floatX)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ff: Feed Forward (normal neural net), only useful to put after lstm\n",
    "#     before the classifier.\n",
    "layers = {'lstm': (param_init_lstm, lstm_layer), \\\n",
    "         'gru': (param_init_gru, gru_layer), \\\n",
    "         'vanilla': (param_init_vanilla, vanilla_layer)}\n",
    "\n",
    "def get_layer(name):\n",
    "    fns = layers[name]\n",
    "    return fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_params(options):\n",
    "    \"\"\"\n",
    "    Global (not LSTM) parameters. For the embeding and the classifier.\n",
    "    \"\"\"\n",
    "    params = OrderedDict()\n",
    "\n",
    "    # embedding layer\n",
    "    if(options['gensim_Wemb']):\n",
    "        f = open('/Users/lifa08/Local_documents/Machine_Learning/miniproject/gensim/gensim_imdb_Wemb.pkl', 'rb')\n",
    "        Wemb = pkl.load(f)\n",
    "        f.close()\n",
    "\n",
    "        params['Wemb'] = (Wemb[:options['n_words']]).astype(config.floatX)\n",
    "        # print(params['Wemb'].shape)\n",
    "        assert params['Wemb'].shape[0] == options['n_words']\n",
    "        assert params['Wemb'].shape[1] == options['dim_proj']\n",
    "    else:\n",
    "        print('wemb random initialization')\n",
    "        randn = numpy.random.rand(options['n_words'],\n",
    "                                  options['dim_proj']) # n_words = vocabulary size, dim_proj = word embedding size\n",
    "        params['Wemb'] = (0.01 * randn).astype(config.floatX)\n",
    "\n",
    "    # lstm layer\n",
    "    params = get_layer(options['encoder'])[0](options, params, prefix=options['encoder'])\n",
    "\n",
    "    # classifier\n",
    "    params['U'] = 0.01 * numpy.random.randn(options['dim_proj'],\n",
    "                                            options['ydim']).astype(config.floatX)\n",
    "    params['b'] = numpy.zeros((options['ydim'],)).astype(config.floatX)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_tparams(params):\n",
    "    tparams = OrderedDict()\n",
    "    for kk, pp in params.items():\n",
    "        tparams[kk] = theano.shared(params[kk], name=kk)\n",
    "    return tparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# state_before is proj namely hidden state\n",
    "def dropout_layer(state_before, use_noise, trng):\n",
    "    proj = tensor.switch(use_noise,\n",
    "                         (state_before *\n",
    "                          trng.binomial(state_before.shape,\n",
    "                                        p=0.5, n=1,\n",
    "                                        dtype=state_before.dtype)),\n",
    "                         state_before * 0.5)\n",
    "    return proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numpy_floatX(data):\n",
    "    return numpy.asarray(data, dtype=config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(tparams, options):\n",
    "    trng = RandomStreams(SEED)\n",
    "\n",
    "    # Used for dropout.\n",
    "    use_noise = theano.shared(numpy_floatX(0.)) # --> [0], thus this line of code is the same as use_noise=[0]\n",
    "\n",
    "    x = tensor.matrix('x', dtype='int64') # x is a sentence with each word form a row?\n",
    "    mask = tensor.matrix('mask', dtype=config.floatX) # sequence mask\n",
    "    y = tensor.vector('y', dtype='int64')\n",
    "\n",
    "    n_timesteps = x.shape[0] # number of words per sentence\n",
    "    n_samples = x.shape[1] # number of sentences in a minibatch\n",
    "\n",
    "    # x.flatten() --> number_timesteps * n_samples\n",
    "    emb = tparams['Wemb'][x.flatten()].reshape([n_timesteps,\n",
    "                                                n_samples,\n",
    "                                                options['dim_proj']]) # each word in the sentence\n",
    "                                                                      # get different weights\n",
    "\n",
    "    # call get_layer(options['encoder'])[1] --> lstm_layer --> lstm_layer(tparams,...,)\n",
    "    # here emb is used to initialize states_below in lstm_layer function\n",
    "    proj = get_layer(options['encoder'])[1](tparams, emb, options,\n",
    "                                            prefix=options['encoder'],\n",
    "                                            mask=mask)\n",
    "\n",
    "    projshape = theano.printing.Print('proj')(proj.shape)\n",
    "    # mean over word hidden representations in a sentence, proj is of shape(n_samples, dim_proj)\n",
    "    if options['encoder'] == 'lstm' or options['encoder'] == 'gru':\n",
    "        proj = (proj * mask[:, :, None]).sum(axis=0) # add up the hidden representations of all words in a sentence\n",
    "        proj = proj / mask.sum(axis=0)[:, None] # mask.sum(axis=0) is the length of sentence without padding\n",
    "    if options['use_dropout']:\n",
    "        proj = dropout_layer(proj, use_noise, trng)\n",
    "\n",
    "    # last layer\n",
    "    pred = tensor.nnet.softmax(tensor.dot(proj, tparams['U']) + tparams['b'])\n",
    "    # pred is the probability of a sentence belonging to a class. It is of shape (n_samples, class)\n",
    "\n",
    "    f_pred_prob = theano.function([x, mask], pred, name='f_pred_prob')\n",
    "    f_pred = theano.function([x, mask], pred.argmax(axis=1), name='f_pred')\n",
    "\n",
    "    off = 1e-8\n",
    "    if pred.dtype == 'float16':\n",
    "        off = 1e-6\n",
    "\n",
    "    # get the probability of the right class and calculate its negative log. --> negative log likelihood\n",
    "    cost = -tensor.log(pred[tensor.arange(n_samples), y] + off).mean()\n",
    "\n",
    "    return use_noise, x, mask, y, f_pred_prob, f_pred, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatches_idx(n, minibatch_size, shuffle=False):\n",
    "    \"\"\"\n",
    "    Used to shuffle the dataset at each iteration.\n",
    "    \"\"\"\n",
    "\n",
    "    idx_list = numpy.arange(n, dtype=\"int32\")\n",
    "\n",
    "    if shuffle:\n",
    "        numpy.random.shuffle(idx_list)\n",
    "\n",
    "    minibatches = []\n",
    "    minibatch_start = 0\n",
    "    for i in range(n // minibatch_size):\n",
    "        minibatches.append(idx_list[minibatch_start:\n",
    "                                    minibatch_start + minibatch_size])\n",
    "        minibatch_start += minibatch_size\n",
    "\n",
    "    if (minibatch_start != n):\n",
    "        # Make a minibatch out of what is left\n",
    "        minibatches.append(idx_list[minibatch_start:])\n",
    "\n",
    "    return zip(range(len(minibatches)), minibatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unzip(zipped):\n",
    "    \"\"\"\n",
    "    When we pickle the model. Needed for the GPU stuff.\n",
    "    \"\"\"\n",
    "    new_params = OrderedDict()\n",
    "    for kk, vv in zipped.items():\n",
    "        new_params[kk] = vv.get_value()\n",
    "    return new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zipp(params, tparams):\n",
    "    \"\"\"\n",
    "    When we reload the model. Needed for the GPU stuff.\n",
    "    \"\"\"\n",
    "    for kk, vv in params.items():\n",
    "        tparams[kk].set_value(vv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pred_error(f_pred, prepare_data, data, iterator, verbose=False):\n",
    "    \"\"\"\n",
    "    Just compute the error\n",
    "    f_pred: Theano fct computing the prediction\n",
    "    prepare_data: usual prepare_data for that dataset.\n",
    "    \"\"\"\n",
    "    valid_err = 0\n",
    "    for _, valid_index in iterator:\n",
    "        x, mask, y = prepare_data([data[0][t] for t in valid_index],\n",
    "                                  numpy.array(data[1])[valid_index],\n",
    "                                  maxlen=None)\n",
    "        preds = f_pred(x, mask)\n",
    "        targets = numpy.array(data[1])[valid_index]\n",
    "        valid_err += (preds == targets).sum()\n",
    "    valid_err = 1. - numpy_floatX(valid_err) / len(data[0])\n",
    "\n",
    "    return valid_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dataset(name):\n",
    "    return datasets[name][0], datasets[name][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    encoder,\n",
    "    gensim_Wemb, # Use gensim word embedding or not\n",
    "    dim_proj=128,  # word embeding dimension and the number of hidden units of LSTM.\n",
    "    patience=10,  # Number of epoch to wait before early stop if no progress\n",
    "    max_epochs=100,  # The maximum number of epoch to run\n",
    "    dispFreq=10,  # Display to stdout the training progress every N updates\n",
    "    decay_c=0.,  # Weight decay for the classifier applied to the U weights.\n",
    "    lrate=0.0001,  # Learning rate for sgd (not used for adadelta and rmsprop)\n",
    "    n_words=100000,  # Vocabulary size actual size is about 114526\n",
    "    optimizer=adadelta,  # sgd, adadelta and rmsprop available, sgd very hard to use, not recommanded\n",
    "    saveto='lstm_model.npz',  # The best model will be saved there\n",
    "    validFreq=370,  # Compute the validation error after this number of update.\n",
    "    saveFreq=1110,  # Save the parameters after every saveFreq updates\n",
    "    maxlen=700,  # Sequence longer then this get ignored\n",
    "    batch_size=16,  # The batch size during training.\n",
    "    valid_batch_size=64,  # The batch size used for validation/test set.\n",
    "    dataset='imdb',\n",
    "    gensim_retrain=False, # retrain word2vec or not\n",
    "\n",
    "    # Parameter for extra option\n",
    "    noise_std=0.,\n",
    "    use_dropout=True,  # if False slightly faster, but worst test error\n",
    "                       # This frequently need a bigger model.\n",
    "    reload_model=None,  # Path to a saved model we want to start from.\n",
    "    test_size=-1,  # If >0, we keep only this number of test example.\n",
    "):\n",
    "    model_options = locals().copy()\n",
    "    if gensim_retrain:\n",
    "        path = '/Users/lifa08/Local_documents/Machine_Learning/Miniproject_test/aclImdb/'\n",
    "        print('Training gensim word2vec model')\n",
    "        Wemb_gensim.train_gensim_w2vec(path, Wemb_size=dim_proj, w2v_iter=20)\n",
    "\n",
    "    # Get the function names from imdb module\n",
    "    load_data, prepare_data = get_dataset(model_options['dataset'])\n",
    "\n",
    "    print('Loading data')\n",
    "    train, valid, test = load_data(\n",
    "        path='/Users/lifa08/Local_documents/Machine_Learning/miniproject/gensim/gensim_imdb.pkl',\n",
    "        n_words=n_words,\n",
    "        valid_portion=0.05)\n",
    "\n",
    "    if test_size > 0:\n",
    "        print(len(test[0]))\n",
    "        idx = numpy.arange(len(test[0])) # test[0] is test_x\n",
    "        numpy.random.shuffle(idx)\n",
    "        idx = idx[:test_size]\n",
    "        test = ([test[0][n] for n in idx], [test[1][n] for n in idx])\n",
    "\n",
    "    # Get the number of sentimental class\n",
    "    ydim = numpy.max(train[1]) + 1 # train_y = train[1]\n",
    "    model_options['ydim'] = ydim\n",
    "\n",
    "    print('Building model')\n",
    "    params = init_params(model_options)\n",
    "\n",
    "    # This create Theano Shared Variable from the parameters.\n",
    "    # Dict name (string) -> Theano Tensor Shared Variable\n",
    "    # params and tparams have different copy of the weights.\n",
    "    tparams = init_tparams(params)\n",
    "\n",
    "    (use_noise, x, mask,\n",
    "     y, f_pred_prob, f_pred, cost) = build_model(tparams, model_options)\n",
    "\n",
    "    # add weight decay\n",
    "    if decay_c > 0.:\n",
    "        decay_c = theano.shared(numpy_floatX(decay_c), name='decay_c')\n",
    "        weight_decay = 0.\n",
    "        weight_decay += (tparams['U'] ** 2).sum()\n",
    "        weight_decay *= decay_c\n",
    "        cost += weight_decay\n",
    "\n",
    "    # define cost that includes weight decay\n",
    "    f_cost = theano.function([x, mask, y], cost, name='f_cost')\n",
    "\n",
    "    # define gradient functions\n",
    "    grads = tensor.grad(cost, wrt=list(tparams.values()))\n",
    "    f_grad = theano.function([x, mask, y], grads, name='f_grad')\n",
    "\n",
    "    # define optmizer\n",
    "    lr = tensor.scalar(name='lr')\n",
    "    f_grad_shared, f_update = optimizer(lr, tparams, grads,\n",
    "                                        x, mask, y, cost)\n",
    "\n",
    "    print('Optimization')\n",
    "    kf_valid = get_minibatches_idx(len(valid[0]), valid_batch_size)\n",
    "    kf_test = get_minibatches_idx(len(test[0]), valid_batch_size)\n",
    "\n",
    "    print(\"%d train examples\" % len(train[0]))\n",
    "    print(\"%d valid examples\" % len(valid[0]))\n",
    "    print(\"%d test examples\" % len(test[0]))\n",
    "\n",
    "    history_errs = []\n",
    "    best_p = None # for storing best parameters\n",
    "    bad_count = 0\n",
    "\n",
    "    if validFreq == -1:\n",
    "        validFreq = len(train[0]) // batch_size\n",
    "    if saveFreq == -1:\n",
    "        saveFreq = len(train[0]) // batch_size\n",
    "\n",
    "    uidx = 0  # the number of update done\n",
    "    estop = False  # early stop\n",
    "\n",
    "    train_costs = numpy.array([]) # add for plot\n",
    "    train_accus = numpy.array([])\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        for eidx in range(max_epochs):\n",
    "            n_samples = 0 # the number of sentence\n",
    "\n",
    "            # Get new shuffled index for the training set.\n",
    "            kf = get_minibatches_idx(len(train[0]), batch_size, shuffle=True)\n",
    "\n",
    "            for _, train_index in kf:\n",
    "                uidx += 1\n",
    "                use_noise.set_value(1.) # going to use noise\n",
    "\n",
    "                # Select the random examples for this minibatch\n",
    "                y = [train[1][t] for t in train_index]\n",
    "                x = [train[0][t] for t in train_index] # wrap 16 sentences into an array \n",
    "                                            # as a result, x is of 2 dimensions, size of 1998/16 * 16 sentences\n",
    "                # Get the data in numpy.ndarray format\n",
    "                # This swap the axis!\n",
    "                # Return something of shape (minibatch maxlen, n samples)\n",
    "                # n_samples is the number of sentences in a minibatch\n",
    "                x, mask, y = prepare_data(x, y) # all the sentence will have the same length which is minibatch maxlen\n",
    "                n_samples += x.shape[1] # x.shape[1] is the number of sentence in a minibatch\n",
    "\n",
    "                cost = f_grad_shared(x, mask, y) # call the model\n",
    "                f_update(lrate)\n",
    "\n",
    "                accuracy = (f_pred(x, mask) == y).sum() / len(y)\n",
    "                train_costs = numpy.append(train_costs, cost)\n",
    "                train_accus = numpy.append(train_accus, accuracy)\n",
    "\n",
    "                if numpy.isnan(cost) or numpy.isinf(cost):\n",
    "                    print('bad cost detected: ', cost)\n",
    "                    break\n",
    "                if numpy.mod(uidx, dispFreq) == 0:\n",
    "                    print('Epoch ', eidx, 'Update ', uidx, 'Cost ', cost)\n",
    "                if saveto and numpy.mod(uidx, saveFreq) == 0:\n",
    "                    print('Saving...')\n",
    "\n",
    "                    if best_p is not None:\n",
    "                        params = best_p\n",
    "                    else:\n",
    "                        params = unzip(tparams)\n",
    "                        numpy.savez(saveto, history_errs=history_errs, **params)\n",
    "                        pickle.dump(model_options, open('%s.pkl' % saveto, 'wb'), -1)\n",
    "                        print('Done')\n",
    "                if numpy.mod(uidx, validFreq) == 0:\n",
    "                    use_noise.set_value(0.) # at validation stage, dont use noise\n",
    "                    train_err = pred_error(f_pred, prepare_data, train, kf)\n",
    "                    valid_err = pred_error(f_pred, prepare_data, valid, kf_valid)\n",
    "                    test_err = pred_error(f_pred, prepare_data, test, kf_test)\n",
    "\n",
    "                    kf_valid = get_minibatches_idx(len(valid[0]), valid_batch_size)\n",
    "                    kf_test = get_minibatches_idx(len(test[0]), valid_batch_size)\n",
    "\n",
    "                    history_errs.append([valid_err, test_err])\n",
    "\n",
    "                    if (best_p is None or\n",
    "                        valid_err <= numpy.array(history_errs)[:, 0].min()):\n",
    "\n",
    "                        best_p = unzip(tparams)\n",
    "                        bad_counter = 0\n",
    "\n",
    "                    print('Train ', train_err, 'Valid ', valid_err,\n",
    "                           'Test ', test_err)\n",
    "\n",
    "                    if (len(history_errs) > patience and\n",
    "                        valid_err >= numpy.array(history_errs)[:-patience,\n",
    "                                                               0].min()):\n",
    "                        bad_counter += 1\n",
    "                        if bad_counter > patience:\n",
    "                            print('Early Stop!')\n",
    "                            estop = True\n",
    "                            break\n",
    "\n",
    "            print('Seen %d samples' % n_samples)\n",
    "\n",
    "            if estop:\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interupted\")\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    if best_p is not None:\n",
    "        zipp(best_p, tparams)\n",
    "    else:\n",
    "        best_p = unzip(tparams)\n",
    "\n",
    "    use_noise.set_value(0.)\n",
    "    kf_train_sorted = get_minibatches_idx(len(train[0]), batch_size)\n",
    "    train_err = pred_error(f_pred, prepare_data, train, kf_train_sorted)\n",
    "    valid_err = pred_error(f_pred, prepare_data, valid, kf_valid)\n",
    "    test_err = pred_error(f_pred, prepare_data, test, kf_test)\n",
    "\n",
    "    print( 'Train ', train_err, 'Valid ', valid_err, 'Test ', test_err )\n",
    "    if saveto:\n",
    "        numpy.savez(saveto, train_err=train_err,\n",
    "                    valid_err=valid_err, test_err=test_err,\n",
    "                    history_errs=history_errs, **best_p)\n",
    "    print('The code run for %d epochs, with %f sec/epochs' % (\n",
    "        (eidx + 1), (end_time - start_time) / (1. * (eidx + 1))))\n",
    "    print( ('Training took %.1fs' %\n",
    "            (end_time - start_time)), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-05 17:26:58,137 : INFO : loading Word2Vec object from /Users/lifa08/Local_documents/Machine_Learning/miniproject/gensim/imdb_gensim_w2vmodel\n",
      "2018-07-05 17:26:58,466 : INFO : loading wv recursively from /Users/lifa08/Local_documents/Machine_Learning/miniproject/gensim/imdb_gensim_w2vmodel.wv.* with mmap=None\n",
      "2018-07-05 17:26:58,467 : INFO : loading syn0 from /Users/lifa08/Local_documents/Machine_Learning/miniproject/gensim/imdb_gensim_w2vmodel.wv.syn0.npy with mmap=None\n",
      "2018-07-05 17:26:58,508 : INFO : setting ignored attribute syn0norm to None\n",
      "2018-07-05 17:26:58,512 : INFO : loading syn1neg from /Users/lifa08/Local_documents/Machine_Learning/miniproject/gensim/imdb_gensim_w2vmodel.syn1neg.npy with mmap=None\n",
      "2018-07-05 17:26:58,582 : INFO : setting ignored attribute cum_table to None\n",
      "2018-07-05 17:26:58,583 : INFO : loaded /Users/lifa08/Local_documents/Machine_Learning/miniproject/gensim/imdb_gensim_w2vmodel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Optimization\n",
      "23750 train examples\n",
      "1250 valid examples\n",
      "25000 test examples\n",
      "Epoch  0 Update  10 Cost  0.701655720959929\n",
      "Epoch  0 Update  20 Cost  0.6805804526234278\n",
      "Epoch  0 Update  30 Cost  0.635947638173079\n",
      "Epoch  0 Update  40 Cost  0.7204430620624165\n",
      "Epoch  0 Update  50 Cost  0.6820114936783723\n",
      "Epoch  0 Update  60 Cost  0.6575556982707522\n",
      "Epoch  0 Update  70 Cost  0.6751729584743539\n",
      "Epoch  0 Update  80 Cost  0.73171882622664\n",
      "Epoch  0 Update  90 Cost  0.6831198804326808\n",
      "Epoch  0 Update  100 Cost  0.671047671736507\n",
      "Epoch  0 Update  110 Cost  0.5967023098253601\n",
      "Epoch  0 Update  120 Cost  0.6775253537167729\n",
      "Epoch  0 Update  130 Cost  0.6094848512099187\n",
      "Epoch  0 Update  140 Cost  0.4588260271018071\n",
      "Epoch  0 Update  150 Cost  0.579088558112826\n",
      "Epoch  0 Update  160 Cost  0.6174837591504063\n",
      "Epoch  0 Update  170 Cost  0.5415996328663981\n",
      "Epoch  0 Update  180 Cost  0.5775223142329229\n",
      "Epoch  0 Update  190 Cost  0.5630115471196382\n",
      "Epoch  0 Update  200 Cost  0.6885083202977145\n",
      "Epoch  0 Update  210 Cost  0.5640432488403169\n",
      "Epoch  0 Update  220 Cost  0.657262379321067\n",
      "Epoch  0 Update  230 Cost  0.6599754029584433\n",
      "Epoch  0 Update  240 Cost  0.49508947975664463\n",
      "Epoch  0 Update  250 Cost  0.5008017296323674\n",
      "Epoch  0 Update  260 Cost  0.34594233294728177\n",
      "Epoch  0 Update  270 Cost  0.4641798853686429\n",
      "Epoch  0 Update  280 Cost  0.6175365103023409\n",
      "Epoch  0 Update  290 Cost  0.5218749508814722\n",
      "Epoch  0 Update  300 Cost  0.4951979754985572\n",
      "Epoch  0 Update  310 Cost  0.4203179548824858\n",
      "Epoch  0 Update  320 Cost  0.5340639225496818\n",
      "Epoch  0 Update  330 Cost  0.460500562486246\n",
      "Epoch  0 Update  340 Cost  0.35087437989473647\n",
      "Epoch  0 Update  350 Cost  0.3674257761959693\n",
      "Epoch  0 Update  360 Cost  0.4780581430582246\n",
      "Epoch  0 Update  370 Cost  0.266332523723131\n",
      "Train  0.383831578947 Valid  0.1928 Test  0.18152\n",
      "Seen 5920 samples\n",
      "Epoch  1 Update  380 Cost  0.6343502264295696\n",
      "Epoch  1 Update  390 Cost  0.49319270288481565\n",
      "Epoch  1 Update  400 Cost  0.3171934624961156\n",
      "Epoch  1 Update  410 Cost  0.4118762163324794\n",
      "Epoch  1 Update  420 Cost  0.6061721438224436\n",
      "Epoch  1 Update  430 Cost  0.3580411387358728\n",
      "Epoch  1 Update  440 Cost  0.33230807154955977\n",
      "Epoch  1 Update  450 Cost  0.325791024890775\n",
      "Epoch  1 Update  460 Cost  0.3149343711041943\n",
      "Epoch  1 Update  470 Cost  0.3275877291706139\n",
      "Epoch  1 Update  480 Cost  0.6225199656500631\n",
      "Epoch  1 Update  490 Cost  0.4195407073017088\n",
      "Epoch  1 Update  500 Cost  0.33345335094844414\n",
      "Epoch  1 Update  510 Cost  0.5355489058497098\n",
      "Epoch  1 Update  520 Cost  0.2602429401766615\n",
      "Epoch  1 Update  530 Cost  0.27334577470218685\n",
      "Epoch  1 Update  540 Cost  0.3601196594213845\n",
      "Epoch  1 Update  550 Cost  0.42463082950890674\n",
      "Epoch  1 Update  560 Cost  0.5231584352661706\n",
      "Epoch  1 Update  570 Cost  0.7683396468168144\n",
      "Epoch  1 Update  580 Cost  0.5508396125554519\n",
      "Epoch  1 Update  590 Cost  0.3218450274642501\n",
      "Epoch  1 Update  600 Cost  0.30398782525274304\n",
      "Epoch  1 Update  610 Cost  0.24588292290243838\n",
      "Epoch  1 Update  620 Cost  0.29382053303182964\n",
      "Epoch  1 Update  630 Cost  0.31532772510137036\n",
      "Epoch  1 Update  640 Cost  0.8006323870086635\n",
      "Epoch  1 Update  650 Cost  0.6587066933121103\n",
      "Epoch  1 Update  660 Cost  0.2442302771428574\n",
      "Epoch  1 Update  670 Cost  0.42770888781398664\n",
      "Epoch  1 Update  680 Cost  0.7130252861857997\n",
      "Epoch  1 Update  690 Cost  0.5215620936158465\n",
      "Epoch  1 Update  700 Cost  0.3663801798787709\n",
      "Epoch  1 Update  710 Cost  0.37576321763068793\n",
      "Epoch  1 Update  720 Cost  0.2697565802895914\n",
      "Epoch  1 Update  730 Cost  0.3786012441427635\n",
      "Epoch  1 Update  740 Cost  0.7175459164785524\n",
      "Train  0.384673684211 Valid  0.2032 Test  0.18196\n",
      "Seen 5920 samples\n",
      "Epoch  2 Update  750 Cost  0.26023899516443133\n",
      "Epoch  2 Update  760 Cost  0.39440349472663055\n",
      "Epoch  2 Update  770 Cost  0.7101630434316217\n",
      "Epoch  2 Update  780 Cost  0.3149520395376785\n",
      "Epoch  2 Update  790 Cost  0.4978995125893445\n",
      "Epoch  2 Update  800 Cost  0.5526697355683422\n",
      "Epoch  2 Update  810 Cost  0.2396574093705016\n",
      "Epoch  2 Update  820 Cost  0.43650575716235174\n",
      "Epoch  2 Update  830 Cost  0.373531074106975\n",
      "Epoch  2 Update  840 Cost  0.4498917711342169\n",
      "Epoch  2 Update  850 Cost  0.32122729190101296\n",
      "Epoch  2 Update  860 Cost  0.23278798818404017\n",
      "Epoch  2 Update  870 Cost  0.5691475409951661\n",
      "Epoch  2 Update  880 Cost  0.40238056536807093\n",
      "Epoch  2 Update  890 Cost  0.3412778985354902\n",
      "Epoch  2 Update  900 Cost  0.11219318724412894\n",
      "Epoch  2 Update  910 Cost  0.22507966481504557\n",
      "Epoch  2 Update  920 Cost  0.3188527389316242\n",
      "Epoch  2 Update  930 Cost  0.5674923957971844\n",
      "Epoch  2 Update  940 Cost  0.36970049534185767\n",
      "Epoch  2 Update  950 Cost  0.5158127373450807\n",
      "Epoch  2 Update  960 Cost  0.3833970253623371\n",
      "Epoch  2 Update  970 Cost  0.40859610012949743\n",
      "Epoch  2 Update  980 Cost  0.19710316716050047\n",
      "Epoch  2 Update  990 Cost  0.20437102157120365\n",
      "Epoch  2 Update  1000 Cost  0.3397457856440059\n",
      "Epoch  2 Update  1010 Cost  0.32148996351532383\n",
      "Epoch  2 Update  1020 Cost  0.22754328494864104\n",
      "Epoch  2 Update  1030 Cost  0.17915278612558572\n",
      "Epoch  2 Update  1040 Cost  0.5671616371496291\n",
      "Epoch  2 Update  1050 Cost  0.25187133844171544\n",
      "Epoch  2 Update  1060 Cost  0.3237854849414326\n",
      "Epoch  2 Update  1070 Cost  0.3615152945160246\n",
      "Epoch  2 Update  1080 Cost  0.18936935992114098\n",
      "Epoch  2 Update  1090 Cost  0.2927944433984671\n",
      "Epoch  2 Update  1100 Cost  0.5172858021156811\n",
      "Epoch  2 Update  1110 Cost  0.38782081629466486\n",
      "Saving...\n",
      "Train  0.388 Valid  0.1984 Test  0.19592\n",
      "Seen 5920 samples\n",
      "Epoch  3 Update  1120 Cost  0.28046881055717837\n",
      "Epoch  3 Update  1130 Cost  0.18033465338416477\n",
      "Epoch  3 Update  1140 Cost  0.4078261355837178\n",
      "Epoch  3 Update  1150 Cost  0.34432608885978216\n",
      "Epoch  3 Update  1160 Cost  0.46230232971550034\n",
      "Epoch  3 Update  1170 Cost  0.4921596129242106\n",
      "Epoch  3 Update  1180 Cost  0.20442207972797993\n",
      "Epoch  3 Update  1190 Cost  0.2548930940007562\n",
      "Epoch  3 Update  1200 Cost  0.8859264837762116\n",
      "Epoch  3 Update  1210 Cost  0.228386114148556\n",
      "Epoch  3 Update  1220 Cost  0.15430893611058433\n",
      "Epoch  3 Update  1230 Cost  0.20379372117151215\n",
      "Epoch  3 Update  1240 Cost  0.18378580831701455\n",
      "Epoch  3 Update  1250 Cost  0.289022677517631\n",
      "Epoch  3 Update  1260 Cost  0.27872175476953615\n",
      "Epoch  3 Update  1270 Cost  0.23362027878528108\n",
      "Epoch  3 Update  1280 Cost  0.21374429891799415\n",
      "Epoch  3 Update  1290 Cost  0.4760303951840621\n",
      "Epoch  3 Update  1300 Cost  0.27644997031258334\n",
      "Epoch  3 Update  1310 Cost  0.16547001991310398\n",
      "Epoch  3 Update  1320 Cost  0.277804910122957\n",
      "Epoch  3 Update  1330 Cost  0.35113189911632775\n",
      "Epoch  3 Update  1340 Cost  0.3734860762505894\n",
      "Epoch  3 Update  1350 Cost  0.2847482454977396\n",
      "Epoch  3 Update  1360 Cost  0.46766955693065354\n",
      "Epoch  3 Update  1370 Cost  0.2338126051995472\n",
      "Epoch  3 Update  1380 Cost  0.5667351962020261\n",
      "Epoch  3 Update  1390 Cost  0.07479835952078409\n",
      "Epoch  3 Update  1400 Cost  0.29084170272827875\n",
      "Epoch  3 Update  1410 Cost  0.1893893835477308\n",
      "Epoch  3 Update  1420 Cost  0.5431111434344417\n",
      "Epoch  3 Update  1430 Cost  0.1942150392597546\n",
      "Epoch  3 Update  1440 Cost  0.32626439318793216\n",
      "Epoch  3 Update  1450 Cost  0.2193766799074426\n",
      "Epoch  3 Update  1460 Cost  0.5683019790210943\n",
      "Epoch  3 Update  1470 Cost  0.22464674832266232\n",
      "Epoch  3 Update  1480 Cost  0.2507190993537643\n",
      "Train  0.345221052632 Valid  0.1456 Test  0.13364\n",
      "Seen 5920 samples\n",
      "Epoch  4 Update  1490 Cost  0.5412842898085971\n",
      "Epoch  4 Update  1500 Cost  0.3019320863525732\n",
      "Epoch  4 Update  1510 Cost  0.20914620831437344\n",
      "Epoch  4 Update  1520 Cost  0.3753860117194316\n",
      "Epoch  4 Update  1530 Cost  0.4533927989245772\n",
      "Epoch  4 Update  1540 Cost  0.2878792831596191\n",
      "Epoch  4 Update  1550 Cost  0.4291572067137223\n",
      "Epoch  4 Update  1560 Cost  0.33053012725731346\n",
      "Epoch  4 Update  1570 Cost  0.29148345535517234\n",
      "Epoch  4 Update  1580 Cost  0.35742198480143283\n",
      "Epoch  4 Update  1590 Cost  0.30783314635147047\n",
      "Epoch  4 Update  1600 Cost  0.38865746467016715\n",
      "Epoch  4 Update  1610 Cost  0.22071590949299594\n",
      "Epoch  4 Update  1620 Cost  0.25465626366424343\n",
      "Epoch  4 Update  1630 Cost  0.138118853403068\n",
      "Epoch  4 Update  1640 Cost  0.3302222551820205\n",
      "Epoch  4 Update  1650 Cost  0.13036329868956903\n",
      "Epoch  4 Update  1660 Cost  0.08357985979250136\n",
      "Epoch  4 Update  1670 Cost  0.10578050512904534\n",
      "Epoch  4 Update  1680 Cost  0.24724781359677822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4 Update  1690 Cost  0.508341011284072\n",
      "Epoch  4 Update  1700 Cost  0.5409904196691964\n",
      "Epoch  4 Update  1710 Cost  0.17429767196906185\n",
      "Epoch  4 Update  1720 Cost  0.3340582124014568\n",
      "Epoch  4 Update  1730 Cost  0.3276019050362859\n",
      "Epoch  4 Update  1740 Cost  0.4950608463576128\n",
      "Epoch  4 Update  1750 Cost  0.32800937462908586\n",
      "Epoch  4 Update  1760 Cost  0.21274608245537271\n",
      "Epoch  4 Update  1770 Cost  0.30396857979879427\n",
      "Epoch  4 Update  1780 Cost  0.2074568888503629\n",
      "Epoch  4 Update  1790 Cost  0.3290860413721645\n",
      "Epoch  4 Update  1800 Cost  0.10076914763576196\n",
      "Epoch  4 Update  1810 Cost  0.2780180859044699\n",
      "Epoch  4 Update  1820 Cost  0.1659469068154479\n",
      "Epoch  4 Update  1830 Cost  0.21426876871739972\n",
      "Epoch  4 Update  1840 Cost  0.38152621714425944\n",
      "Epoch  4 Update  1850 Cost  0.14086258348244024\n",
      "Train  0.340926315789 Valid  0.1408 Test  0.13208\n",
      "Seen 5920 samples\n",
      "Epoch  5 Update  1860 Cost  0.23909021049301907\n",
      "Epoch  5 Update  1870 Cost  0.4093439476563874\n",
      "Epoch  5 Update  1880 Cost  0.25399126909486064\n",
      "Epoch  5 Update  1890 Cost  0.17937684916879076\n",
      "Epoch  5 Update  1900 Cost  0.19640090221529571\n",
      "Epoch  5 Update  1910 Cost  0.27912624585279155\n",
      "Epoch  5 Update  1920 Cost  0.11889289934418613\n",
      "Epoch  5 Update  1930 Cost  0.3380528786925432\n",
      "Epoch  5 Update  1940 Cost  0.3538593046488151\n",
      "Epoch  5 Update  1950 Cost  0.4314848813796491\n",
      "Epoch  5 Update  1960 Cost  0.1580686108553451\n",
      "Epoch  5 Update  1970 Cost  0.17062209744313697\n",
      "Epoch  5 Update  1980 Cost  0.44473924364665773\n",
      "Epoch  5 Update  1990 Cost  0.3057021159001596\n",
      "Epoch  5 Update  2000 Cost  0.22403467687803708\n",
      "Epoch  5 Update  2010 Cost  0.11564390462673134\n",
      "Epoch  5 Update  2020 Cost  0.5286906167378144\n",
      "Epoch  5 Update  2030 Cost  0.175668574753019\n",
      "Epoch  5 Update  2040 Cost  0.395786505169041\n",
      "Epoch  5 Update  2050 Cost  0.11326336907765727\n",
      "Epoch  5 Update  2060 Cost  0.35903879378519254\n",
      "Epoch  5 Update  2070 Cost  0.14456238684332115\n",
      "Epoch  5 Update  2080 Cost  0.2221916508835689\n",
      "Epoch  5 Update  2090 Cost  0.2967819945634537\n",
      "Epoch  5 Update  2100 Cost  0.1056544879304318\n",
      "Epoch  5 Update  2110 Cost  0.2138922792046083\n",
      "Epoch  5 Update  2120 Cost  0.2136760503487753\n",
      "Epoch  5 Update  2130 Cost  0.17790504664851067\n",
      "Epoch  5 Update  2140 Cost  0.3024058620507333\n",
      "Epoch  5 Update  2150 Cost  0.18328566780857686\n",
      "Epoch  5 Update  2160 Cost  0.32075291951287965\n",
      "Epoch  5 Update  2170 Cost  0.24539962153676886\n",
      "Epoch  5 Update  2180 Cost  0.20948831100684062\n",
      "Epoch  5 Update  2190 Cost  0.17419430411574158\n",
      "Epoch  5 Update  2200 Cost  0.4013694486228859\n",
      "Epoch  5 Update  2210 Cost  0.6396910807015358\n",
      "Epoch  5 Update  2220 Cost  0.4993646430756235\n",
      "Saving...\n",
      "Train  0.344084210526 Valid  0.1416 Test  0.13652\n",
      "Seen 5920 samples\n",
      "Epoch  6 Update  2230 Cost  0.37053763854277955\n",
      "Epoch  6 Update  2240 Cost  0.3481811707170505\n",
      "Epoch  6 Update  2250 Cost  0.21114972342485133\n",
      "Epoch  6 Update  2260 Cost  0.1733288265815585\n",
      "Epoch  6 Update  2270 Cost  0.20024501671007516\n",
      "Epoch  6 Update  2280 Cost  0.2659705792082267\n",
      "Epoch  6 Update  2290 Cost  0.43004488158252013\n",
      "Epoch  6 Update  2300 Cost  0.47947830764993443\n",
      "Epoch  6 Update  2310 Cost  0.10877021859545256\n",
      "Epoch  6 Update  2320 Cost  0.1591713732899999\n",
      "Epoch  6 Update  2330 Cost  0.42755870480993974\n",
      "Epoch  6 Update  2340 Cost  0.21706311322716693\n",
      "Epoch  6 Update  2350 Cost  0.2839943078555251\n",
      "Epoch  6 Update  2360 Cost  0.27299343959050215\n",
      "Epoch  6 Update  2370 Cost  0.22196733765601043\n",
      "Epoch  6 Update  2380 Cost  0.20711649303840285\n",
      "Epoch  6 Update  2390 Cost  0.19515584547733567\n",
      "Epoch  6 Update  2400 Cost  0.3119124834435249\n",
      "Epoch  6 Update  2410 Cost  0.3939558350626092\n",
      "Epoch  6 Update  2420 Cost  0.36028396946521607\n",
      "Epoch  6 Update  2430 Cost  0.18296378269844002\n",
      "Epoch  6 Update  2440 Cost  0.19063025283842827\n",
      "Epoch  6 Update  2450 Cost  0.1468052941573569\n",
      "Epoch  6 Update  2460 Cost  0.15935022633885476\n",
      "Epoch  6 Update  2470 Cost  0.44849649188473006\n",
      "Epoch  6 Update  2480 Cost  0.1800215800262081\n",
      "Epoch  6 Update  2490 Cost  0.1654452565512669\n",
      "Epoch  6 Update  2500 Cost  0.5726157138479984\n",
      "Epoch  6 Update  2510 Cost  0.18762993220181562\n",
      "Epoch  6 Update  2520 Cost  0.15975097466988938\n",
      "Epoch  6 Update  2530 Cost  0.2671689449284021\n",
      "Epoch  6 Update  2540 Cost  0.22288392597271964\n",
      "Epoch  6 Update  2550 Cost  0.13079051880471637\n",
      "Epoch  6 Update  2560 Cost  0.312786985906373\n",
      "Epoch  6 Update  2570 Cost  0.3711134301323107\n",
      "Epoch  6 Update  2580 Cost  0.14677915988761853\n",
      "Epoch  6 Update  2590 Cost  0.058196804771027365\n",
      "Train  0.327326315789 Valid  0.1312 Test  0.12\n",
      "Seen 5920 samples\n",
      "Epoch  7 Update  2600 Cost  0.2319957804125729\n",
      "Epoch  7 Update  2610 Cost  0.16003590638146223\n",
      "Epoch  7 Update  2620 Cost  0.18158468193614558\n",
      "Epoch  7 Update  2630 Cost  0.20135979805662624\n",
      "Epoch  7 Update  2640 Cost  0.2785665948022198\n",
      "Epoch  7 Update  2650 Cost  0.24163057353728692\n",
      "Epoch  7 Update  2660 Cost  0.09631403253209916\n",
      "Epoch  7 Update  2670 Cost  0.03098271941167511\n",
      "Epoch  7 Update  2680 Cost  0.07854108504538759\n",
      "Epoch  7 Update  2690 Cost  0.13429907372353422\n",
      "Epoch  7 Update  2700 Cost  0.20908845824794572\n",
      "Epoch  7 Update  2710 Cost  0.2541300587416472\n",
      "Epoch  7 Update  2720 Cost  0.08644065037059978\n",
      "Epoch  7 Update  2730 Cost  0.25309408693949514\n",
      "Epoch  7 Update  2740 Cost  0.27698109088533324\n",
      "Epoch  7 Update  2750 Cost  0.07209521940028107\n",
      "Epoch  7 Update  2760 Cost  0.09545459878992436\n",
      "Epoch  7 Update  2770 Cost  0.3166865847557407\n",
      "Epoch  7 Update  2780 Cost  0.20272686067816045\n",
      "Epoch  7 Update  2790 Cost  0.35829927652148863\n",
      "Epoch  7 Update  2800 Cost  0.2412962913684449\n",
      "Epoch  7 Update  2810 Cost  0.2354699831744856\n",
      "Epoch  7 Update  2820 Cost  0.17852979208131656\n",
      "Epoch  7 Update  2830 Cost  0.2799727309980262\n",
      "Epoch  7 Update  2840 Cost  0.15716811846529394\n",
      "Epoch  7 Update  2850 Cost  0.33702737191887405\n",
      "Epoch  7 Update  2860 Cost  0.2848586198486018\n",
      "Epoch  7 Update  2870 Cost  0.3902526960691763\n",
      "Epoch  7 Update  2880 Cost  0.12480931782013074\n",
      "Epoch  7 Update  2890 Cost  0.20063477931731782\n",
      "Epoch  7 Update  2900 Cost  0.30266137174466345\n",
      "Epoch  7 Update  2910 Cost  0.12089795410171887\n",
      "Epoch  7 Update  2920 Cost  0.267789327392475\n",
      "Epoch  7 Update  2930 Cost  0.100378122221222\n",
      "Epoch  7 Update  2940 Cost  0.16437273651017584\n",
      "Epoch  7 Update  2950 Cost  0.2715891626142566\n",
      "Epoch  7 Update  2960 Cost  0.24802932527144592\n",
      "Train  0.324252631579 Valid  0.1328 Test  0.11548\n",
      "Seen 5920 samples\n",
      "Epoch  8 Update  2970 Cost  0.13042330831255894\n",
      "Epoch  8 Update  2980 Cost  0.20635680537540504\n",
      "Epoch  8 Update  2990 Cost  0.5844302427362295\n",
      "Epoch  8 Update  3000 Cost  0.2852209808649733\n",
      "Epoch  8 Update  3010 Cost  0.25125329509837085\n",
      "Epoch  8 Update  3020 Cost  0.1623366726334436\n",
      "Epoch  8 Update  3030 Cost  0.14952219150270254\n",
      "Epoch  8 Update  3040 Cost  0.20770759521424212\n",
      "Epoch  8 Update  3050 Cost  0.27103832957448276\n",
      "Epoch  8 Update  3060 Cost  0.3072204601277221\n",
      "Epoch  8 Update  3070 Cost  0.2626711574069563\n",
      "Epoch  8 Update  3080 Cost  0.10452349173489354\n",
      "Epoch  8 Update  3090 Cost  0.31189600398947576\n",
      "Epoch  8 Update  3100 Cost  0.34112957051266735\n",
      "Epoch  8 Update  3110 Cost  0.06924845939403176\n",
      "Epoch  8 Update  3120 Cost  0.12495349424293924\n",
      "Epoch  8 Update  3130 Cost  0.24401077545235186\n",
      "Epoch  8 Update  3140 Cost  0.34633178335535564\n",
      "Epoch  8 Update  3150 Cost  0.2679374243380078\n",
      "Epoch  8 Update  3160 Cost  0.12082387328521531\n",
      "Epoch  8 Update  3170 Cost  0.4068489644858823\n",
      "Epoch  8 Update  3180 Cost  0.0824007234197066\n",
      "Epoch  8 Update  3190 Cost  0.10336262287602682\n",
      "Epoch  8 Update  3200 Cost  0.1914615531644375\n",
      "Epoch  8 Update  3210 Cost  0.15365116390485162\n",
      "Epoch  8 Update  3220 Cost  0.22331954184445885\n",
      "Epoch  8 Update  3230 Cost  0.16188493273964602\n",
      "Epoch  8 Update  3240 Cost  0.16838935560856605\n",
      "Epoch  8 Update  3250 Cost  0.2272238370266739\n",
      "Epoch  8 Update  3260 Cost  0.15547848203793002\n",
      "Epoch  8 Update  3270 Cost  0.12984914035849107\n",
      "Epoch  8 Update  3280 Cost  0.1610129080114792\n",
      "Epoch  8 Update  3290 Cost  0.16852984122818004\n",
      "Epoch  8 Update  3300 Cost  0.22663033505571825\n",
      "Epoch  8 Update  3310 Cost  0.4203140611447906\n",
      "Epoch  8 Update  3320 Cost  0.32295233455488276\n",
      "Epoch  8 Update  3330 Cost  0.4531825918303103\n",
      "Saving...\n",
      "Train  0.3216 Valid  0.132 Test  0.116\n",
      "Seen 5920 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9 Update  3340 Cost  0.4903801269627894\n",
      "Epoch  9 Update  3350 Cost  0.24653251953083474\n",
      "Epoch  9 Update  3360 Cost  0.18301218229332838\n",
      "Epoch  9 Update  3370 Cost  0.1882958932087848\n",
      "Epoch  9 Update  3380 Cost  0.28596038729417284\n",
      "Epoch  9 Update  3390 Cost  0.5235259893449029\n",
      "Epoch  9 Update  3400 Cost  0.18403297587415346\n",
      "Epoch  9 Update  3410 Cost  0.23342335300351008\n",
      "Epoch  9 Update  3420 Cost  0.12367329451905426\n",
      "Epoch  9 Update  3430 Cost  0.09202382824761007\n",
      "Epoch  9 Update  3440 Cost  0.1668383285812308\n",
      "Epoch  9 Update  3450 Cost  0.15749966670309742\n",
      "Epoch  9 Update  3460 Cost  0.058556921025677285\n",
      "Epoch  9 Update  3470 Cost  0.48230005229671813\n",
      "Epoch  9 Update  3480 Cost  0.39405538595090656\n",
      "Epoch  9 Update  3490 Cost  0.10704655435831592\n",
      "Epoch  9 Update  3500 Cost  0.18850772093156837\n",
      "Epoch  9 Update  3510 Cost  0.31665197226992786\n",
      "Epoch  9 Update  3520 Cost  0.5542925698293578\n",
      "Epoch  9 Update  3530 Cost  0.21447187452823915\n",
      "Epoch  9 Update  3540 Cost  0.10167329664641243\n",
      "Epoch  9 Update  3550 Cost  0.38942103841248554\n",
      "Epoch  9 Update  3560 Cost  0.07889082935219974\n",
      "Epoch  9 Update  3570 Cost  0.23567223333557166\n",
      "Epoch  9 Update  3580 Cost  0.24191627230047444\n",
      "Epoch  9 Update  3590 Cost  0.34187060650618284\n",
      "Epoch  9 Update  3600 Cost  0.30047855226617926\n",
      "Epoch  9 Update  3610 Cost  0.5773956378299079\n",
      "Epoch  9 Update  3620 Cost  0.41752146212701424\n",
      "Epoch  9 Update  3630 Cost  0.1181913180580408\n",
      "Epoch  9 Update  3640 Cost  0.3555926250142983\n",
      "Epoch  9 Update  3650 Cost  0.7587209230247902\n",
      "Epoch  9 Update  3660 Cost  0.17413471368518949\n",
      "Epoch  9 Update  3670 Cost  0.08121895868462618\n",
      "Epoch  9 Update  3680 Cost  0.1587626759767674\n",
      "Epoch  9 Update  3690 Cost  0.16536058334165699\n",
      "Epoch  9 Update  3700 Cost  0.24586505514833917\n",
      "Train  0.336126315789 Valid  0.152 Test  0.14356\n",
      "Seen 5920 samples\n",
      "Epoch  10 Update  3710 Cost  0.36888326820338563\n",
      "Epoch  10 Update  3720 Cost  0.262892142120938\n",
      "Epoch  10 Update  3730 Cost  0.21392423061505678\n",
      "Epoch  10 Update  3740 Cost  0.20205210761508152\n",
      "Epoch  10 Update  3750 Cost  0.4332038411780973\n",
      "Epoch  10 Update  3760 Cost  0.14676714751585068\n",
      "Epoch  10 Update  3770 Cost  0.31250960032303177\n",
      "Epoch  10 Update  3780 Cost  0.8422484336465206\n",
      "Epoch  10 Update  3790 Cost  0.2591035371369528\n",
      "Epoch  10 Update  3800 Cost  0.175589976395544\n",
      "Epoch  10 Update  3810 Cost  0.5259617373802338\n",
      "Epoch  10 Update  3820 Cost  0.6692919639614047\n",
      "Epoch  10 Update  3830 Cost  0.037613617313423886\n",
      "Epoch  10 Update  3840 Cost  0.039150361540562326\n",
      "Epoch  10 Update  3850 Cost  0.5241030930547352\n",
      "Epoch  10 Update  3860 Cost  1.1643218339524308\n",
      "Epoch  10 Update  3870 Cost  0.29413031466260614\n",
      "Epoch  10 Update  3880 Cost  0.17649741919073103\n",
      "Epoch  10 Update  3890 Cost  0.11663235076385431\n",
      "Epoch  10 Update  3900 Cost  0.1596510243075412\n",
      "Epoch  10 Update  3910 Cost  0.4870221849662014\n",
      "Epoch  10 Update  3920 Cost  0.26915855630904145\n",
      "Epoch  10 Update  3930 Cost  0.22722155895749602\n",
      "Epoch  10 Update  3940 Cost  0.06293996468443287\n",
      "Epoch  10 Update  3950 Cost  0.19060721992004764\n",
      "Epoch  10 Update  3960 Cost  0.1890829848731752\n",
      "Epoch  10 Update  3970 Cost  0.19148455852380916\n",
      "Epoch  10 Update  3980 Cost  0.4632617244327345\n",
      "Epoch  10 Update  3990 Cost  0.18079851852041776\n",
      "Epoch  10 Update  4000 Cost  0.13323867620636962\n",
      "Epoch  10 Update  4010 Cost  0.40331430855356243\n",
      "Epoch  10 Update  4020 Cost  0.43289372267668774\n",
      "Epoch  10 Update  4030 Cost  0.06253816228185127\n",
      "Epoch  10 Update  4040 Cost  0.19144949720363005\n",
      "Epoch  10 Update  4050 Cost  0.29456637208681824\n",
      "Epoch  10 Update  4060 Cost  0.23379057105259093\n",
      "Epoch  10 Update  4070 Cost  0.5829889765015906\n",
      "Train  0.317221052632 Valid  0.124 Test  0.11244\n",
      "Seen 5920 samples\n",
      "Epoch  11 Update  4080 Cost  0.303645784052944\n",
      "Epoch  11 Update  4090 Cost  0.25224324671960585\n",
      "Epoch  11 Update  4100 Cost  0.057664797122988554\n",
      "Epoch  11 Update  4110 Cost  0.19674550893756002\n",
      "Epoch  11 Update  4120 Cost  0.13508721409105176\n",
      "Epoch  11 Update  4130 Cost  0.14379135927112346\n",
      "Epoch  11 Update  4140 Cost  0.22692657709820063\n",
      "Epoch  11 Update  4150 Cost  0.060467944777459336\n",
      "Epoch  11 Update  4160 Cost  0.476767992228003\n",
      "Epoch  11 Update  4170 Cost  0.08958227608081497\n",
      "Epoch  11 Update  4180 Cost  0.15019525075122866\n",
      "Epoch  11 Update  4190 Cost  0.30569313010452315\n",
      "Epoch  11 Update  4200 Cost  0.28500943394364825\n",
      "Epoch  11 Update  4210 Cost  0.2594588011873341\n",
      "Epoch  11 Update  4220 Cost  0.311028693920492\n",
      "Epoch  11 Update  4230 Cost  0.1380713903160562\n",
      "Epoch  11 Update  4240 Cost  0.12184855876464354\n",
      "Epoch  11 Update  4250 Cost  0.106219654550346\n",
      "Epoch  11 Update  4260 Cost  0.09728597357932897\n",
      "Epoch  11 Update  4270 Cost  0.7284200241547567\n",
      "Epoch  11 Update  4280 Cost  0.1323394389004663\n",
      "Epoch  11 Update  4290 Cost  0.5168363345694925\n",
      "Epoch  11 Update  4300 Cost  0.6175131250823404\n",
      "Epoch  11 Update  4310 Cost  0.21170247690105878\n",
      "Epoch  11 Update  4320 Cost  0.15841690932847055\n",
      "Epoch  11 Update  4330 Cost  0.5012621602405436\n",
      "Epoch  11 Update  4340 Cost  0.12627864847283257\n",
      "Epoch  11 Update  4350 Cost  0.2844003398872658\n",
      "Epoch  11 Update  4360 Cost  0.05739769432073403\n",
      "Epoch  11 Update  4370 Cost  0.1704328222359136\n",
      "Epoch  11 Update  4380 Cost  0.1845629966437663\n",
      "Epoch  11 Update  4390 Cost  0.17711977938295023\n",
      "Epoch  11 Update  4400 Cost  0.14157108292882029\n",
      "Epoch  11 Update  4410 Cost  0.05085817432053256\n",
      "Epoch  11 Update  4420 Cost  0.20920873973617815\n",
      "Epoch  11 Update  4430 Cost  0.14391371479893508\n",
      "Epoch  11 Update  4440 Cost  0.3551074837222263\n",
      "Saving...\n",
      "Train  0.316589473684 Valid  0.124 Test  0.11488\n",
      "Seen 5920 samples\n",
      "Epoch  12 Update  4450 Cost  0.22199884625961092\n",
      "Epoch  12 Update  4460 Cost  0.2636528349315277\n",
      "Epoch  12 Update  4470 Cost  0.056928256126807755\n",
      "Epoch  12 Update  4480 Cost  0.14913518819984833\n",
      "Epoch  12 Update  4490 Cost  0.1048549938507977\n",
      "Epoch  12 Update  4500 Cost  0.11419722157581101\n",
      "Epoch  12 Update  4510 Cost  0.40152779195034355\n",
      "Epoch  12 Update  4520 Cost  0.04597882569433866\n",
      "Epoch  12 Update  4530 Cost  0.10931526254264876\n",
      "Epoch  12 Update  4540 Cost  0.23009075568609916\n",
      "Epoch  12 Update  4550 Cost  0.3209510052520823\n",
      "Epoch  12 Update  4560 Cost  0.1810840882403093\n",
      "Epoch  12 Update  4570 Cost  0.2619939598639038\n",
      "Epoch  12 Update  4580 Cost  0.08300079500799436\n",
      "Epoch  12 Update  4590 Cost  0.10238205123176747\n",
      "Epoch  12 Update  4600 Cost  0.4264182156466551\n",
      "Epoch  12 Update  4610 Cost  0.21506361958345785\n",
      "Epoch  12 Update  4620 Cost  0.9167506122137618\n",
      "Epoch  12 Update  4630 Cost  0.44006132047986907\n",
      "Epoch  12 Update  4640 Cost  0.4111286986492747\n",
      "Epoch  12 Update  4650 Cost  0.49698995186676187\n",
      "Epoch  12 Update  4660 Cost  0.26587062110849863\n",
      "Epoch  12 Update  4670 Cost  0.09736611186199567\n",
      "Epoch  12 Update  4680 Cost  0.07002695453476991\n",
      "Epoch  12 Update  4690 Cost  0.39829624891989573\n",
      "Epoch  12 Update  4700 Cost  0.10758227357034711\n",
      "Epoch  12 Update  4710 Cost  0.07726371712928849\n",
      "Epoch  12 Update  4720 Cost  0.20992199754347973\n",
      "Epoch  12 Update  4730 Cost  0.5217638089335113\n",
      "Epoch  12 Update  4740 Cost  0.1584365623392708\n",
      "Epoch  12 Update  4750 Cost  0.25065123671547523\n",
      "Epoch  12 Update  4760 Cost  0.097147998316087\n",
      "Epoch  12 Update  4770 Cost  0.18977806000091116\n",
      "Epoch  12 Update  4780 Cost  0.33721617152808375\n",
      "Epoch  12 Update  4790 Cost  0.06691368818028257\n",
      "Epoch  12 Update  4800 Cost  0.15425430277859903\n",
      "Epoch  12 Update  4810 Cost  0.09683153840110266\n",
      "Train  0.312673684211 Valid  0.1264 Test  0.11636\n",
      "Seen 5920 samples\n",
      "Epoch  13 Update  4820 Cost  0.026697862151827404\n",
      "Epoch  13 Update  4830 Cost  0.07807737821670915\n",
      "Epoch  13 Update  4840 Cost  0.27352010162153517\n",
      "Epoch  13 Update  4850 Cost  0.11295272525726069\n",
      "Epoch  13 Update  4860 Cost  0.08715039541337206\n",
      "Epoch  13 Update  4870 Cost  0.41885938170944237\n",
      "Epoch  13 Update  4880 Cost  0.22797832564538953\n",
      "Epoch  13 Update  4890 Cost  0.11847883933140974\n",
      "Epoch  13 Update  4900 Cost  0.2230002586049519\n",
      "Epoch  13 Update  4910 Cost  0.18401135651781214\n",
      "Epoch  13 Update  4920 Cost  0.5510750670533792\n",
      "Epoch  13 Update  4930 Cost  0.10241758265214593\n",
      "Epoch  13 Update  4940 Cost  0.25695726250376183\n",
      "Epoch  13 Update  4950 Cost  0.0753898968756838\n",
      "Epoch  13 Update  4960 Cost  0.47667139885818094\n",
      "Epoch  13 Update  4970 Cost  0.15105753969839453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  13 Update  4980 Cost  0.2421698044001705\n",
      "Epoch  13 Update  4990 Cost  0.2935944317804164\n",
      "Epoch  13 Update  5000 Cost  0.08251748257059266\n",
      "Epoch  13 Update  5010 Cost  0.2080892104675144\n",
      "Epoch  13 Update  5020 Cost  0.15481702321940508\n",
      "Epoch  13 Update  5030 Cost  0.32670579160635754\n",
      "Epoch  13 Update  5040 Cost  0.16459082569944591\n",
      "Epoch  13 Update  5050 Cost  0.08424283266815619\n",
      "Epoch  13 Update  5060 Cost  0.5199818190792804\n",
      "Epoch  13 Update  5070 Cost  0.3023740203823771\n",
      "Epoch  13 Update  5080 Cost  0.4262343355416624\n",
      "Epoch  13 Update  5090 Cost  0.16586081674569414\n",
      "Epoch  13 Update  5100 Cost  0.3758543640914897\n",
      "Epoch  13 Update  5110 Cost  0.37756335294949495\n",
      "Epoch  13 Update  5120 Cost  0.2602743958694375\n",
      "Epoch  13 Update  5130 Cost  0.16442667940261793\n",
      "Epoch  13 Update  5140 Cost  0.11717071372752054\n",
      "Epoch  13 Update  5150 Cost  0.17541823176484422\n",
      "Epoch  13 Update  5160 Cost  0.0946922222803755\n",
      "Epoch  13 Update  5170 Cost  0.11160944675557999\n",
      "Epoch  13 Update  5180 Cost  0.19978238251245037\n",
      "Train  0.306947368421 Valid  0.1232 Test  0.11092\n",
      "Seen 5920 samples\n",
      "Epoch  14 Update  5190 Cost  0.047872084924564094\n",
      "Epoch  14 Update  5200 Cost  0.15359163722868385\n",
      "Epoch  14 Update  5210 Cost  0.14488874604039884\n",
      "Epoch  14 Update  5220 Cost  0.15208198911241982\n",
      "Epoch  14 Update  5230 Cost  0.3415980021774767\n",
      "Epoch  14 Update  5240 Cost  0.3571272757038524\n",
      "Epoch  14 Update  5250 Cost  0.4218614893468552\n",
      "Epoch  14 Update  5260 Cost  0.18106209204322174\n",
      "Epoch  14 Update  5270 Cost  0.08939120859502317\n",
      "Epoch  14 Update  5280 Cost  0.6747377520451737\n",
      "Epoch  14 Update  5290 Cost  0.22722261377287684\n",
      "Epoch  14 Update  5300 Cost  0.17374794102292596\n",
      "Epoch  14 Update  5310 Cost  0.49729987394557074\n",
      "Epoch  14 Update  5320 Cost  0.44838192742098504\n",
      "Epoch  14 Update  5330 Cost  0.23253768953220655\n",
      "Epoch  14 Update  5340 Cost  0.24101553473047607\n",
      "Epoch  14 Update  5350 Cost  0.09741890648574865\n",
      "Epoch  14 Update  5360 Cost  0.3634165496065553\n",
      "Epoch  14 Update  5370 Cost  0.09714224674246115\n",
      "Epoch  14 Update  5380 Cost  0.10372427214343995\n",
      "Epoch  14 Update  5390 Cost  0.07487312109365864\n",
      "Epoch  14 Update  5400 Cost  0.6427388370524333\n",
      "Epoch  14 Update  5410 Cost  0.11639828408386328\n",
      "Epoch  14 Update  5420 Cost  0.1634282962745802\n",
      "Epoch  14 Update  5430 Cost  0.31449475346607536\n",
      "Epoch  14 Update  5440 Cost  0.4247645477891925\n",
      "Epoch  14 Update  5450 Cost  0.09574947712631894\n",
      "Epoch  14 Update  5460 Cost  0.5300652102560544\n",
      "Epoch  14 Update  5470 Cost  0.24121101474983797\n",
      "Epoch  14 Update  5480 Cost  0.19524221909080938\n",
      "Epoch  14 Update  5490 Cost  0.15723204902528093\n",
      "Epoch  14 Update  5500 Cost  0.21487599155241285\n",
      "Epoch  14 Update  5510 Cost  0.2168881017660584\n",
      "Epoch  14 Update  5520 Cost  0.12158669594717318\n",
      "Epoch  14 Update  5530 Cost  0.36280543217238537\n",
      "Epoch  14 Update  5540 Cost  0.04548988996777633\n",
      "Epoch  14 Update  5550 Cost  0.2403166532308111\n",
      "Saving...\n",
      "Train  0.305557894737 Valid  0.1248 Test  0.11452\n",
      "Seen 5920 samples\n",
      "Epoch  15 Update  5560 Cost  0.07704822474129881\n",
      "Epoch  15 Update  5570 Cost  0.236693142615044\n",
      "Epoch  15 Update  5580 Cost  0.1431537215453228\n",
      "Epoch  15 Update  5590 Cost  0.3709812501078189\n",
      "Epoch  15 Update  5600 Cost  0.14981627733264358\n",
      "Epoch  15 Update  5610 Cost  0.6254836360271309\n",
      "Epoch  15 Update  5620 Cost  0.20760222619651558\n",
      "Epoch  15 Update  5630 Cost  0.12477104920126961\n",
      "Epoch  15 Update  5640 Cost  0.10469262680457778\n",
      "Epoch  15 Update  5650 Cost  0.04865915802715072\n",
      "Epoch  15 Update  5660 Cost  0.4824698778549024\n",
      "Epoch  15 Update  5670 Cost  0.10811499145907356\n",
      "Epoch  15 Update  5680 Cost  0.2856634517742461\n",
      "Epoch  15 Update  5690 Cost  0.29694948519843\n",
      "Epoch  15 Update  5700 Cost  0.36834616927361746\n",
      "Epoch  15 Update  5710 Cost  0.34789665235649603\n",
      "Epoch  15 Update  5720 Cost  0.15675765617339235\n",
      "Epoch  15 Update  5730 Cost  0.046536350752199626\n",
      "Epoch  15 Update  5740 Cost  0.07069720440302427\n",
      "Epoch  15 Update  5750 Cost  0.19300276682486683\n",
      "Epoch  15 Update  5760 Cost  0.046712214680955204\n",
      "Epoch  15 Update  5770 Cost  0.3195011296720683\n",
      "Epoch  15 Update  5780 Cost  0.3840421241407329\n",
      "Epoch  15 Update  5790 Cost  0.05699174497955317\n",
      "Epoch  15 Update  5800 Cost  0.14352172407334202\n",
      "Epoch  15 Update  5810 Cost  0.10019266852\n",
      "Epoch  15 Update  5820 Cost  0.19959830372767037\n",
      "Epoch  15 Update  5830 Cost  0.1372623850608816\n",
      "Epoch  15 Update  5840 Cost  0.12728085115878918\n",
      "Epoch  15 Update  5850 Cost  0.03288530454450365\n",
      "Epoch  15 Update  5860 Cost  0.31453331165711007\n",
      "Epoch  15 Update  5870 Cost  0.1788356662739196\n",
      "Epoch  15 Update  5880 Cost  0.21391101579837402\n",
      "Epoch  15 Update  5890 Cost  0.16097881499169536\n",
      "Epoch  15 Update  5900 Cost  0.10724179721482853\n",
      "Epoch  15 Update  5910 Cost  0.44481388569671826\n",
      "Epoch  15 Update  5920 Cost  0.228008169933452\n",
      "Train  0.303031578947 Valid  0.1232 Test  0.11256\n",
      "Seen 5920 samples\n",
      "Epoch  16 Update  5930 Cost  0.3025520474175602\n",
      "Epoch  16 Update  5940 Cost  0.12223165209272152\n",
      "Epoch  16 Update  5950 Cost  0.2750311268084015\n",
      "Epoch  16 Update  5960 Cost  0.2802775912811576\n",
      "Epoch  16 Update  5970 Cost  0.15341369728873497\n",
      "Epoch  16 Update  5980 Cost  0.06606357203989055\n",
      "Epoch  16 Update  5990 Cost  0.2155457999044507\n",
      "Epoch  16 Update  6000 Cost  0.049295280712976564\n",
      "Epoch  16 Update  6010 Cost  0.03524625110900732\n",
      "Epoch  16 Update  6020 Cost  0.09145962512222047\n",
      "Epoch  16 Update  6030 Cost  0.319926406060472\n",
      "Epoch  16 Update  6040 Cost  0.23712129191455766\n",
      "Epoch  16 Update  6050 Cost  0.15436800219962304\n",
      "Epoch  16 Update  6060 Cost  0.34431095960338587\n",
      "Epoch  16 Update  6070 Cost  0.2504067435235681\n",
      "Epoch  16 Update  6080 Cost  0.08668216802180485\n",
      "Epoch  16 Update  6090 Cost  0.05890985322769338\n",
      "Epoch  16 Update  6100 Cost  0.03306478895709333\n",
      "Epoch  16 Update  6110 Cost  0.07565082116796537\n",
      "Epoch  16 Update  6120 Cost  0.2707124117001179\n",
      "Epoch  16 Update  6130 Cost  0.384674010177551\n",
      "Epoch  16 Update  6140 Cost  0.6972777028937917\n",
      "Epoch  16 Update  6150 Cost  0.10484500829824252\n",
      "Epoch  16 Update  6160 Cost  0.12672340742066304\n",
      "Epoch  16 Update  6170 Cost  0.39623792923965695\n",
      "Epoch  16 Update  6180 Cost  0.27060049476438713\n",
      "Epoch  16 Update  6190 Cost  0.10302359663722191\n",
      "Epoch  16 Update  6200 Cost  0.05618329873543416\n",
      "Epoch  16 Update  6210 Cost  0.02747771215829931\n",
      "Epoch  16 Update  6220 Cost  0.1227441795410705\n",
      "Epoch  16 Update  6230 Cost  0.25855414108099495\n",
      "Epoch  16 Update  6240 Cost  0.05919902231010351\n",
      "Epoch  16 Update  6250 Cost  0.11091281430802484\n",
      "Epoch  16 Update  6260 Cost  0.06221455980946355\n",
      "Epoch  16 Update  6270 Cost  0.2720516802369937\n",
      "Epoch  16 Update  6280 Cost  0.14503650527770137\n",
      "Epoch  16 Update  6290 Cost  0.09329269008778841\n",
      "Train  0.306021052632 Valid  0.116 Test  0.11204\n",
      "Seen 5920 samples\n",
      "Epoch  17 Update  6300 Cost  0.07021547507800964\n",
      "Epoch  17 Update  6310 Cost  0.26603590118568254\n",
      "Epoch  17 Update  6320 Cost  0.2521684838811325\n",
      "Epoch  17 Update  6330 Cost  0.15112587742729125\n",
      "Epoch  17 Update  6340 Cost  0.18498775945163\n",
      "Epoch  17 Update  6350 Cost  0.14434376727066048\n",
      "Epoch  17 Update  6360 Cost  0.030626108363863133\n",
      "Epoch  17 Update  6370 Cost  0.07354034486265654\n",
      "Epoch  17 Update  6380 Cost  0.057205645227252855\n",
      "Epoch  17 Update  6390 Cost  0.3187156412683444\n",
      "Epoch  17 Update  6400 Cost  0.22068254009028418\n",
      "Epoch  17 Update  6410 Cost  0.09874907927766945\n",
      "Epoch  17 Update  6420 Cost  0.08683103641797169\n",
      "Epoch  17 Update  6430 Cost  0.16725959848521035\n",
      "Epoch  17 Update  6440 Cost  0.051856482982012814\n",
      "Epoch  17 Update  6450 Cost  0.16321367753377516\n",
      "Epoch  17 Update  6460 Cost  0.12746269348100844\n",
      "Epoch  17 Update  6470 Cost  0.26479985932335703\n",
      "Epoch  17 Update  6480 Cost  0.33183180599904083\n",
      "Epoch  17 Update  6490 Cost  0.03378347831782845\n",
      "Epoch  17 Update  6500 Cost  0.10813130937400676\n",
      "Epoch  17 Update  6510 Cost  0.3458346468373078\n",
      "Epoch  17 Update  6520 Cost  0.06440594096373414\n",
      "Epoch  17 Update  6530 Cost  0.428534141588355\n",
      "Epoch  17 Update  6540 Cost  0.5316491695815395\n",
      "Epoch  17 Update  6550 Cost  0.06263707679585612\n",
      "Epoch  17 Update  6560 Cost  0.06774157030212372\n",
      "Epoch  17 Update  6570 Cost  0.0706658609599688\n",
      "Epoch  17 Update  6580 Cost  0.24296884896746193\n",
      "Epoch  17 Update  6590 Cost  0.21524338087112974\n",
      "Epoch  17 Update  6600 Cost  0.303744813826131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  17 Update  6610 Cost  0.07652736832604999\n",
      "Epoch  17 Update  6620 Cost  0.24868254036531523\n",
      "Epoch  17 Update  6630 Cost  0.14564300727279686\n",
      "Epoch  17 Update  6640 Cost  0.08932021977034467\n",
      "Epoch  17 Update  6650 Cost  0.07167287695084358\n",
      "Epoch  17 Update  6660 Cost  0.031859143034016255\n",
      "Saving...\n",
      "Train  0.299073684211 Valid  0.112 Test  0.1102\n",
      "Seen 5920 samples\n",
      "Epoch  18 Update  6670 Cost  0.2728892580668094\n",
      "Epoch  18 Update  6680 Cost  0.12444492139135385\n",
      "Epoch  18 Update  6690 Cost  0.0798633128500432\n",
      "Epoch  18 Update  6700 Cost  0.44420456809070463\n",
      "Epoch  18 Update  6710 Cost  0.18610432052421397\n",
      "Epoch  18 Update  6720 Cost  0.1930174332716813\n",
      "Epoch  18 Update  6730 Cost  0.1269901249189242\n",
      "Epoch  18 Update  6740 Cost  0.09475746651549653\n",
      "Epoch  18 Update  6750 Cost  0.2479557834437643\n",
      "Epoch  18 Update  6760 Cost  0.1349338097399549\n",
      "Epoch  18 Update  6770 Cost  0.12835317214688144\n",
      "Epoch  18 Update  6780 Cost  0.1109221184052699\n",
      "Epoch  18 Update  6790 Cost  0.13073651345639667\n",
      "Epoch  18 Update  6800 Cost  0.3377475128023421\n",
      "Epoch  18 Update  6810 Cost  0.08902633360870593\n",
      "Epoch  18 Update  6820 Cost  0.04791654558937454\n",
      "Epoch  18 Update  6830 Cost  0.2791921343998712\n",
      "Epoch  18 Update  6840 Cost  0.22051181301591913\n",
      "Epoch  18 Update  6850 Cost  0.07170803614977111\n",
      "Epoch  18 Update  6860 Cost  0.12950196366916278\n",
      "Epoch  18 Update  6870 Cost  0.09504442620379845\n",
      "Epoch  18 Update  6880 Cost  0.0901414372312173\n",
      "Epoch  18 Update  6890 Cost  0.3942120088290981\n",
      "Epoch  18 Update  6900 Cost  0.17520507649290665\n",
      "Epoch  18 Update  6910 Cost  0.11140080331803325\n",
      "Epoch  18 Update  6920 Cost  0.05853732379660417\n",
      "Epoch  18 Update  6930 Cost  0.07834184614238045\n",
      "Epoch  18 Update  6940 Cost  0.4522104885857188\n",
      "Epoch  18 Update  6950 Cost  0.14095518404713803\n",
      "Epoch  18 Update  6960 Cost  0.34171159378443555\n",
      "Epoch  18 Update  6970 Cost  0.0718117640272924\n",
      "Epoch  18 Update  6980 Cost  0.4540833484721004\n",
      "Epoch  18 Update  6990 Cost  0.059907897323683776\n",
      "Epoch  18 Update  7000 Cost  0.3271513009257119\n",
      "Epoch  18 Update  7010 Cost  0.052041162465531116\n",
      "Epoch  18 Update  7020 Cost  0.1533541797767345\n",
      "Epoch  18 Update  7030 Cost  0.3629568687971023\n",
      "Train  0.295789473684 Valid  0.1104 Test  0.10628\n",
      "Seen 5920 samples\n",
      "Epoch  19 Update  7040 Cost  0.05788845391917179\n",
      "Epoch  19 Update  7050 Cost  0.31539710390665626\n",
      "Epoch  19 Update  7060 Cost  0.09715290594206366\n",
      "Epoch  19 Update  7070 Cost  0.060533876507045586\n",
      "Epoch  19 Update  7080 Cost  0.4888275358739183\n",
      "Epoch  19 Update  7090 Cost  0.6312835291127886\n",
      "Epoch  19 Update  7100 Cost  0.14727765081656835\n",
      "Epoch  19 Update  7110 Cost  0.05950789569489451\n",
      "Epoch  19 Update  7120 Cost  0.04128160080351218\n",
      "Epoch  19 Update  7130 Cost  0.03210579627491995\n",
      "Epoch  19 Update  7140 Cost  0.15881503659693677\n",
      "Epoch  19 Update  7150 Cost  0.26535486392687774\n",
      "Epoch  19 Update  7160 Cost  0.037068742359603626\n",
      "Epoch  19 Update  7170 Cost  0.048198670680258404\n",
      "Epoch  19 Update  7180 Cost  0.1731237290710961\n",
      "Epoch  19 Update  7190 Cost  0.32373747954147813\n",
      "Epoch  19 Update  7200 Cost  0.10713056684091712\n",
      "Epoch  19 Update  7210 Cost  0.13551049520556704\n",
      "Epoch  19 Update  7220 Cost  0.1838355956590223\n",
      "Epoch  19 Update  7230 Cost  0.34833679655350747\n",
      "Epoch  19 Update  7240 Cost  0.20378416088402104\n",
      "Epoch  19 Update  7250 Cost  0.23148020324962543\n",
      "Epoch  19 Update  7260 Cost  0.22139900382825337\n",
      "Epoch  19 Update  7270 Cost  0.07574054825377471\n",
      "Epoch  19 Update  7280 Cost  0.1974539163111146\n",
      "Epoch  19 Update  7290 Cost  0.14609045359053832\n",
      "Epoch  19 Update  7300 Cost  0.09715080704763118\n",
      "Epoch  19 Update  7310 Cost  0.24617117677163508\n",
      "Epoch  19 Update  7320 Cost  0.10509822810730418\n",
      "Epoch  19 Update  7330 Cost  0.015731133400405224\n",
      "Epoch  19 Update  7340 Cost  0.10866567972592077\n",
      "Epoch  19 Update  7350 Cost  0.09486567422779744\n",
      "Epoch  19 Update  7360 Cost  0.06771325343920569\n",
      "Epoch  19 Update  7370 Cost  0.1947045292435395\n",
      "Epoch  19 Update  7380 Cost  0.6609064503676348\n",
      "Epoch  19 Update  7390 Cost  0.0595245393415765\n",
      "Epoch  19 Update  7400 Cost  0.2464875701995564\n",
      "Train  0.292378947368 Valid  0.1088 Test  0.10616\n",
      "Seen 5920 samples\n",
      "Epoch  20 Update  7410 Cost  0.11640081334113994\n",
      "Epoch  20 Update  7420 Cost  0.2722011843164053\n",
      "Epoch  20 Update  7430 Cost  0.05761827189241578\n",
      "Epoch  20 Update  7440 Cost  0.051590107309178686\n",
      "Epoch  20 Update  7450 Cost  0.05200479721006283\n",
      "Epoch  20 Update  7460 Cost  0.12999638418844017\n",
      "Epoch  20 Update  7470 Cost  0.06852419984430043\n",
      "Epoch  20 Update  7480 Cost  0.38815473042707094\n",
      "Epoch  20 Update  7490 Cost  0.26653820124727934\n",
      "Epoch  20 Update  7500 Cost  0.162327496558627\n",
      "Epoch  20 Update  7510 Cost  0.14328874673735006\n",
      "Epoch  20 Update  7520 Cost  0.7200571856571804\n",
      "Epoch  20 Update  7530 Cost  0.053732422166416736\n",
      "Epoch  20 Update  7540 Cost  0.04318024317550455\n",
      "Epoch  20 Update  7550 Cost  0.03659244323342001\n",
      "Epoch  20 Update  7560 Cost  0.1011729871186751\n",
      "Epoch  20 Update  7570 Cost  0.22915519068346196\n",
      "Epoch  20 Update  7580 Cost  0.2350398799724114\n",
      "Epoch  20 Update  7590 Cost  0.04535870187986626\n",
      "Epoch  20 Update  7600 Cost  0.09970064840218013\n",
      "Epoch  20 Update  7610 Cost  0.13519318446298462\n",
      "Epoch  20 Update  7620 Cost  0.15679454114567937\n",
      "Epoch  20 Update  7630 Cost  0.27781757612102115\n",
      "Epoch  20 Update  7640 Cost  0.10619652467369098\n",
      "Epoch  20 Update  7650 Cost  0.04064031191201285\n",
      "Epoch  20 Update  7660 Cost  0.09254530626776461\n",
      "Epoch  20 Update  7670 Cost  0.04411901264139793\n",
      "Epoch  20 Update  7680 Cost  0.1454058404245681\n",
      "Epoch  20 Update  7690 Cost  0.04260437260543705\n",
      "Epoch  20 Update  7700 Cost  0.2948499331311505\n",
      "Epoch  20 Update  7710 Cost  0.18863828612633057\n",
      "Epoch  20 Update  7720 Cost  0.04903743474016395\n",
      "Epoch  20 Update  7730 Cost  0.11773794815447285\n",
      "Epoch  20 Update  7740 Cost  0.02070455431377178\n",
      "Epoch  20 Update  7750 Cost  0.07367450801761964\n",
      "Epoch  20 Update  7760 Cost  0.028633002206183943\n",
      "Epoch  20 Update  7770 Cost  0.1699014072603365\n",
      "Saving...\n",
      "Train  0.293178947368 Valid  0.1128 Test  0.11292\n",
      "Seen 5920 samples\n",
      "Epoch  21 Update  7780 Cost  0.07573408999142157\n",
      "Epoch  21 Update  7790 Cost  0.09177284310158651\n",
      "Epoch  21 Update  7800 Cost  0.3405804866151494\n",
      "Epoch  21 Update  7810 Cost  0.0437892613242597\n",
      "Epoch  21 Update  7820 Cost  0.5184002456621705\n",
      "Epoch  21 Update  7830 Cost  0.269135429258836\n",
      "Epoch  21 Update  7840 Cost  0.07185777288804673\n",
      "Epoch  21 Update  7850 Cost  0.04778834306240625\n",
      "Epoch  21 Update  7860 Cost  0.05417978504508182\n",
      "Epoch  21 Update  7870 Cost  0.13283893703736446\n",
      "Epoch  21 Update  7880 Cost  0.31224676448997546\n",
      "Epoch  21 Update  7890 Cost  0.03535965379916453\n",
      "Epoch  21 Update  7900 Cost  0.11337897485784838\n",
      "Epoch  21 Update  7910 Cost  0.4407255868625633\n",
      "Epoch  21 Update  7920 Cost  0.28653220993281137\n",
      "Epoch  21 Update  7930 Cost  0.18011276339660592\n",
      "Epoch  21 Update  7940 Cost  0.032709516087074626\n",
      "Epoch  21 Update  7950 Cost  0.02157805147845852\n",
      "Epoch  21 Update  7960 Cost  0.3953427247220947\n",
      "Epoch  21 Update  7970 Cost  0.08185981060697195\n",
      "Epoch  21 Update  7980 Cost  0.04185886878181288\n",
      "Epoch  21 Update  7990 Cost  0.12574837189002835\n",
      "Epoch  21 Update  8000 Cost  0.02809164677536505\n",
      "Epoch  21 Update  8010 Cost  0.024703452417035626\n",
      "Epoch  21 Update  8020 Cost  0.2486575181952451\n",
      "Epoch  21 Update  8030 Cost  0.043028987380088056\n",
      "Epoch  21 Update  8040 Cost  0.2693776730526553\n",
      "Epoch  21 Update  8050 Cost  0.05729729246377918\n",
      "Epoch  21 Update  8060 Cost  0.017225403548479422\n",
      "Epoch  21 Update  8070 Cost  0.09581627334978102\n",
      "Epoch  21 Update  8080 Cost  0.0276802024543339\n",
      "Epoch  21 Update  8090 Cost  0.1536041190078072\n",
      "Epoch  21 Update  8100 Cost  0.09103457158292923\n",
      "Epoch  21 Update  8110 Cost  0.04436499211105892\n",
      "Epoch  21 Update  8120 Cost  0.38940550263414664\n",
      "Epoch  21 Update  8130 Cost  0.10339888174129105\n",
      "Epoch  21 Update  8140 Cost  0.06287388593242717\n",
      "Train  0.287663157895 Valid  0.1248 Test  0.10656\n",
      "Seen 5920 samples\n",
      "Epoch  22 Update  8150 Cost  0.03947234623633806\n",
      "Epoch  22 Update  8160 Cost  0.0635410003479068\n",
      "Epoch  22 Update  8170 Cost  0.07959210907206689\n",
      "Epoch  22 Update  8180 Cost  0.2987586215333676\n",
      "Epoch  22 Update  8190 Cost  0.02434612299607587\n",
      "Epoch  22 Update  8200 Cost  0.039446839109734774\n",
      "Epoch  22 Update  8210 Cost  0.2541332607396839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  22 Update  8220 Cost  0.09823084243138232\n",
      "Epoch  22 Update  8230 Cost  0.1275955912937728\n",
      "Epoch  22 Update  8240 Cost  0.06182150787402796\n",
      "Epoch  22 Update  8250 Cost  0.07457450586495638\n",
      "Epoch  22 Update  8260 Cost  0.07870624686595916\n",
      "Epoch  22 Update  8270 Cost  0.13329139599684908\n",
      "Epoch  22 Update  8280 Cost  0.031091763988551287\n",
      "Epoch  22 Update  8290 Cost  0.1313215430192843\n",
      "Epoch  22 Update  8300 Cost  0.14093828226550448\n",
      "Epoch  22 Update  8310 Cost  0.02018632382759446\n",
      "Epoch  22 Update  8320 Cost  0.27624759943783356\n",
      "Epoch  22 Update  8330 Cost  0.21572223708889032\n",
      "Epoch  22 Update  8340 Cost  0.04542585972751\n",
      "Epoch  22 Update  8350 Cost  0.09806604208374695\n",
      "Epoch  22 Update  8360 Cost  0.05026560885773523\n",
      "Epoch  22 Update  8370 Cost  0.026579863426499237\n",
      "Epoch  22 Update  8380 Cost  0.21190487954661902\n",
      "Epoch  22 Update  8390 Cost  0.02284101454975802\n",
      "Epoch  22 Update  8400 Cost  0.057399728181368306\n",
      "Epoch  22 Update  8410 Cost  0.2870885492044361\n",
      "Epoch  22 Update  8420 Cost  0.03026133362100261\n",
      "Epoch  22 Update  8430 Cost  0.0747668678688468\n",
      "Epoch  22 Update  8440 Cost  0.09266911587878723\n",
      "Epoch  22 Update  8450 Cost  0.0733798440017195\n",
      "Epoch  22 Update  8460 Cost  0.10611062132928073\n",
      "Epoch  22 Update  8470 Cost  0.6096029740133126\n",
      "Epoch  22 Update  8480 Cost  0.06866931275887314\n",
      "Epoch  22 Update  8490 Cost  0.23647530592601418\n",
      "Epoch  22 Update  8500 Cost  0.2568323612458156\n",
      "Epoch  22 Update  8510 Cost  0.03060660989329295\n",
      "Train  0.285305263158 Valid  0.1128 Test  0.10924\n",
      "Seen 5920 samples\n",
      "Epoch  23 Update  8520 Cost  0.03480606263307044\n",
      "Epoch  23 Update  8530 Cost  0.030977576721970494\n",
      "Epoch  23 Update  8540 Cost  0.26909232491117263\n",
      "Epoch  23 Update  8550 Cost  0.046247381655891644\n",
      "Epoch  23 Update  8560 Cost  0.13357136545927029\n",
      "Epoch  23 Update  8570 Cost  0.02000311920880984\n",
      "Epoch  23 Update  8580 Cost  0.2857964518857661\n",
      "Epoch  23 Update  8590 Cost  0.04472104626998366\n",
      "Epoch  23 Update  8600 Cost  0.06576513023287876\n",
      "Epoch  23 Update  8610 Cost  0.2139046871250957\n",
      "Epoch  23 Update  8620 Cost  0.08116461183949333\n",
      "Epoch  23 Update  8630 Cost  0.05759669285082643\n",
      "Epoch  23 Update  8640 Cost  0.09912766908727108\n",
      "Epoch  23 Update  8650 Cost  0.1256220197143587\n",
      "Epoch  23 Update  8660 Cost  0.06958775482123591\n",
      "Epoch  23 Update  8670 Cost  0.017434436924245567\n",
      "Epoch  23 Update  8680 Cost  0.16434489888049805\n",
      "Epoch  23 Update  8690 Cost  0.2819261706892254\n",
      "Epoch  23 Update  8700 Cost  0.02716932172049008\n",
      "Epoch  23 Update  8710 Cost  0.25067642687700065\n",
      "Epoch  23 Update  8720 Cost  0.09668117652220626\n",
      "Epoch  23 Update  8730 Cost  0.19981738651989048\n",
      "Epoch  23 Update  8740 Cost  0.0547683097333448\n",
      "Epoch  23 Update  8750 Cost  0.23109468475310552\n",
      "Epoch  23 Update  8760 Cost  0.09175293956492017\n",
      "Epoch  23 Update  8770 Cost  0.2836119208590334\n",
      "Epoch  23 Update  8780 Cost  0.047466433628719044\n",
      "Epoch  23 Update  8790 Cost  0.4986364214131041\n",
      "Epoch  23 Update  8800 Cost  0.054748153638961086\n",
      "Epoch  23 Update  8810 Cost  0.056897055224359415\n",
      "Epoch  23 Update  8820 Cost  0.14849442430404128\n",
      "Epoch  23 Update  8830 Cost  0.015859519297271897\n",
      "Epoch  23 Update  8840 Cost  0.34670906127639173\n",
      "Epoch  23 Update  8850 Cost  0.32703031280198314\n",
      "Epoch  23 Update  8860 Cost  0.2046699610666877\n",
      "Epoch  23 Update  8870 Cost  0.3206212011958573\n",
      "Epoch  23 Update  8880 Cost  1.0856120045000361\n",
      "Saving...\n",
      "Train  0.284126315789 Valid  0.1184 Test  0.11112\n",
      "Seen 5920 samples\n",
      "Epoch  24 Update  8890 Cost  0.4256193016937855\n",
      "Epoch  24 Update  8900 Cost  0.10401979396493789\n",
      "Epoch  24 Update  8910 Cost  0.08749680722274933\n",
      "Epoch  24 Update  8920 Cost  0.08430681804779876\n",
      "Epoch  24 Update  8930 Cost  0.0974203458443344\n",
      "Epoch  24 Update  8940 Cost  0.24568773438199884\n",
      "Epoch  24 Update  8950 Cost  0.2491478527022256\n",
      "Epoch  24 Update  8960 Cost  0.1726802011158871\n",
      "Epoch  24 Update  8970 Cost  0.11606588096947129\n",
      "Epoch  24 Update  8980 Cost  0.06854817658204251\n",
      "Epoch  24 Update  8990 Cost  0.03616799369247125\n",
      "Epoch  24 Update  9000 Cost  0.08353813810656785\n",
      "Epoch  24 Update  9010 Cost  0.025811139996628193\n",
      "Epoch  24 Update  9020 Cost  0.08709472279198201\n",
      "Epoch  24 Update  9030 Cost  0.12843089119038426\n",
      "Epoch  24 Update  9040 Cost  0.08440974140250428\n",
      "Epoch  24 Update  9050 Cost  0.08787774593016338\n",
      "Epoch  24 Update  9060 Cost  0.035654917167751356\n",
      "Epoch  24 Update  9070 Cost  0.04149032413606646\n",
      "Epoch  24 Update  9080 Cost  0.2898570397972493\n",
      "Epoch  24 Update  9090 Cost  0.006275139869314766\n",
      "Epoch  24 Update  9100 Cost  0.1184435054028255\n",
      "Epoch  24 Update  9110 Cost  0.21174769066137616\n",
      "Epoch  24 Update  9120 Cost  0.12191917593275844\n",
      "Epoch  24 Update  9130 Cost  0.029395027697542916\n",
      "Epoch  24 Update  9140 Cost  0.12077160796840167\n",
      "Epoch  24 Update  9150 Cost  0.010684483755522214\n",
      "Epoch  24 Update  9160 Cost  0.04903041511713703\n",
      "Epoch  24 Update  9170 Cost  0.3476682108233481\n",
      "Epoch  24 Update  9180 Cost  0.05566975961499859\n",
      "Epoch  24 Update  9190 Cost  0.18338869436452243\n",
      "Epoch  24 Update  9200 Cost  0.1878318502329112\n",
      "Epoch  24 Update  9210 Cost  0.054936137796799754\n",
      "Epoch  24 Update  9220 Cost  0.03167430074653009\n",
      "Epoch  24 Update  9230 Cost  0.05036501130560725\n",
      "Epoch  24 Update  9240 Cost  0.06527285020638444\n",
      "Epoch  24 Update  9250 Cost  0.03968041657809606\n",
      "Train  0.282273684211 Valid  0.1184 Test  0.11072\n",
      "Seen 5920 samples\n",
      "Epoch  25 Update  9260 Cost  0.03365255073100435\n",
      "Epoch  25 Update  9270 Cost  0.41099821378580514\n",
      "Epoch  25 Update  9280 Cost  0.016818189307773082\n",
      "Epoch  25 Update  9290 Cost  0.1245146365928684\n",
      "Epoch  25 Update  9300 Cost  0.1182375377351785\n",
      "Epoch  25 Update  9310 Cost  0.4545371558621831\n",
      "Epoch  25 Update  9320 Cost  0.32865065699593676\n",
      "Epoch  25 Update  9330 Cost  0.023380614039783837\n",
      "Epoch  25 Update  9340 Cost  0.04579661741007414\n",
      "Epoch  25 Update  9350 Cost  0.05364260267934225\n",
      "Epoch  25 Update  9360 Cost  0.26588089507394164\n",
      "Epoch  25 Update  9370 Cost  0.07702515340374544\n",
      "Epoch  25 Update  9380 Cost  0.03992461986240401\n",
      "Epoch  25 Update  9390 Cost  0.6033715322828576\n",
      "Epoch  25 Update  9400 Cost  0.3670901477215176\n",
      "Epoch  25 Update  9410 Cost  0.1667389150045119\n",
      "Epoch  25 Update  9420 Cost  0.07811796694583162\n",
      "Epoch  25 Update  9430 Cost  0.0608606155820518\n",
      "Epoch  25 Update  9440 Cost  0.00833752292712215\n",
      "Epoch  25 Update  9450 Cost  0.05533709653859734\n",
      "Epoch  25 Update  9460 Cost  0.10951846939031372\n",
      "Epoch  25 Update  9470 Cost  0.14067710274354422\n",
      "Epoch  25 Update  9480 Cost  0.12970840189417945\n",
      "Epoch  25 Update  9490 Cost  0.05656728627645295\n",
      "Epoch  25 Update  9500 Cost  0.06759719352940735\n",
      "Epoch  25 Update  9510 Cost  0.07937742213219931\n",
      "Epoch  25 Update  9520 Cost  0.06999452597510367\n",
      "Epoch  25 Update  9530 Cost  0.13900294439619457\n",
      "Epoch  25 Update  9540 Cost  0.16723783045047289\n",
      "Epoch  25 Update  9550 Cost  0.11181584061641851\n",
      "Epoch  25 Update  9560 Cost  0.2190014884047048\n",
      "Epoch  25 Update  9570 Cost  0.4350655864240185\n",
      "Epoch  25 Update  9580 Cost  0.034200957891526294\n",
      "Epoch  25 Update  9590 Cost  0.11711072679105242\n",
      "Epoch  25 Update  9600 Cost  0.5821354645984833\n",
      "Epoch  25 Update  9610 Cost  0.03785348595381622\n",
      "Epoch  25 Update  9620 Cost  0.09111250863390932\n",
      "Train  0.280463157895 Valid  0.1264 Test  0.11204\n",
      "Seen 5920 samples\n",
      "Epoch  26 Update  9630 Cost  0.08391750688431454\n",
      "Epoch  26 Update  9640 Cost  0.012451290486571833\n",
      "Epoch  26 Update  9650 Cost  0.13151999145108925\n",
      "Epoch  26 Update  9660 Cost  0.04021295550381631\n",
      "Epoch  26 Update  9670 Cost  0.5496797961809223\n",
      "Epoch  26 Update  9680 Cost  0.01828992202166911\n",
      "Epoch  26 Update  9690 Cost  0.01795790035617603\n",
      "Epoch  26 Update  9700 Cost  0.0781364308900347\n",
      "Epoch  26 Update  9710 Cost  0.0394439599916277\n",
      "Epoch  26 Update  9720 Cost  0.4823323931289867\n",
      "Epoch  26 Update  9730 Cost  0.1390490265443835\n",
      "Epoch  26 Update  9740 Cost  0.12458676534398692\n",
      "Epoch  26 Update  9750 Cost  0.23080955376090817\n",
      "Epoch  26 Update  9760 Cost  0.00854185579701982\n",
      "Epoch  26 Update  9770 Cost  0.09587456956798937\n",
      "Epoch  26 Update  9780 Cost  0.028236304443720103\n",
      "Epoch  26 Update  9790 Cost  0.03276337731208151\n",
      "Epoch  26 Update  9800 Cost  0.04364613824625438\n",
      "Epoch  26 Update  9810 Cost  0.1863462978128391\n",
      "Epoch  26 Update  9820 Cost  0.14555861275978357\n",
      "Epoch  26 Update  9830 Cost  0.7459776119189213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  26 Update  9840 Cost  0.06255230463075775\n",
      "Epoch  26 Update  9850 Cost  0.08662256657237324\n",
      "Epoch  26 Update  9860 Cost  0.1389274760013865\n",
      "Epoch  26 Update  9870 Cost  0.017113524174438663\n",
      "Epoch  26 Update  9880 Cost  0.10163888075738174\n",
      "Epoch  26 Update  9890 Cost  0.3032244406234932\n",
      "Epoch  26 Update  9900 Cost  0.09214073592805258\n",
      "Epoch  26 Update  9910 Cost  0.27559220127239065\n",
      "Epoch  26 Update  9920 Cost  0.5258691476705017\n",
      "Epoch  26 Update  9930 Cost  0.047732614503293164\n",
      "Epoch  26 Update  9940 Cost  0.038918542570047794\n",
      "Epoch  26 Update  9950 Cost  0.19691224518713102\n",
      "Epoch  26 Update  9960 Cost  0.20357183036984325\n",
      "Epoch  26 Update  9970 Cost  0.035250877304566615\n",
      "Epoch  26 Update  9980 Cost  0.27925207691396603\n",
      "Epoch  26 Update  9990 Cost  0.1251470398520928\n",
      "Saving...\n",
      "Train  0.289684210526 Valid  0.1424 Test  0.12588\n",
      "Seen 5920 samples\n",
      "Epoch  27 Update  10000 Cost  0.0530663581395419\n",
      "Epoch  27 Update  10010 Cost  0.08861379824872195\n",
      "Epoch  27 Update  10020 Cost  0.08196704278946147\n",
      "Epoch  27 Update  10030 Cost  0.019046645436080527\n",
      "Epoch  27 Update  10040 Cost  0.0373357953218259\n",
      "Epoch  27 Update  10050 Cost  0.06582686880337978\n",
      "Epoch  27 Update  10060 Cost  0.018550550209810397\n",
      "Epoch  27 Update  10070 Cost  0.0788317887428858\n",
      "Epoch  27 Update  10080 Cost  0.023206628588732493\n",
      "Epoch  27 Update  10090 Cost  0.16651069152037706\n",
      "Epoch  27 Update  10100 Cost  0.30979304410866665\n",
      "Epoch  27 Update  10110 Cost  0.07680832544669405\n",
      "Epoch  27 Update  10120 Cost  0.08292677328783915\n",
      "Epoch  27 Update  10130 Cost  0.49463905353926485\n",
      "Epoch  27 Update  10140 Cost  0.021125373729763912\n",
      "Epoch  27 Update  10150 Cost  0.05549400089985064\n",
      "Epoch  27 Update  10160 Cost  0.08281626284202487\n",
      "Epoch  27 Update  10170 Cost  0.21578988703101343\n",
      "Epoch  27 Update  10180 Cost  0.013835073684331772\n",
      "Epoch  27 Update  10190 Cost  0.0043381839611706585\n",
      "Epoch  27 Update  10200 Cost  0.2169274514481847\n",
      "Epoch  27 Update  10210 Cost  0.032104587634419604\n",
      "Epoch  27 Update  10220 Cost  0.057304275445162645\n",
      "Epoch  27 Update  10230 Cost  0.1506382023096061\n",
      "Epoch  27 Update  10240 Cost  0.06830411850795268\n",
      "Epoch  27 Update  10250 Cost  0.1018095884724071\n",
      "Epoch  27 Update  10260 Cost  0.054167609708835766\n",
      "Epoch  27 Update  10270 Cost  0.17139904663983502\n",
      "Epoch  27 Update  10280 Cost  0.15015052874972237\n",
      "Epoch  27 Update  10290 Cost  0.2775381873495443\n",
      "Epoch  27 Update  10300 Cost  0.0326055774801118\n",
      "Epoch  27 Update  10310 Cost  0.04152452329195058\n",
      "Epoch  27 Update  10320 Cost  0.16147093540923785\n",
      "Epoch  27 Update  10330 Cost  0.16458680981139745\n",
      "Epoch  27 Update  10340 Cost  0.278874539981614\n",
      "Epoch  27 Update  10350 Cost  0.11200860424257933\n",
      "Epoch  27 Update  10360 Cost  0.06981722049203218\n",
      "Train  0.282105263158 Valid  0.1248 Test  0.11804\n",
      "Seen 5920 samples\n",
      "Epoch  28 Update  10370 Cost  0.0457942659129902\n",
      "Epoch  28 Update  10380 Cost  0.05207233085945411\n",
      "Epoch  28 Update  10390 Cost  0.04279224429694752\n",
      "Epoch  28 Update  10400 Cost  0.02117928474153732\n",
      "Epoch  28 Update  10410 Cost  0.06020386826937987\n",
      "Epoch  28 Update  10420 Cost  0.0849273577772949\n",
      "Epoch  28 Update  10430 Cost  0.057757289571037485\n",
      "Epoch  28 Update  10440 Cost  0.11625492521841059\n",
      "Epoch  28 Update  10450 Cost  0.1916654021614931\n",
      "Epoch  28 Update  10460 Cost  0.01740593460843329\n",
      "Epoch  28 Update  10470 Cost  0.25491539625292736\n",
      "Epoch  28 Update  10480 Cost  0.11366575924987277\n",
      "Epoch  28 Update  10490 Cost  0.1178440579423689\n",
      "Epoch  28 Update  10500 Cost  0.01878524897089929\n",
      "Epoch  28 Update  10510 Cost  0.29064636947640665\n",
      "Epoch  28 Update  10520 Cost  0.006586066832956423\n",
      "Epoch  28 Update  10530 Cost  0.09643313468602488\n",
      "Epoch  28 Update  10540 Cost  0.14318123931784546\n",
      "Epoch  28 Update  10550 Cost  0.007801237342257366\n",
      "Epoch  28 Update  10560 Cost  0.15596507910636553\n",
      "Epoch  28 Update  10570 Cost  0.008031336316211172\n",
      "Epoch  28 Update  10580 Cost  0.0834485412957272\n",
      "Epoch  28 Update  10590 Cost  0.05561359540348351\n",
      "Epoch  28 Update  10600 Cost  0.011098057476361459\n",
      "Epoch  28 Update  10610 Cost  0.018918869689785546\n",
      "Epoch  28 Update  10620 Cost  0.08148434769441888\n",
      "Epoch  28 Update  10630 Cost  0.02622477156934683\n",
      "Epoch  28 Update  10640 Cost  0.018492950097084102\n",
      "Epoch  28 Update  10650 Cost  0.018393082441593645\n",
      "Epoch  28 Update  10660 Cost  0.036790588191992935\n",
      "Epoch  28 Update  10670 Cost  0.09532314602632445\n",
      "Epoch  28 Update  10680 Cost  0.09415485077132074\n",
      "Epoch  28 Update  10690 Cost  0.042837864132700694\n",
      "Epoch  28 Update  10700 Cost  0.1370425729710805\n",
      "Epoch  28 Update  10710 Cost  0.020958470836696403\n",
      "Epoch  28 Update  10720 Cost  0.2241200826810452\n",
      "Epoch  28 Update  10730 Cost  0.1850753790413042\n",
      "Train  0.275831578947 Valid  0.1144 Test  0.11772\n",
      "Seen 5920 samples\n",
      "Epoch  29 Update  10740 Cost  0.09312084166175487\n",
      "Epoch  29 Update  10750 Cost  0.08128602831550095\n",
      "Epoch  29 Update  10760 Cost  0.0201683253902662\n",
      "Epoch  29 Update  10770 Cost  0.01340043867772303\n",
      "Epoch  29 Update  10780 Cost  0.004754438276062806\n",
      "Epoch  29 Update  10790 Cost  0.047106996728858624\n",
      "Epoch  29 Update  10800 Cost  0.13248733073212396\n",
      "Epoch  29 Update  10810 Cost  0.012787305933414093\n",
      "Epoch  29 Update  10820 Cost  0.018543766361327436\n",
      "Epoch  29 Update  10830 Cost  0.11459788294609329\n",
      "Epoch  29 Update  10840 Cost  0.07201595788661969\n",
      "Epoch  29 Update  10850 Cost  0.1070299344705082\n",
      "Epoch  29 Update  10860 Cost  0.007245711063287135\n",
      "Epoch  29 Update  10870 Cost  0.3866607781268609\n",
      "Epoch  29 Update  10880 Cost  0.0771073022448448\n",
      "Epoch  29 Update  10890 Cost  0.03933122543874382\n",
      "Epoch  29 Update  10900 Cost  0.03025509649739424\n",
      "Epoch  29 Update  10910 Cost  0.1316548265909432\n",
      "Epoch  29 Update  10920 Cost  0.17928175514452788\n",
      "Epoch  29 Update  10930 Cost  0.11734451806466259\n",
      "Epoch  29 Update  10940 Cost  0.04967732181174723\n",
      "Epoch  29 Update  10950 Cost  0.050070338275778475\n",
      "Epoch  29 Update  10960 Cost  0.01712520524738905\n",
      "Epoch  29 Update  10970 Cost  0.026656861377126983\n",
      "Epoch  29 Update  10980 Cost  0.22071564563950716\n",
      "Epoch  29 Update  10990 Cost  0.12382621593228492\n",
      "Epoch  29 Update  11000 Cost  0.049987980954767114\n",
      "Epoch  29 Update  11010 Cost  0.01990718345057288\n",
      "Epoch  29 Update  11020 Cost  0.13587272113171509\n",
      "Epoch  29 Update  11030 Cost  0.03358082176283295\n",
      "Epoch  29 Update  11040 Cost  0.26972173042783026\n",
      "Epoch  29 Update  11050 Cost  0.139797257960483\n",
      "Epoch  29 Update  11060 Cost  0.06709107409120615\n",
      "Epoch  29 Update  11070 Cost  0.09303981078311413\n",
      "Epoch  29 Update  11080 Cost  0.13974263805076317\n",
      "Epoch  29 Update  11090 Cost  0.005858217380687265\n",
      "Epoch  29 Update  11100 Cost  0.05314201924499265\n",
      "Saving...\n",
      "Train  0.275747368421 Valid  0.1088 Test  0.1134\n",
      "Seen 5920 samples\n",
      "Epoch  30 Update  11110 Cost  0.08199659420831369\n",
      "Epoch  30 Update  11120 Cost  0.052063576159071076\n",
      "Epoch  30 Update  11130 Cost  0.0048742352982324945\n",
      "Epoch  30 Update  11140 Cost  0.1218078602553047\n",
      "Epoch  30 Update  11150 Cost  0.04358499397806835\n",
      "Epoch  30 Update  11160 Cost  0.0338706440077275\n",
      "Epoch  30 Update  11170 Cost  0.0886587876897719\n",
      "Epoch  30 Update  11180 Cost  0.02951837519531945\n",
      "Epoch  30 Update  11190 Cost  0.13057214162237185\n",
      "Epoch  30 Update  11200 Cost  0.034476703479245814\n",
      "Epoch  30 Update  11210 Cost  0.10516216602420964\n",
      "Epoch  30 Update  11220 Cost  0.18563153329134996\n",
      "Epoch  30 Update  11230 Cost  0.03200181384597724\n",
      "Epoch  30 Update  11240 Cost  0.2343482099385138\n",
      "Epoch  30 Update  11250 Cost  0.03130866801637051\n",
      "Epoch  30 Update  11260 Cost  0.01255558818729237\n",
      "Epoch  30 Update  11270 Cost  0.4131553116539805\n",
      "Epoch  30 Update  11280 Cost  0.029152377757931426\n",
      "Epoch  30 Update  11290 Cost  0.20186357146619735\n",
      "Epoch  30 Update  11300 Cost  0.058698588372488016\n",
      "Epoch  30 Update  11310 Cost  0.015695724721538275\n",
      "Epoch  30 Update  11320 Cost  0.023452328294464638\n",
      "Epoch  30 Update  11330 Cost  0.058209103871956665\n",
      "Epoch  30 Update  11340 Cost  0.04874394576911414\n",
      "Epoch  30 Update  11350 Cost  0.009130473255180044\n",
      "Epoch  30 Update  11360 Cost  0.14491952164367883\n",
      "Epoch  30 Update  11370 Cost  0.07041819639295518\n",
      "Epoch  30 Update  11380 Cost  0.05693312707185208\n",
      "Epoch  30 Update  11390 Cost  0.005327144786917204\n",
      "Epoch  30 Update  11400 Cost  0.024803021362530694\n",
      "Epoch  30 Update  11410 Cost  0.00508835365815327\n",
      "Epoch  30 Update  11420 Cost  0.03976570066855314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  30 Update  11430 Cost  0.08093181241378197\n",
      "Epoch  30 Update  11440 Cost  0.029200543044968805\n",
      "Epoch  30 Update  11450 Cost  0.009253741607913061\n",
      "Epoch  30 Update  11460 Cost  0.0470845508782454\n",
      "Epoch  30 Update  11470 Cost  0.02228663325619247\n",
      "Train  0.273010526316 Valid  0.1192 Test  0.11184\n",
      "Seen 5920 samples\n",
      "Epoch  31 Update  11480 Cost  0.11566904910274636\n",
      "Epoch  31 Update  11490 Cost  0.14453163941070155\n",
      "Epoch  31 Update  11500 Cost  0.18634900584100786\n",
      "Epoch  31 Update  11510 Cost  0.11935294586254738\n",
      "Epoch  31 Update  11520 Cost  0.053824848563786554\n",
      "Epoch  31 Update  11530 Cost  0.018328434345830542\n",
      "Epoch  31 Update  11540 Cost  0.09415479302404911\n",
      "Epoch  31 Update  11550 Cost  0.23668795468687348\n",
      "Epoch  31 Update  11560 Cost  0.02046434195468793\n",
      "Epoch  31 Update  11570 Cost  0.02535666462065755\n",
      "Epoch  31 Update  11580 Cost  0.06859626675079296\n",
      "Epoch  31 Update  11590 Cost  0.013731074765246345\n",
      "Epoch  31 Update  11600 Cost  0.0782558054595381\n",
      "Epoch  31 Update  11610 Cost  0.0030924446037499807\n",
      "Epoch  31 Update  11620 Cost  0.2155748397724334\n",
      "Epoch  31 Update  11630 Cost  0.017151261260882525\n",
      "Epoch  31 Update  11640 Cost  0.015950577259181073\n",
      "Epoch  31 Update  11650 Cost  0.010624176902044\n",
      "Epoch  31 Update  11660 Cost  0.11140419004668246\n",
      "Epoch  31 Update  11670 Cost  0.07214080979273825\n",
      "Epoch  31 Update  11680 Cost  0.002870182598998899\n",
      "Epoch  31 Update  11690 Cost  0.005373448519587449\n",
      "Epoch  31 Update  11700 Cost  0.022218819533869578\n",
      "Epoch  31 Update  11710 Cost  0.08991349272047396\n",
      "Epoch  31 Update  11720 Cost  0.06340401873014562\n",
      "Epoch  31 Update  11730 Cost  0.010092122541718575\n",
      "Epoch  31 Update  11740 Cost  0.004625752578143702\n",
      "Epoch  31 Update  11750 Cost  0.11867471175919958\n",
      "Epoch  31 Update  11760 Cost  0.03056504611012947\n",
      "Epoch  31 Update  11770 Cost  0.09949103865500178\n",
      "Epoch  31 Update  11780 Cost  0.2535049827751479\n",
      "Epoch  31 Update  11790 Cost  0.010560858150017277\n",
      "Epoch  31 Update  11800 Cost  0.01235157798535829\n",
      "Epoch  31 Update  11810 Cost  0.057041557301830734\n",
      "Epoch  31 Update  11820 Cost  0.13435265684069564\n",
      "Epoch  31 Update  11830 Cost  0.04123718206024251\n",
      "Epoch  31 Update  11840 Cost  0.016020402084133273\n",
      "Train  0.273178947368 Valid  0.1216 Test  0.11708\n",
      "Seen 5920 samples\n",
      "Epoch  32 Update  11850 Cost  0.0015328509676643231\n",
      "Epoch  32 Update  11860 Cost  0.008612973926349956\n",
      "Epoch  32 Update  11870 Cost  0.19246294234308903\n",
      "Epoch  32 Update  11880 Cost  0.08196650395777952\n",
      "Epoch  32 Update  11890 Cost  0.07199216969804528\n",
      "Epoch  32 Update  11900 Cost  0.007411357985418695\n",
      "Epoch  32 Update  11910 Cost  0.014091773301023097\n",
      "Epoch  32 Update  11920 Cost  0.07539873434534031\n",
      "Epoch  32 Update  11930 Cost  0.23179148044631817\n",
      "Epoch  32 Update  11940 Cost  0.023649030868332577\n",
      "Epoch  32 Update  11950 Cost  0.07198117047307726\n",
      "Epoch  32 Update  11960 Cost  0.10062748170784012\n",
      "Epoch  32 Update  11970 Cost  0.013592703329404312\n",
      "Epoch  32 Update  11980 Cost  0.1539021042889133\n",
      "Epoch  32 Update  11990 Cost  0.08048005973167459\n",
      "Epoch  32 Update  12000 Cost  0.17172286693913724\n",
      "Epoch  32 Update  12010 Cost  0.005230693879638415\n",
      "Epoch  32 Update  12020 Cost  0.018242518769709705\n",
      "Epoch  32 Update  12030 Cost  0.0038013789611012153\n",
      "Epoch  32 Update  12040 Cost  0.05391454121028208\n",
      "Epoch  32 Update  12050 Cost  0.009493329215081406\n",
      "Epoch  32 Update  12060 Cost  0.021627043470890395\n",
      "Epoch  32 Update  12070 Cost  0.1731996538033233\n",
      "Epoch  32 Update  12080 Cost  0.062439414298700445\n",
      "Epoch  32 Update  12090 Cost  0.4224755342170635\n",
      "Epoch  32 Update  12100 Cost  0.48403617441926444\n",
      "Epoch  32 Update  12110 Cost  0.3035598151485657\n",
      "Epoch  32 Update  12120 Cost  0.0230655855604336\n",
      "Epoch  32 Update  12130 Cost  0.11789550038163255\n",
      "Epoch  32 Update  12140 Cost  0.1717364435354565\n",
      "Epoch  32 Update  12150 Cost  0.21977105680425513\n",
      "Epoch  32 Update  12160 Cost  0.020202069017615967\n",
      "Epoch  32 Update  12170 Cost  0.022939897270682626\n",
      "Epoch  32 Update  12180 Cost  0.2939771792819164\n",
      "Epoch  32 Update  12190 Cost  0.1302234616318055\n",
      "Epoch  32 Update  12200 Cost  0.07067499951454546\n",
      "Epoch  32 Update  12210 Cost  0.03019481122195991\n",
      "Saving...\n",
      "Train  0.268926315789 Valid  0.1184 Test  0.10956\n",
      "Seen 5920 samples\n",
      "Epoch  33 Update  12220 Cost  0.10296454445987051\n",
      "Epoch  33 Update  12230 Cost  0.1439326930711508\n",
      "Epoch  33 Update  12240 Cost  0.07227389597413218\n",
      "Epoch  33 Update  12250 Cost  0.10460427204609263\n",
      "Epoch  33 Update  12260 Cost  0.020175054488576313\n",
      "Epoch  33 Update  12270 Cost  0.024898704972922833\n",
      "Epoch  33 Update  12280 Cost  0.12680186010430602\n",
      "Epoch  33 Update  12290 Cost  0.2417249445363983\n",
      "Epoch  33 Update  12300 Cost  0.00437925252644025\n",
      "Epoch  33 Update  12310 Cost  0.015311891008078948\n",
      "Epoch  33 Update  12320 Cost  0.5981400477602299\n",
      "Epoch  33 Update  12330 Cost  0.45186453268094123\n",
      "Epoch  33 Update  12340 Cost  0.08346940372179654\n",
      "Epoch  33 Update  12350 Cost  0.1620923764752363\n",
      "Epoch  33 Update  12360 Cost  0.0850867012084422\n",
      "Epoch  33 Update  12370 Cost  0.07088372897306712\n",
      "Epoch  33 Update  12380 Cost  0.07300083892027362\n",
      "Epoch  33 Update  12390 Cost  0.3197289309147733\n",
      "Epoch  33 Update  12400 Cost  0.013985085876384484\n",
      "Epoch  33 Update  12410 Cost  0.07097576084784173\n",
      "Epoch  33 Update  12420 Cost  0.04962301999590513\n",
      "Epoch  33 Update  12430 Cost  0.014046609923520213\n",
      "Epoch  33 Update  12440 Cost  0.003898300187119511\n",
      "Epoch  33 Update  12450 Cost  0.01379116540843797\n",
      "Epoch  33 Update  12460 Cost  0.03825271509584133\n",
      "Epoch  33 Update  12470 Cost  0.048873401130710196\n",
      "Epoch  33 Update  12480 Cost  0.016529503233944396\n",
      "Epoch  33 Update  12490 Cost  0.06336311763816425\n",
      "Epoch  33 Update  12500 Cost  0.015752162227700425\n",
      "Epoch  33 Update  12510 Cost  0.1526279541305684\n",
      "Epoch  33 Update  12520 Cost  0.02739163119099424\n",
      "Epoch  33 Update  12530 Cost  0.5998079775514538\n",
      "Epoch  33 Update  12540 Cost  0.02210790817686269\n",
      "Epoch  33 Update  12550 Cost  0.03249779968864199\n",
      "Epoch  33 Update  12560 Cost  0.14927722951347605\n",
      "Epoch  33 Update  12570 Cost  0.02592189235806489\n",
      "Epoch  33 Update  12580 Cost  0.027941143611707738\n",
      "Train  0.266989473684 Valid  0.1104 Test  0.11272\n",
      "Seen 5920 samples\n",
      "Epoch  34 Update  12590 Cost  0.015410074147341407\n",
      "Epoch  34 Update  12600 Cost  0.05823298721753932\n",
      "Epoch  34 Update  12610 Cost  0.0070246319652551735\n",
      "Epoch  34 Update  12620 Cost  0.00847443242141487\n",
      "Epoch  34 Update  12630 Cost  0.14640947858954484\n",
      "Epoch  34 Update  12640 Cost  0.07435070208301889\n",
      "Epoch  34 Update  12650 Cost  0.0031632181634180644\n",
      "Epoch  34 Update  12660 Cost  0.0035204832128391357\n",
      "Epoch  34 Update  12670 Cost  0.029421580069684542\n",
      "Epoch  34 Update  12680 Cost  0.02765107234186772\n",
      "Epoch  34 Update  12690 Cost  0.2849392003588286\n",
      "Epoch  34 Update  12700 Cost  0.5147411373497005\n",
      "Epoch  34 Update  12710 Cost  0.10683674172585914\n",
      "Epoch  34 Update  12720 Cost  0.16991758655922937\n",
      "Epoch  34 Update  12730 Cost  0.032845428869433355\n",
      "Epoch  34 Update  12740 Cost  0.23003056866017624\n",
      "Epoch  34 Update  12750 Cost  0.05564315137419634\n",
      "Epoch  34 Update  12760 Cost  0.07805815753891071\n",
      "Epoch  34 Update  12770 Cost  0.30836646288460823\n",
      "Epoch  34 Update  12780 Cost  0.0024744133207406063\n",
      "Epoch  34 Update  12790 Cost  0.13861999518002768\n",
      "Epoch  34 Update  12800 Cost  0.08909911636753082\n",
      "Epoch  34 Update  12810 Cost  0.03968204092909595\n",
      "Epoch  34 Update  12820 Cost  0.020471927609341148\n",
      "Epoch  34 Update  12830 Cost  0.14278321983664843\n",
      "Epoch  34 Update  12840 Cost  0.052317578264102535\n",
      "Epoch  34 Update  12850 Cost  0.007625533503570176\n",
      "Epoch  34 Update  12860 Cost  0.040142065987086625\n",
      "Epoch  34 Update  12870 Cost  0.022066083667187063\n",
      "Epoch  34 Update  12880 Cost  0.09865377907764354\n",
      "Epoch  34 Update  12890 Cost  0.34266583093116115\n",
      "Epoch  34 Update  12900 Cost  0.0160083747913145\n",
      "Epoch  34 Update  12910 Cost  0.013205766285418266\n",
      "Epoch  34 Update  12920 Cost  0.007014054591165168\n",
      "Epoch  34 Update  12930 Cost  0.02886037149024308\n",
      "Epoch  34 Update  12940 Cost  0.015747302889675636\n",
      "Epoch  34 Update  12950 Cost  0.01154376965641123\n",
      "Train  0.269347368421 Valid  0.1184 Test  0.11576\n",
      "Seen 5920 samples\n",
      "Epoch  35 Update  12960 Cost  0.39939966111897685\n",
      "Epoch  35 Update  12970 Cost  0.01156751807041932\n",
      "Epoch  35 Update  12980 Cost  0.02026858340118809\n",
      "Epoch  35 Update  12990 Cost  0.2758586206608133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  35 Update  13000 Cost  0.02917736723597089\n",
      "Epoch  35 Update  13010 Cost  0.02254457570438242\n",
      "Epoch  35 Update  13020 Cost  0.020035647028754343\n",
      "Epoch  35 Update  13030 Cost  0.09400820956189138\n",
      "Epoch  35 Update  13040 Cost  0.008181062166541629\n",
      "Epoch  35 Update  13050 Cost  0.02460162221184207\n",
      "Epoch  35 Update  13060 Cost  0.0687124835351568\n",
      "Epoch  35 Update  13070 Cost  0.04299369162509629\n",
      "Epoch  35 Update  13080 Cost  0.006209258203730679\n",
      "Epoch  35 Update  13090 Cost  0.01617768905564663\n",
      "Epoch  35 Update  13100 Cost  0.07960769304573267\n",
      "Epoch  35 Update  13110 Cost  0.06399464538144481\n",
      "Epoch  35 Update  13120 Cost  0.1517893026827274\n",
      "Epoch  35 Update  13130 Cost  0.012119599711194461\n",
      "Epoch  35 Update  13140 Cost  0.09763975764178076\n",
      "Epoch  35 Update  13150 Cost  0.007315280817807185\n",
      "Epoch  35 Update  13160 Cost  0.03542395495927937\n",
      "Epoch  35 Update  13170 Cost  0.007278961467004838\n",
      "Epoch  35 Update  13180 Cost  0.005879884799986461\n",
      "Epoch  35 Update  13190 Cost  0.015700095205247434\n",
      "Epoch  35 Update  13200 Cost  0.020310111656638912\n",
      "Epoch  35 Update  13210 Cost  0.06335860587111561\n",
      "Epoch  35 Update  13220 Cost  0.10333800298181572\n",
      "Epoch  35 Update  13230 Cost  0.5062828296780234\n",
      "Epoch  35 Update  13240 Cost  0.05623745635024229\n",
      "Epoch  35 Update  13250 Cost  0.010842629673723953\n",
      "Epoch  35 Update  13260 Cost  0.01753709033921552\n",
      "Epoch  35 Update  13270 Cost  0.007896733137963557\n",
      "Epoch  35 Update  13280 Cost  0.0007562245641880015\n",
      "Epoch  35 Update  13290 Cost  0.2651271413694046\n",
      "Epoch  35 Update  13300 Cost  0.017741658738774854\n",
      "Epoch  35 Update  13310 Cost  0.0399193619210948\n",
      "Epoch  35 Update  13320 Cost  0.0175670008872016\n",
      "Saving...\n",
      "Train  0.267873684211 Valid  0.1096 Test  0.1152\n",
      "Seen 5920 samples\n",
      "Epoch  36 Update  13330 Cost  0.19359473008902628\n",
      "Epoch  36 Update  13340 Cost  0.09111199352773205\n",
      "Epoch  36 Update  13350 Cost  0.014140701878065614\n",
      "Epoch  36 Update  13360 Cost  0.05727453378273967\n",
      "Epoch  36 Update  13370 Cost  0.009131954390229272\n",
      "Epoch  36 Update  13380 Cost  0.0021656818490498626\n",
      "Epoch  36 Update  13390 Cost  0.053563516464926894\n",
      "Epoch  36 Update  13400 Cost  0.017987669921162696\n",
      "Epoch  36 Update  13410 Cost  0.011883002173319066\n",
      "Epoch  36 Update  13420 Cost  0.015676236189613755\n",
      "Epoch  36 Update  13430 Cost  0.019115016264084614\n",
      "Epoch  36 Update  13440 Cost  0.03589685271599138\n",
      "Epoch  36 Update  13450 Cost  0.24703974129558018\n",
      "Epoch  36 Update  13460 Cost  0.0227804693207945\n",
      "Epoch  36 Update  13470 Cost  0.004763435239909505\n",
      "Epoch  36 Update  13480 Cost  0.01136761766906911\n",
      "Epoch  36 Update  13490 Cost  0.02851272253478557\n",
      "Epoch  36 Update  13500 Cost  0.08451352217475007\n",
      "Epoch  36 Update  13510 Cost  0.020253724815211396\n",
      "Epoch  36 Update  13520 Cost  0.02036868264800524\n",
      "Epoch  36 Update  13530 Cost  0.03244429554070152\n",
      "Epoch  36 Update  13540 Cost  0.03205811277859328\n",
      "Epoch  36 Update  13550 Cost  0.06595182456974875\n",
      "Epoch  36 Update  13560 Cost  0.06811319588407821\n",
      "Epoch  36 Update  13570 Cost  0.24880315327636796\n",
      "Epoch  36 Update  13580 Cost  0.10812178347626654\n",
      "Epoch  36 Update  13590 Cost  0.049609841582650784\n",
      "Epoch  36 Update  13600 Cost  0.0003146193426328086\n",
      "Epoch  36 Update  13610 Cost  0.024132668888325697\n",
      "Epoch  36 Update  13620 Cost  0.029779466124727143\n",
      "Epoch  36 Update  13630 Cost  0.02600150048787232\n",
      "Epoch  36 Update  13640 Cost  0.08512700173829742\n",
      "Epoch  36 Update  13650 Cost  0.06433853805020245\n",
      "Epoch  36 Update  13660 Cost  0.20596412242649842\n",
      "Epoch  36 Update  13670 Cost  0.032452137395630395\n",
      "Epoch  36 Update  13680 Cost  0.06822556408177027\n",
      "Epoch  36 Update  13690 Cost  0.3590853579585239\n",
      "Train  0.274105263158 Valid  0.1176 Test  0.12608\n",
      "Seen 5920 samples\n",
      "Epoch  37 Update  13700 Cost  0.09474999016153517\n",
      "Epoch  37 Update  13710 Cost  0.014019940200380927\n",
      "Epoch  37 Update  13720 Cost  0.13078504431262394\n",
      "Epoch  37 Update  13730 Cost  0.015602608569919588\n",
      "Epoch  37 Update  13740 Cost  0.008308236996502901\n",
      "Epoch  37 Update  13750 Cost  0.10629498581263366\n",
      "Epoch  37 Update  13760 Cost  0.01925324467857852\n",
      "Epoch  37 Update  13770 Cost  0.01780996574890487\n",
      "Epoch  37 Update  13780 Cost  0.05069419081147573\n",
      "Epoch  37 Update  13790 Cost  0.010765086738420583\n",
      "Epoch  37 Update  13800 Cost  0.16678254157905262\n",
      "Epoch  37 Update  13810 Cost  0.007670354528738541\n",
      "Epoch  37 Update  13820 Cost  0.035452179082624956\n",
      "Epoch  37 Update  13830 Cost  0.008179167447226715\n",
      "Epoch  37 Update  13840 Cost  0.14417079045553294\n",
      "Epoch  37 Update  13850 Cost  0.12437001186627865\n",
      "Epoch  37 Update  13860 Cost  0.036957907770857716\n",
      "Epoch  37 Update  13870 Cost  0.1762499732817809\n",
      "Epoch  37 Update  13880 Cost  0.10083531061634418\n",
      "Epoch  37 Update  13890 Cost  0.002434293873067392\n",
      "Epoch  37 Update  13900 Cost  0.04882217202759846\n",
      "Epoch  37 Update  13910 Cost  0.02239395847469879\n",
      "Epoch  37 Update  13920 Cost  0.03931212845560465\n",
      "Epoch  37 Update  13930 Cost  0.22000117598820385\n",
      "Epoch  37 Update  13940 Cost  0.004372443006106638\n",
      "Epoch  37 Update  13950 Cost  0.0009788328954519552\n",
      "Epoch  37 Update  13960 Cost  0.003760991053383916\n",
      "Epoch  37 Update  13970 Cost  0.22103036495189007\n",
      "Epoch  37 Update  13980 Cost  0.01674106320043518\n",
      "Epoch  37 Update  13990 Cost  0.003237081704633921\n",
      "Epoch  37 Update  14000 Cost  0.006575296596829064\n",
      "Epoch  37 Update  14010 Cost  0.06889827040943154\n",
      "Epoch  37 Update  14020 Cost  0.02072713479981556\n",
      "Epoch  37 Update  14030 Cost  0.06616800700066884\n",
      "Epoch  37 Update  14040 Cost  0.03858321586555564\n",
      "Epoch  37 Update  14050 Cost  0.01526253141460916\n",
      "Epoch  37 Update  14060 Cost  0.017473442852711234\n",
      "Train  0.263957894737 Valid  0.1216 Test  0.11452\n",
      "Seen 5920 samples\n",
      "Epoch  38 Update  14070 Cost  0.00476016480861029\n",
      "Epoch  38 Update  14080 Cost  0.0773922526092044\n",
      "Epoch  38 Update  14090 Cost  0.0013396713164256547\n",
      "Epoch  38 Update  14100 Cost  0.019023884703647267\n",
      "Epoch  38 Update  14110 Cost  0.013685228699473011\n",
      "Epoch  38 Update  14120 Cost  0.054376180236171565\n",
      "Epoch  38 Update  14130 Cost  0.04268731341211199\n",
      "Epoch  38 Update  14140 Cost  0.011978022227785247\n",
      "Epoch  38 Update  14150 Cost  0.02445975083374784\n",
      "Epoch  38 Update  14160 Cost  0.03837817461254954\n",
      "Epoch  38 Update  14170 Cost  0.012148980720928874\n",
      "Epoch  38 Update  14180 Cost  0.00540588029313135\n",
      "Epoch  38 Update  14190 Cost  0.03616221568969868\n",
      "Epoch  38 Update  14200 Cost  0.03618181727899638\n",
      "Epoch  38 Update  14210 Cost  0.08135454819479249\n",
      "Epoch  38 Update  14220 Cost  0.18765652688422804\n",
      "Epoch  38 Update  14230 Cost  0.13315494089779376\n",
      "Epoch  38 Update  14240 Cost  0.1510675828796328\n",
      "Epoch  38 Update  14250 Cost  0.03222787387533076\n",
      "Epoch  38 Update  14260 Cost  0.019130233791846574\n",
      "Epoch  38 Update  14270 Cost  0.0024425827024044827\n",
      "Epoch  38 Update  14280 Cost  0.18416694810482445\n",
      "Epoch  38 Update  14290 Cost  0.1230572431615585\n",
      "Epoch  38 Update  14300 Cost  0.04608373324355667\n",
      "Epoch  38 Update  14310 Cost  0.12571598594829592\n",
      "Epoch  38 Update  14320 Cost  0.08411804133629056\n",
      "Epoch  38 Update  14330 Cost  0.04956155444161942\n",
      "Epoch  38 Update  14340 Cost  0.02107873251981509\n",
      "Epoch  38 Update  14350 Cost  0.006259032477660363\n",
      "Epoch  38 Update  14360 Cost  0.0014761706916023485\n",
      "Epoch  38 Update  14370 Cost  0.18063294952994033\n",
      "Epoch  38 Update  14380 Cost  0.03419821673065655\n",
      "Epoch  38 Update  14390 Cost  0.08201251615879514\n",
      "Epoch  38 Update  14400 Cost  0.005309747826590953\n",
      "Epoch  38 Update  14410 Cost  0.025172522377117788\n",
      "Epoch  38 Update  14420 Cost  0.025414281735329333\n",
      "Epoch  38 Update  14430 Cost  0.01378366701628691\n",
      "Saving...\n",
      "Train  0.267157894737 Valid  0.1192 Test  0.12184\n",
      "Seen 5920 samples\n",
      "Epoch  39 Update  14440 Cost  0.047129267791249624\n",
      "Epoch  39 Update  14450 Cost  0.01948413338842242\n",
      "Epoch  39 Update  14460 Cost  0.023268535512584874\n",
      "Epoch  39 Update  14470 Cost  0.09951461591312248\n",
      "Epoch  39 Update  14480 Cost  0.018465487293839046\n",
      "Epoch  39 Update  14490 Cost  0.00652757618301981\n",
      "Epoch  39 Update  14500 Cost  0.042882801908668576\n",
      "Epoch  39 Update  14510 Cost  0.08140347290082721\n",
      "Epoch  39 Update  14520 Cost  0.022669128140186227\n",
      "Epoch  39 Update  14530 Cost  0.09235141611803625\n",
      "Epoch  39 Update  14540 Cost  0.14713938463879395\n",
      "Epoch  39 Update  14550 Cost  0.004379409266839961\n",
      "Epoch  39 Update  14560 Cost  0.11681563306694354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  39 Update  14570 Cost  0.04867484720119101\n",
      "Epoch  39 Update  14580 Cost  0.7097022203426937\n",
      "Epoch  39 Update  14590 Cost  0.047439784832913175\n",
      "Epoch  39 Update  14600 Cost  0.011138696523661892\n",
      "Epoch  39 Update  14610 Cost  0.22793059859773343\n",
      "Epoch  39 Update  14620 Cost  0.006823050359283044\n",
      "Epoch  39 Update  14630 Cost  0.0580825238222168\n",
      "Epoch  39 Update  14640 Cost  0.018682092279270195\n",
      "Epoch  39 Update  14650 Cost  0.10652335921254615\n",
      "Epoch  39 Update  14660 Cost  0.10626073385484684\n",
      "Epoch  39 Update  14670 Cost  0.010670974950892378\n",
      "Epoch  39 Update  14680 Cost  0.013163165224477497\n",
      "Epoch  39 Update  14690 Cost  0.02377081421353925\n",
      "Epoch  39 Update  14700 Cost  0.013958076534691619\n",
      "Epoch  39 Update  14710 Cost  0.04768764149450124\n",
      "Epoch  39 Update  14720 Cost  0.042831125820486725\n",
      "Epoch  39 Update  14730 Cost  0.0035230864941113156\n",
      "Epoch  39 Update  14740 Cost  0.00541419585720383\n",
      "Epoch  39 Update  14750 Cost  0.005974057440924076\n",
      "Epoch  39 Update  14760 Cost  0.0012882842507012866\n",
      "Epoch  39 Update  14770 Cost  0.03483615536760294\n",
      "Epoch  39 Update  14780 Cost  0.012946494663329408\n",
      "Epoch  39 Update  14790 Cost  0.0034708385214160887\n",
      "Epoch  39 Update  14800 Cost  0.2890975154622175\n",
      "Train  0.273894736842 Valid  0.136 Test  0.132\n",
      "Early Stop!\n",
      "Seen 5920 samples\n",
      "Train  0.0287578947368 Valid  0.1088 Test  0.1134\n",
      "The code run for 40 epochs, with 802.936201 sec/epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training took 32117.4s\n"
     ]
    }
   ],
   "source": [
    "train_model(encoder='lstm', gensim_Wemb=True, dim_proj=128, n_words=5000, maxlen=400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
