{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proprocess raw texts to digital indices and train them to word embeddings via gensim's word2vec\n",
    "\n",
    "**Libraries used:**\n",
    "1. nltk: used to tokenize text\n",
    "\n",
    "2. gensim: use its models.word2vec to produce word vectors.\n",
    "\n",
    "**Note: This is summerized in a more concise python file Wemb_gensim.py **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "import nltk\n",
    "import numpy\n",
    "import pickle as pkl\n",
    "import sys\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Strip punctuation/symbols from words\n",
    "def normalize_text(text):\n",
    "    norm_text = text.lower()\n",
    "    \n",
    "    # Replace breaks with spaces\n",
    "    norm_text = norm_text.replace('<br />', ' ')\n",
    "\n",
    "    # Pad punctuation with spaces on both sides\n",
    "    for char in [':', '\"', ',', '(', ')', '!', '?', ';', '*']:\n",
    "        norm_text = norm_text.replace(char, ' ')\n",
    "\n",
    "    norm_text = norm_text.replace('.', ' ' + '.')\n",
    "    return norm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_dataset(path):\n",
    "    if sys.version > '3':\n",
    "        control_chars = [chr(0x85)] # UTF-8\n",
    "    else:\n",
    "        control_chars = [unichr(0x85)] # UTF-16 which is often used in other language codes (e.g. Chinese)\n",
    "\n",
    "    dataset = []\n",
    "    currdir = os.getcwd()\n",
    "    os.chdir(path)\n",
    "    #i = 0\n",
    "    for ff in glob.glob(\"*.txt\"):\n",
    "        #i += 1\n",
    "        with open(ff, \"r\") as f:\n",
    "            line_txt = f.readline().strip()\n",
    "            line_norm = normalize_text(line_txt)\n",
    "            dataset.append(line_norm)\n",
    "        #if(i==100):\n",
    "            #break\n",
    "    os.chdir(currdir)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dict(path, Wemb_size=128, iter=10):\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "\n",
    "    sentences_pos = read_dataset(path+'/pos/')\n",
    "    sentences_neg = read_dataset(path+'/neg/')\n",
    "\n",
    "    sentences_train = sentences_pos + sentences_neg\n",
    "    tokenized_sentences = [nltk.word_tokenize(sent.decode(\"utf8\").encode('ascii', 'ignore')) for sent in sentences_train]\n",
    "\n",
    "    model = gensim.models.Word2Vec(tokenized_sentences, min_count=1,\n",
    "                                   size=Wemb_size, window=5,\n",
    "                                   workers=cores, iter=iter)\n",
    "\n",
    "    tok_sents_pos = tokenized_sentences[:len(sentences_pos)]\n",
    "    tok_sents_neg = tokenized_sentences[len(sentences_pos):]\n",
    "\n",
    "    return {'model': model, 'tok_sents_pos': tok_sents_pos, 'tok_sents_neg': tok_sents_neg}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert words in text sentences to their corresponding indices in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence2idx(tokenized_sentences, model):\n",
    "    idx = []\n",
    "    for tok_sen in tokenized_sentences:\n",
    "        idx_sent = numpy.zeros(len(tok_sen), dtype=numpy.int)\n",
    "        for i, word in enumerate(tok_sen):\n",
    "            if word in model.wv.vocab:\n",
    "                idx_sent[i] = model.wv.vocab[word].index\n",
    "            else:\n",
    "                idx_sent[i] = model.wv.vocab['.'].index\n",
    "        idx.append(idx_sent)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert words in the dictionary into embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, there is no need to create word embedding specifically, because it is included in the trained gensim model and can be acessed by `model.wv.syn0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_Wemb(model, Wemb_size=128):\n",
    "    Wemb = numpy.zeros((len(model.wv.vocab), Wemb_size))\n",
    "    for i in range(len(model.wv.vocab)):\n",
    "        embedding_vector = model.wv[model.wv.index2word[i]]\n",
    "        if embedding_vector is not None:\n",
    "            Wemb[i] = embedding_vector\n",
    "    return Wemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '../data/aclImdb/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lifa08/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-12 15:44:25,902 : INFO : collecting all words and their counts\n",
      "2018-07-12 15:44:25,903 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-07-12 15:44:26,854 : INFO : PROGRESS: at sentence #10000, processed 2518094 words, keeping 62658 word types\n",
      "2018-07-12 15:44:27,847 : INFO : PROGRESS: at sentence #20000, processed 5010152 words, keeping 89834 word types\n",
      "2018-07-12 15:44:28,313 : INFO : collected 100352 word types from a corpus of 6250510 raw words and 25000 sentences\n",
      "2018-07-12 15:44:28,315 : INFO : Loading a fresh vocabulary\n",
      "2018-07-12 15:44:29,472 : INFO : effective_min_count=1 retains 100352 unique words (100% of original 100352, drops 0)\n",
      "2018-07-12 15:44:29,473 : INFO : effective_min_count=1 leaves 6250510 word corpus (100% of original 6250510, drops 0)\n",
      "2018-07-12 15:44:29,822 : INFO : deleting the raw counts dictionary of 100352 items\n",
      "2018-07-12 15:44:29,832 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2018-07-12 15:44:29,833 : INFO : downsampling leaves estimated 4558796 word corpus (72.9% of prior 6250510)\n",
      "2018-07-12 15:44:30,295 : INFO : estimated required memory for 100352 words and 128 dimensions: 152936448 bytes\n",
      "2018-07-12 15:44:30,297 : INFO : resetting layer weights\n",
      "2018-07-12 15:44:31,715 : INFO : training model with 4 workers on 100352 vocabulary and 128 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-07-12 15:44:32,736 : INFO : EPOCH 1 - PROGRESS: at 16.46% examples, 747920 words/s, in_qsize 7, out_qsize 1\n",
      "2018-07-12 15:44:33,736 : INFO : EPOCH 1 - PROGRESS: at 34.22% examples, 780817 words/s, in_qsize 8, out_qsize 1\n",
      "2018-07-12 15:44:34,745 : INFO : EPOCH 1 - PROGRESS: at 51.53% examples, 787146 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:44:35,740 : INFO : EPOCH 1 - PROGRESS: at 68.63% examples, 783300 words/s, in_qsize 8, out_qsize 2\n",
      "2018-07-12 15:44:36,757 : INFO : EPOCH 1 - PROGRESS: at 85.68% examples, 779469 words/s, in_qsize 6, out_qsize 2\n",
      "2018-07-12 15:44:37,578 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-12 15:44:37,583 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-12 15:44:37,593 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-12 15:44:37,596 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-12 15:44:37,598 : INFO : EPOCH - 1 : training on 6250510 raw words (4559224 effective words) took 5.9s, 776231 effective words/s\n",
      "2018-07-12 15:44:38,603 : INFO : EPOCH 2 - PROGRESS: at 15.58% examples, 711434 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:44:39,621 : INFO : EPOCH 2 - PROGRESS: at 32.18% examples, 731155 words/s, in_qsize 8, out_qsize 1\n",
      "2018-07-12 15:44:40,642 : INFO : EPOCH 2 - PROGRESS: at 40.01% examples, 603505 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:44:41,655 : INFO : EPOCH 2 - PROGRESS: at 47.01% examples, 533902 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:44:42,693 : INFO : EPOCH 2 - PROGRESS: at 56.75% examples, 512231 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:44:43,707 : INFO : EPOCH 2 - PROGRESS: at 67.67% examples, 508709 words/s, in_qsize 8, out_qsize 1\n",
      "2018-07-12 15:44:44,711 : INFO : EPOCH 2 - PROGRESS: at 80.41% examples, 516749 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:44:45,725 : INFO : EPOCH 2 - PROGRESS: at 93.44% examples, 524915 words/s, in_qsize 7, out_qsize 1\n",
      "2018-07-12 15:44:46,110 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-12 15:44:46,119 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-12 15:44:46,122 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-12 15:44:46,126 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-12 15:44:46,128 : INFO : EPOCH - 2 : training on 6250510 raw words (4557958 effective words) took 8.5s, 534560 effective words/s\n",
      "2018-07-12 15:44:47,142 : INFO : EPOCH 3 - PROGRESS: at 12.70% examples, 580984 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:44:48,154 : INFO : EPOCH 3 - PROGRESS: at 22.40% examples, 508505 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:44:49,154 : INFO : EPOCH 3 - PROGRESS: at 31.85% examples, 484457 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:44:50,156 : INFO : EPOCH 3 - PROGRESS: at 42.45% examples, 484951 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:44:51,162 : INFO : EPOCH 3 - PROGRESS: at 53.31% examples, 487520 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:44:52,189 : INFO : EPOCH 3 - PROGRESS: at 63.98% examples, 483887 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:44:53,200 : INFO : EPOCH 3 - PROGRESS: at 69.48% examples, 449994 words/s, in_qsize 7, out_qsize 1\n",
      "2018-07-12 15:44:54,214 : INFO : EPOCH 3 - PROGRESS: at 78.59% examples, 444095 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:44:55,216 : INFO : EPOCH 3 - PROGRESS: at 88.55% examples, 445438 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:44:56,097 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-12 15:44:56,118 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-12 15:44:56,125 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-12 15:44:56,140 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-12 15:44:56,145 : INFO : EPOCH - 3 : training on 6250510 raw words (4557450 effective words) took 10.0s, 455302 effective words/s\n",
      "2018-07-12 15:44:57,179 : INFO : EPOCH 4 - PROGRESS: at 10.19% examples, 462966 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:44:58,180 : INFO : EPOCH 4 - PROGRESS: at 19.62% examples, 445928 words/s, in_qsize 8, out_qsize 1\n",
      "2018-07-12 15:44:59,184 : INFO : EPOCH 4 - PROGRESS: at 32.48% examples, 493827 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:45:00,196 : INFO : EPOCH 4 - PROGRESS: at 40.48% examples, 460691 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:45:01,212 : INFO : EPOCH 4 - PROGRESS: at 46.71% examples, 426015 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:45:02,228 : INFO : EPOCH 4 - PROGRESS: at 55.60% examples, 421762 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:45:03,235 : INFO : EPOCH 4 - PROGRESS: at 68.32% examples, 442569 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:45:04,244 : INFO : EPOCH 4 - PROGRESS: at 78.14% examples, 441492 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:45:05,276 : INFO : EPOCH 4 - PROGRESS: at 85.05% examples, 426719 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:45:06,277 : INFO : EPOCH 4 - PROGRESS: at 92.71% examples, 418481 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:45:07,104 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-12 15:45:07,109 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-12 15:45:07,120 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-12 15:45:07,134 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-12 15:45:07,136 : INFO : EPOCH - 4 : training on 6250510 raw words (4558059 effective words) took 11.0s, 415433 effective words/s\n",
      "2018-07-12 15:45:08,158 : INFO : EPOCH 5 - PROGRESS: at 9.28% examples, 427131 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:45:09,163 : INFO : EPOCH 5 - PROGRESS: at 18.29% examples, 417458 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:45:10,173 : INFO : EPOCH 5 - PROGRESS: at 27.69% examples, 421619 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:45:11,200 : INFO : EPOCH 5 - PROGRESS: at 38.14% examples, 431411 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:45:12,203 : INFO : EPOCH 5 - PROGRESS: at 44.97% examples, 409789 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:45:13,251 : INFO : EPOCH 5 - PROGRESS: at 53.45% examples, 402725 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:45:14,280 : INFO : EPOCH 5 - PROGRESS: at 61.38% examples, 394579 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:45:15,291 : INFO : EPOCH 5 - PROGRESS: at 70.91% examples, 398365 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:45:16,301 : INFO : EPOCH 5 - PROGRESS: at 83.61% examples, 417811 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:45:17,307 : INFO : EPOCH 5 - PROGRESS: at 94.12% examples, 422147 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:45:17,718 : INFO : worker thread finished; awaiting finish of 3 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-12 15:45:17,724 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-12 15:45:17,732 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-12 15:45:17,734 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-12 15:45:17,736 : INFO : EPOCH - 5 : training on 6250510 raw words (4558033 effective words) took 10.6s, 430475 effective words/s\n",
      "2018-07-12 15:45:18,750 : INFO : EPOCH 6 - PROGRESS: at 11.29% examples, 515455 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:45:19,753 : INFO : EPOCH 6 - PROGRESS: at 25.70% examples, 587948 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:45:20,756 : INFO : EPOCH 6 - PROGRESS: at 42.45% examples, 646963 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:45:21,757 : INFO : EPOCH 6 - PROGRESS: at 56.58% examples, 647680 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:45:22,767 : INFO : EPOCH 6 - PROGRESS: at 71.23% examples, 648278 words/s, in_qsize 8, out_qsize 1\n",
      "2018-07-12 15:45:23,769 : INFO : EPOCH 6 - PROGRESS: at 85.53% examples, 648555 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:45:24,618 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-12 15:45:24,624 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-12 15:45:24,631 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-12 15:45:24,634 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-12 15:45:24,636 : INFO : EPOCH - 6 : training on 6250510 raw words (4558415 effective words) took 6.9s, 661046 effective words/s\n",
      "2018-07-12 15:45:25,648 : INFO : EPOCH 7 - PROGRESS: at 16.30% examples, 744066 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:45:26,650 : INFO : EPOCH 7 - PROGRESS: at 33.31% examples, 760527 words/s, in_qsize 7, out_qsize 1\n",
      "2018-07-12 15:45:27,654 : INFO : EPOCH 7 - PROGRESS: at 49.37% examples, 754524 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:45:28,666 : INFO : EPOCH 7 - PROGRESS: at 64.97% examples, 738873 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:45:29,673 : INFO : EPOCH 7 - PROGRESS: at 82.34% examples, 748981 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:45:30,660 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-12 15:45:30,670 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-12 15:45:30,672 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-12 15:45:30,676 : INFO : EPOCH 7 - PROGRESS: at 100.00% examples, 755567 words/s, in_qsize 0, out_qsize 1\n",
      "2018-07-12 15:45:30,678 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-12 15:45:30,680 : INFO : EPOCH - 7 : training on 6250510 raw words (4560492 effective words) took 6.0s, 755103 effective words/s\n",
      "2018-07-12 15:45:31,703 : INFO : EPOCH 8 - PROGRESS: at 16.89% examples, 776600 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:45:32,711 : INFO : EPOCH 8 - PROGRESS: at 33.46% examples, 763895 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:45:33,714 : INFO : EPOCH 8 - PROGRESS: at 51.12% examples, 780575 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:45:34,731 : INFO : EPOCH 8 - PROGRESS: at 69.64% examples, 791446 words/s, in_qsize 7, out_qsize 2\n",
      "2018-07-12 15:45:35,731 : INFO : EPOCH 8 - PROGRESS: at 88.55% examples, 804584 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:45:36,276 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-12 15:45:36,291 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-12 15:45:36,293 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-12 15:45:36,296 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-12 15:45:36,298 : INFO : EPOCH - 8 : training on 6250510 raw words (4558512 effective words) took 5.6s, 814672 effective words/s\n",
      "2018-07-12 15:45:37,309 : INFO : EPOCH 9 - PROGRESS: at 17.22% examples, 789678 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:45:38,311 : INFO : EPOCH 9 - PROGRESS: at 34.23% examples, 783513 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:45:39,314 : INFO : EPOCH 9 - PROGRESS: at 51.86% examples, 793227 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:45:40,321 : INFO : EPOCH 9 - PROGRESS: at 69.91% examples, 798264 words/s, in_qsize 7, out_qsize 1\n",
      "2018-07-12 15:45:41,328 : INFO : EPOCH 9 - PROGRESS: at 88.55% examples, 805695 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:45:41,899 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-12 15:45:41,903 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-12 15:45:41,910 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-12 15:45:41,913 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-12 15:45:41,915 : INFO : EPOCH - 9 : training on 6250510 raw words (4559243 effective words) took 5.6s, 812886 effective words/s\n",
      "2018-07-12 15:45:42,934 : INFO : EPOCH 10 - PROGRESS: at 17.52% examples, 799549 words/s, in_qsize 8, out_qsize 1\n",
      "2018-07-12 15:45:43,937 : INFO : EPOCH 10 - PROGRESS: at 35.03% examples, 796034 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:45:44,943 : INFO : EPOCH 10 - PROGRESS: at 52.33% examples, 796604 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:45:45,945 : INFO : EPOCH 10 - PROGRESS: at 70.09% examples, 797278 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-12 15:45:46,958 : INFO : EPOCH 10 - PROGRESS: at 88.40% examples, 801958 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:45:47,583 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-12 15:45:47,599 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-12 15:45:47,606 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-12 15:45:47,610 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-12 15:45:47,613 : INFO : EPOCH - 10 : training on 6250510 raw words (4559890 effective words) took 5.7s, 800969 effective words/s\n",
      "2018-07-12 15:45:47,615 : INFO : training on a 62505100 raw words (45587276 effective words) took 75.9s, 600639 effective words/s\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "result = build_dict(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents_pos = result['tok_sents_pos']\n",
    "model = result['model']\n",
    "train_x_pos = sentence2idx(sents_pos, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-12 15:45:50,905 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('decent', 0.7668520212173462),\n",
       " ('great', 0.715586245059967),\n",
       " ('bad', 0.6942986845970154),\n",
       " ('cool', 0.6417878866195679),\n",
       " ('nice', 0.6319639086723328),\n",
       " ('fine', 0.6245481967926025),\n",
       " ('solid', 0.59840989112854),\n",
       " ('lousy', 0.5939891338348389),\n",
       " ('ok', 0.5814189910888672),\n",
       " ('terrific', 0.5808237195014954)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  792,    16,    29,     4,     0,   118,  1755,  7082,    10,\n",
      "          19,   985,     5,    28, 19990,     5,     7,    12,  2375,\n",
      "        1807,   128,  2235,     5,     3,  6995,   299,     1,  2582,\n",
      "        2315,     0,    19,    36,   483,  5888,    12,  3361,     2,\n",
      "          39,    12,     3,  1006,   175,    21,    50,   782,     1]), array([   78,     1,     1, 51922,     9,     0,   197,   624,   131,\n",
      "           8,  3335,     2,     9,   250,     0,    17,   179,   710,\n",
      "           5,   107,     1,   190,    32, 12474,     5,    99, 10177,\n",
      "        1477,     2,  3363,     1,  1644,    71,   147,   103,   624,\n",
      "         168,  4411,   204,   102,    32,  1751,     0,    90,    29,\n",
      "           9,   375,  2381, 25259,   151,    63,   536,   305,     6,\n",
      "        2193,     4,  9988,     0,   861,   137,     8,  6084,   397,\n",
      "          55,    32,   955,   155,    29,     4,   132, 18943, 12641,\n",
      "           4,     3,    17, 10177,  5479,    61,    23,    28,  1146,\n",
      "           3,    86,    17,     8,   189,     7,    61,    23,    28,\n",
      "          56,  1146,    34,  8111,     4,     3,    86,    17,    15,\n",
      "           7,   445,    59,     5,    51,    75,    11,     1,    14,\n",
      "           9,  3317,     5,   850,   150,     7,     9,  1902,    11,\n",
      "          57,   500,   624,     5,  7412,     2,     9,  3188, 29750,\n",
      "          11,     0,   115,   240,    36,     0,  1735,    55,  9301,\n",
      "         144,     0,   251,     2,   498,    71,     1,    72,    32,\n",
      "          65,   104,    11,  5770,     0,    81,    35,    66, 16243,\n",
      "           8,     0,    84,   103,    59,    14,   150,    32,    70,\n",
      "         177,    36,     0,  1314,  1206,    55,    99,     0,    17,\n",
      "          16,     0,  2530,   169, 12510,   101,   125,  1210,     9,\n",
      "          55,   113,  1419,  2638,    10,    17,     9,   375,    39,\n",
      "          24,  9492,   737,     5,   163,   182,    11,  3548,    78,\n",
      "           1])]\n"
     ]
    }
   ],
   "source": [
    "sents_neg = result['tok_sents_neg']\n",
    "train_x_neg = sentence2idx(sents_neg, model)\n",
    "print(train_x_neg[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = train_x_pos + train_x_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = [1] * len(train_x_pos) + [0] * len(train_x_neg)\n",
    "# print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_path = '../data/aclImdb/test'\n",
    "test_sents_pos = read_dataset(test_path+'/pos/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok_test_sents_pos = [nltk.word_tokenize(sent.decode(\"utf8\").encode('ascii', 'ignore')) for sent in test_sents_pos]\n",
    "test_x_pos = sentence2idx(tok_test_sents_pos, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sents_neg = read_dataset(test_path+'/neg/')\n",
    "tok_test_sents_neg = [nltk.word_tokenize(sent.decode(\"utf8\").encode('ascii', 'ignore')) for sent in test_sents_neg]\n",
    "test_x_neg = sentence2idx(tok_test_sents_neg, model)\n",
    "test_x = test_x_pos + test_x_neg\n",
    "test_y = [1] * len(test_x_pos) + [0] * len(test_x_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save converted sentence indices to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('../data/gensim/gensim_imdb.pkl', 'wb')\n",
    "pkl.dump((train_x, train_y), f, -1)\n",
    "pkl.dump((test_x, test_y), f, -1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('../data/gensim/gensim_imdb.pkl', 'rb')\n",
    "train_set = pkl.load(f)\n",
    "test_set = pkl.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save word embeddings to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Wemb = create_Wemb(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('../data/gensim/gensim_imdb_Wemb.pkl', 'wb')\n",
    "pkl.dump(Wemb, f, -1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('../data/gensim/gensim_imdb_Wemb.pkl', 'rb')\n",
    "Wemb_file = pkl.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.14067876e-01 -1.49158609e+00  9.64979351e-01 ... -9.61133957e-01\n",
      "   1.75469482e+00 -1.77968249e-01]\n",
      " [-6.63630664e-01 -2.52432644e-01  8.86730194e-01 ... -1.50446981e-01\n",
      "  -2.50875622e-01 -4.96376812e-01]\n",
      " [-6.16630793e-01 -2.99852312e-01  1.09431922e+00 ... -1.22701669e+00\n",
      "  -1.18284440e+00 -1.39343485e-01]\n",
      " ...\n",
      " [ 2.42219102e-02  3.26500684e-02 -3.38389613e-02 ... -1.85415484e-02\n",
      "  -3.49518023e-02  1.92117393e-02]\n",
      " [-3.34197544e-02 -8.84763058e-03 -5.36686322e-03 ...  1.86094474e-02\n",
      "   2.37452965e-02 -1.52655382e-04]\n",
      " [ 4.41411696e-02  8.57123360e-02 -3.84849380e-03 ... -2.65482161e-02\n",
      "  -2.57179160e-02  3.27176750e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(Wemb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.14067876e-01 -1.49158609e+00  9.64979351e-01 ... -9.61133957e-01\n",
      "   1.75469482e+00 -1.77968249e-01]\n",
      " [-6.63630664e-01 -2.52432644e-01  8.86730194e-01 ... -1.50446981e-01\n",
      "  -2.50875622e-01 -4.96376812e-01]\n",
      " [-6.16630793e-01 -2.99852312e-01  1.09431922e+00 ... -1.22701669e+00\n",
      "  -1.18284440e+00 -1.39343485e-01]\n",
      " ...\n",
      " [ 2.42219102e-02  3.26500684e-02 -3.38389613e-02 ... -1.85415484e-02\n",
      "  -3.49518023e-02  1.92117393e-02]\n",
      " [-3.34197544e-02 -8.84763058e-03 -5.36686322e-03 ...  1.86094474e-02\n",
      "   2.37452965e-02 -1.52655382e-04]\n",
      " [ 4.41411696e-02  8.57123360e-02 -3.84849380e-03 ... -2.65482161e-02\n",
      "  -2.57179160e-02  3.27176750e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(Wemb_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include all above into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_gensim_w2vec(path):\n",
    "    result = build_dict(path+'train')\n",
    "    model = result['model']\n",
    "\n",
    "    sents_pos = result['tok_sents_pos']\n",
    "    train_x_pos = sentence2idx(sents_pos, model)\n",
    "\n",
    "    sents_neg = result['tok_sents_neg']\n",
    "    train_x_neg = sentence2idx(sents_neg, model)\n",
    "    train_x = train_x_pos + train_x_neg\n",
    "    train_y = [1] * len(train_x_pos) + [0] * len(train_x_neg)\n",
    "\n",
    "    test_sents_pos = read_dataset(path+'test/pos/')\n",
    "    tok_test_sents_pos = [nltk.word_tokenize(sent.decode(\"utf8\").encode('ascii', 'ignore')) for sent in test_sents_pos]\n",
    "    test_x_pos = sentence2idx(tok_test_sents_pos, model)\n",
    "\n",
    "    test_sents_neg = read_dataset(path+'test/neg/')\n",
    "    tok_test_sents_neg = [nltk.word_tokenize(sent.decode(\"utf8\").encode('ascii', 'ignore')) for sent in test_sents_neg]\n",
    "    test_x_neg = sentence2idx(tok_test_sents_neg, model)\n",
    "    test_x = test_x_pos + test_x_neg\n",
    "    test_y = [1] * len(test_x_pos) + [0] * len(test_x_neg)\n",
    "\n",
    "    f = open('../data/gensim/gensim_imdb.pkl', 'wb')\n",
    "    pkl.dump((train_x, train_y), f, -1)\n",
    "    pkl.dump((test_x, test_y), f, -1)\n",
    "    f.close()\n",
    "\n",
    "    Wemb = create_Wemb(model)\n",
    "    f = open('../data/gensim/gensim_imdb_Wemb.pkl', 'wb')\n",
    "    pkl.dump(Wemb, f, -1)\n",
    "    f.close()\n",
    "\n",
    "    model.save('../data/gensim/imdb_gensim_w2vmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-12 15:48:48,998 : INFO : collecting all words and their counts\n",
      "2018-07-12 15:48:48,999 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-07-12 15:48:50,007 : INFO : PROGRESS: at sentence #10000, processed 2518094 words, keeping 62658 word types\n",
      "2018-07-12 15:48:51,060 : INFO : PROGRESS: at sentence #20000, processed 5010152 words, keeping 89834 word types\n",
      "2018-07-12 15:48:51,565 : INFO : collected 100352 word types from a corpus of 6250510 raw words and 25000 sentences\n",
      "2018-07-12 15:48:51,567 : INFO : Loading a fresh vocabulary\n",
      "2018-07-12 15:48:52,950 : INFO : effective_min_count=1 retains 100352 unique words (100% of original 100352, drops 0)\n",
      "2018-07-12 15:48:52,951 : INFO : effective_min_count=1 leaves 6250510 word corpus (100% of original 6250510, drops 0)\n",
      "2018-07-12 15:48:53,391 : INFO : deleting the raw counts dictionary of 100352 items\n",
      "2018-07-12 15:48:53,402 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2018-07-12 15:48:53,405 : INFO : downsampling leaves estimated 4558796 word corpus (72.9% of prior 6250510)\n",
      "2018-07-12 15:48:54,017 : INFO : estimated required memory for 100352 words and 128 dimensions: 152936448 bytes\n",
      "2018-07-12 15:48:54,020 : INFO : resetting layer weights\n",
      "2018-07-12 15:48:55,667 : INFO : training model with 4 workers on 100352 vocabulary and 128 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-07-12 15:48:56,683 : INFO : EPOCH 1 - PROGRESS: at 11.15% examples, 512725 words/s, in_qsize 6, out_qsize 2\n",
      "2018-07-12 15:48:57,689 : INFO : EPOCH 1 - PROGRESS: at 24.70% examples, 564307 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:48:58,692 : INFO : EPOCH 1 - PROGRESS: at 40.48% examples, 616831 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:48:59,700 : INFO : EPOCH 1 - PROGRESS: at 57.65% examples, 659432 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:49:00,709 : INFO : EPOCH 1 - PROGRESS: at 76.12% examples, 691263 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:49:01,728 : INFO : EPOCH 1 - PROGRESS: at 91.62% examples, 691728 words/s, in_qsize 6, out_qsize 3\n",
      "2018-07-12 15:49:02,130 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-12 15:49:02,140 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-12 15:49:02,142 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-12 15:49:02,149 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-12 15:49:02,151 : INFO : EPOCH - 1 : training on 6250510 raw words (4559311 effective words) took 6.5s, 704684 effective words/s\n",
      "2018-07-12 15:49:03,163 : INFO : EPOCH 2 - PROGRESS: at 13.83% examples, 631179 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:49:04,166 : INFO : EPOCH 2 - PROGRESS: at 30.48% examples, 699825 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:49:05,171 : INFO : EPOCH 2 - PROGRESS: at 47.16% examples, 721070 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:49:06,185 : INFO : EPOCH 2 - PROGRESS: at 64.97% examples, 738287 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:49:07,191 : INFO : EPOCH 2 - PROGRESS: at 82.64% examples, 752167 words/s, in_qsize 8, out_qsize 2\n",
      "2018-07-12 15:49:08,135 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-12 15:49:08,142 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-12 15:49:08,144 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-12 15:49:08,146 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-12 15:49:08,147 : INFO : EPOCH - 2 : training on 6250510 raw words (4557413 effective words) took 6.0s, 761061 effective words/s\n",
      "2018-07-12 15:49:09,160 : INFO : EPOCH 3 - PROGRESS: at 16.46% examples, 754270 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:49:10,160 : INFO : EPOCH 3 - PROGRESS: at 31.04% examples, 712451 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:49:11,178 : INFO : EPOCH 3 - PROGRESS: at 43.01% examples, 655136 words/s, in_qsize 8, out_qsize 1\n",
      "2018-07-12 15:49:12,188 : INFO : EPOCH 3 - PROGRESS: at 55.95% examples, 638026 words/s, in_qsize 8, out_qsize 2\n",
      "2018-07-12 15:49:13,191 : INFO : EPOCH 3 - PROGRESS: at 73.20% examples, 664376 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:49:14,199 : INFO : EPOCH 3 - PROGRESS: at 91.27% examples, 690379 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:49:14,622 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-12 15:49:14,634 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-12 15:49:14,635 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-12 15:49:14,637 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-12 15:49:14,639 : INFO : EPOCH - 3 : training on 6250510 raw words (4558564 effective words) took 6.5s, 703247 effective words/s\n",
      "2018-07-12 15:49:15,649 : INFO : EPOCH 4 - PROGRESS: at 16.30% examples, 744998 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:49:16,659 : INFO : EPOCH 4 - PROGRESS: at 30.81% examples, 707016 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:49:17,663 : INFO : EPOCH 4 - PROGRESS: at 48.06% examples, 731379 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:49:18,675 : INFO : EPOCH 4 - PROGRESS: at 65.99% examples, 749900 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:49:19,693 : INFO : EPOCH 4 - PROGRESS: at 83.73% examples, 758857 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:49:20,656 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-12 15:49:20,664 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-12 15:49:20,667 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-12 15:49:20,673 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-12 15:49:20,675 : INFO : EPOCH - 4 : training on 6250510 raw words (4558706 effective words) took 6.0s, 755838 effective words/s\n",
      "2018-07-12 15:49:21,682 : INFO : EPOCH 5 - PROGRESS: at 16.75% examples, 767886 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:49:22,711 : INFO : EPOCH 5 - PROGRESS: at 28.85% examples, 656284 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:49:23,712 : INFO : EPOCH 5 - PROGRESS: at 42.09% examples, 638126 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:49:24,713 : INFO : EPOCH 5 - PROGRESS: at 52.99% examples, 603985 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:49:25,716 : INFO : EPOCH 5 - PROGRESS: at 67.67% examples, 615571 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:49:26,734 : INFO : EPOCH 5 - PROGRESS: at 78.59% examples, 593921 words/s, in_qsize 7, out_qsize 1\n",
      "2018-07-12 15:49:27,737 : INFO : EPOCH 5 - PROGRESS: at 92.24% examples, 596606 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:49:28,108 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-12 15:49:28,114 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-12 15:49:28,123 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-12 15:49:28,127 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-12 15:49:28,129 : INFO : EPOCH - 5 : training on 6250510 raw words (4558297 effective words) took 7.4s, 611855 effective words/s\n",
      "2018-07-12 15:49:29,147 : INFO : EPOCH 6 - PROGRESS: at 16.89% examples, 766369 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:49:30,155 : INFO : EPOCH 6 - PROGRESS: at 34.06% examples, 773164 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:49:31,157 : INFO : EPOCH 6 - PROGRESS: at 45.45% examples, 692402 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:49:32,185 : INFO : EPOCH 6 - PROGRESS: at 59.55% examples, 675301 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:49:33,197 : INFO : EPOCH 6 - PROGRESS: at 73.67% examples, 664922 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:49:34,211 : INFO : EPOCH 6 - PROGRESS: at 86.15% examples, 647882 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:49:35,214 : INFO : EPOCH 6 - PROGRESS: at 98.28% examples, 633079 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:49:35,322 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-12 15:49:35,332 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-12 15:49:35,337 : INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-12 15:49:35,356 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-12 15:49:35,357 : INFO : EPOCH - 6 : training on 6250510 raw words (4558231 effective words) took 7.2s, 630941 effective words/s\n",
      "2018-07-12 15:49:36,370 : INFO : EPOCH 7 - PROGRESS: at 9.88% examples, 455273 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:49:37,380 : INFO : EPOCH 7 - PROGRESS: at 23.77% examples, 541906 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:49:38,394 : INFO : EPOCH 7 - PROGRESS: at 37.69% examples, 570826 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:49:39,396 : INFO : EPOCH 7 - PROGRESS: at 53.77% examples, 613900 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:49:40,400 : INFO : EPOCH 7 - PROGRESS: at 71.56% examples, 650627 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:49:41,414 : INFO : EPOCH 7 - PROGRESS: at 89.01% examples, 673942 words/s, in_qsize 7, out_qsize 1\n",
      "2018-07-12 15:49:41,980 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-12 15:49:41,992 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-12 15:49:42,002 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-12 15:49:42,014 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-12 15:49:42,016 : INFO : EPOCH - 7 : training on 6250510 raw words (4558884 effective words) took 6.6s, 685705 effective words/s\n",
      "2018-07-12 15:49:43,029 : INFO : EPOCH 8 - PROGRESS: at 15.58% examples, 708564 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:49:44,037 : INFO : EPOCH 8 - PROGRESS: at 32.64% examples, 743521 words/s, in_qsize 7, out_qsize 1\n",
      "2018-07-12 15:49:45,043 : INFO : EPOCH 8 - PROGRESS: at 50.16% examples, 764180 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:49:46,045 : INFO : EPOCH 8 - PROGRESS: at 64.94% examples, 739060 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:49:47,046 : INFO : EPOCH 8 - PROGRESS: at 80.80% examples, 735562 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:49:48,056 : INFO : EPOCH 8 - PROGRESS: at 98.45% examples, 744151 words/s, in_qsize 7, out_qsize 1\n",
      "2018-07-12 15:49:48,135 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-12 15:49:48,138 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-12 15:49:48,156 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-12 15:49:48,159 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-12 15:49:48,163 : INFO : EPOCH - 8 : training on 6250510 raw words (4558435 effective words) took 6.1s, 742322 effective words/s\n",
      "2018-07-12 15:49:49,186 : INFO : EPOCH 9 - PROGRESS: at 15.86% examples, 718556 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:49:50,195 : INFO : EPOCH 9 - PROGRESS: at 28.41% examples, 649523 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:49:51,197 : INFO : EPOCH 9 - PROGRESS: at 41.50% examples, 630720 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:49:52,208 : INFO : EPOCH 9 - PROGRESS: at 53.60% examples, 610987 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:49:53,231 : INFO : EPOCH 9 - PROGRESS: at 68.32% examples, 618766 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:49:54,234 : INFO : EPOCH 9 - PROGRESS: at 84.22% examples, 635772 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:49:55,132 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-12 15:49:55,144 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-12 15:49:55,148 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-12 15:49:55,155 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-12 15:49:55,158 : INFO : EPOCH - 9 : training on 6250510 raw words (4558388 effective words) took 7.0s, 652671 effective words/s\n",
      "2018-07-12 15:49:56,191 : INFO : EPOCH 10 - PROGRESS: at 13.83% examples, 632085 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:49:57,202 : INFO : EPOCH 10 - PROGRESS: at 29.52% examples, 677236 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:49:58,207 : INFO : EPOCH 10 - PROGRESS: at 45.63% examples, 696080 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:49:59,213 : INFO : EPOCH 10 - PROGRESS: at 62.52% examples, 711999 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-12 15:50:00,224 : INFO : EPOCH 10 - PROGRESS: at 79.21% examples, 718306 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-12 15:50:01,233 : INFO : EPOCH 10 - PROGRESS: at 95.64% examples, 722120 words/s, in_qsize 8, out_qsize 1\n",
      "2018-07-12 15:50:01,436 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-12 15:50:01,441 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-12 15:50:01,447 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-12 15:50:01,449 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-12 15:50:01,451 : INFO : EPOCH - 10 : training on 6250510 raw words (4558884 effective words) took 6.3s, 727867 effective words/s\n",
      "2018-07-12 15:50:01,454 : INFO : training on a 62505100 raw words (45585113 effective words) took 65.8s, 692945 effective words/s\n",
      "2018-07-12 15:51:37,712 : INFO : saving Word2Vec object under ../data/gensim/imdb_gensim_w2vmodel, separately None\n",
      "2018-07-12 15:51:37,715 : INFO : storing np array 'vectors' to ../data/gensim/imdb_gensim_w2vmodel.wv.vectors.npy\n",
      "2018-07-12 15:51:37,990 : INFO : not storing attribute vectors_norm\n",
      "2018-07-12 15:51:37,992 : INFO : storing np array 'syn1neg' to ../data/gensim/imdb_gensim_w2vmodel.trainables.syn1neg.npy\n",
      "2018-07-12 15:51:38,334 : INFO : not storing attribute cum_table\n",
      "2018-07-12 15:51:41,532 : INFO : saved ../data/gensim/imdb_gensim_w2vmodel\n"
     ]
    }
   ],
   "source": [
    "path = '../data/aclImdb/'\n",
    "train_gensim_w2vec(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if embeddings from gensim model equal to the word embeddings stored in the file to make sure the correctness of storing word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_idxwemb_wordemb(sentence_idx):\n",
    "    model_path = '../data/gensim/imdb_gensim_w2vmodel'\n",
    "    model = gensim.models.Word2Vec.load(model_path)\n",
    "    sentence_words = []\n",
    "    for idx, x in enumerate(sentence_idx):\n",
    "        sentence_words.append(model.wv[model.wv.index2word[x]])\n",
    "\n",
    "    f = open('../data/gensim/gensim_imdb_Wemb.pkl', 'rb')\n",
    "    Wemb = pkl.load(f)\n",
    "    f.close()\n",
    "    emb_x = Wemb[sentence_idx]\n",
    "\n",
    "    return (numpy.matrix(sentence_words)==numpy.matrix(emb_x)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-12 15:51:42,157 : INFO : loading Word2Vec object from ../data/gensim/imdb_gensim_w2vmodel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  792    16    29     4     0   118  1755  7082    10    19   985     5\n",
      "    28 19990     5     7    12  2375  1807   128  2235     5     3  6995\n",
      "   299     1  2582  2315     0    19    36   483  5888    12  3361     2\n",
      "    39    12     3  1006   175    21    50   782     1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-12 15:51:42,839 : INFO : loading vocabulary recursively from ../data/gensim/imdb_gensim_w2vmodel.vocabulary.* with mmap=None\n",
      "2018-07-12 15:51:42,841 : INFO : loading wv recursively from ../data/gensim/imdb_gensim_w2vmodel.wv.* with mmap=None\n",
      "2018-07-12 15:51:42,843 : INFO : loading vectors from ../data/gensim/imdb_gensim_w2vmodel.wv.vectors.npy with mmap=None\n",
      "2018-07-12 15:51:42,917 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-07-12 15:51:42,922 : INFO : loading trainables recursively from ../data/gensim/imdb_gensim_w2vmodel.trainables.* with mmap=None\n",
      "2018-07-12 15:51:42,925 : INFO : loading syn1neg from ../data/gensim/imdb_gensim_w2vmodel.trainables.syn1neg.npy with mmap=None\n",
      "2018-07-12 15:51:42,986 : INFO : setting ignored attribute cum_table to None\n",
      "2018-07-12 15:51:42,991 : INFO : loaded ../data/gensim/imdb_gensim_w2vmodel\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_x_neg[0])\n",
    "compare_idxwemb_wordemb(train_x_neg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7",
   "language": "python",
   "name": "python27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
