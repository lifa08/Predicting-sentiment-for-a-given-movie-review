{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proprocess raw texts to digital indices and train them to word embeddings via gensim's word2vec\n",
    "\n",
    "**Libraries used:**\n",
    "1. nltk: used to tokenize text\n",
    "\n",
    "2. gensim: use its models.word2vec to produce word vectors.\n",
    "\n",
    "**Note: This is summerized in a more concise python file Wemb_gensim.py **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "import nltk\n",
    "import numpy\n",
    "import pickle as pkl\n",
    "import sys\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Strip punctuation/symbols from words\n",
    "def normalize_text(text):\n",
    "    norm_text = text.lower()\n",
    "    \n",
    "    # Replace breaks with spaces\n",
    "    norm_text = norm_text.replace('<br />', ' ')\n",
    "\n",
    "    # Pad punctuation with spaces on both sides\n",
    "    for char in [':', '\"', ',', '(', ')', '!', '?', ';', '*']:\n",
    "        norm_text = norm_text.replace(char, ' ')\n",
    "\n",
    "    norm_text = norm_text.replace('.', ' ' + '.')\n",
    "    return norm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_dataset(path):\n",
    "    if sys.version > '3':\n",
    "        control_chars = [chr(0x85)] # UTF-8\n",
    "    else:\n",
    "        control_chars = [unichr(0x85)] # UTF-16 which is often used in other language codes (e.g. Chinese)\n",
    "\n",
    "    dataset = []\n",
    "    currdir = os.getcwd()\n",
    "    os.chdir(path)\n",
    "    #i = 0\n",
    "    for ff in glob.glob(\"*.txt\"):\n",
    "        #i += 1\n",
    "        with open(ff, \"r\") as f:\n",
    "            line_txt = f.readline().strip()\n",
    "            line_norm = normalize_text(line_txt)\n",
    "            dataset.append(line_norm)\n",
    "        #if(i==100):\n",
    "            #break\n",
    "    os.chdir(currdir)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dict(path, Wemb_size=128, iter=10):\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "\n",
    "    sentences_pos = read_dataset(path+'/pos/')\n",
    "    sentences_neg = read_dataset(path+'/neg/')\n",
    "\n",
    "    sentences_train = sentences_pos + sentences_neg\n",
    "    tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences_train]\n",
    "\n",
    "    model = gensim.models.Word2Vec(tokenized_sentences, min_count=1,\n",
    "                                   size=Wemb_size, window=5,\n",
    "                                   workers=cores, iter=iter)\n",
    "\n",
    "    tok_sents_pos = tokenized_sentences[:len(sentences_pos)]\n",
    "    tok_sents_neg = tokenized_sentences[len(sentences_pos):]\n",
    "\n",
    "    return {'model': model, 'tok_sents_pos': tok_sents_pos, 'tok_sents_neg': tok_sents_neg}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert words in text sentences to their corresponding indices in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence2idx(tokenized_sentences, model):\n",
    "    idx = []\n",
    "    for tok_sen in tokenized_sentences:\n",
    "        idx_sent = numpy.zeros(len(tok_sen), dtype=numpy.int)\n",
    "        for i, word in enumerate(tok_sen):\n",
    "            if word in model.wv.vocab:\n",
    "                idx_sent[i] = model.wv.vocab[word].index\n",
    "            else:\n",
    "                idx_sent[i] = model.wv.vocab['.'].index\n",
    "        idx.append(idx_sent)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert words in the dictionary into embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, there is no need to create word embedding specifically, because it is included in the trained gensim model and can be acessed by `model.wv.syn0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_Wemb(model, Wemb_size=128):\n",
    "    Wemb = numpy.zeros((len(model.wv.vocab), Wemb_size))\n",
    "    for i in range(len(model.wv.vocab)):\n",
    "        embedding_vector = model.wv[model.wv.index2word[i]]\n",
    "        if embedding_vector is not None:\n",
    "            Wemb[i] = embedding_vector\n",
    "    return Wemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '../data/Method3/aclImdb/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-10 09:40:49,192 : INFO : collecting all words and their counts\n",
      "2018-07-10 09:40:49,193 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-07-10 09:40:49,716 : INFO : PROGRESS: at sentence #10000, processed 2518483 words, keeping 63047 word types\n",
      "2018-07-10 09:40:50,297 : INFO : PROGRESS: at sentence #20000, processed 5010758 words, keeping 90480 word types\n",
      "2018-07-10 09:40:50,599 : INFO : collected 101119 word types from a corpus of 6251170 raw words and 25000 sentences\n",
      "2018-07-10 09:40:50,601 : INFO : Loading a fresh vocabulary\n",
      "2018-07-10 09:40:51,110 : INFO : min_count=1 retains 101119 unique words (100% of original 101119, drops 0)\n",
      "2018-07-10 09:40:51,112 : INFO : min_count=1 leaves 6251170 word corpus (100% of original 6251170, drops 0)\n",
      "2018-07-10 09:40:51,438 : INFO : deleting the raw counts dictionary of 101119 items\n",
      "2018-07-10 09:40:51,443 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2018-07-10 09:40:51,444 : INFO : downsampling leaves estimated 4559750 word corpus (72.9% of prior 6251170)\n",
      "2018-07-10 09:40:51,447 : INFO : estimated required memory for 101119 words and 128 dimensions: 154105356 bytes\n",
      "2018-07-10 09:40:51,800 : INFO : resetting layer weights\n",
      "2018-07-10 09:40:53,140 : INFO : training model with 4 workers on 101119 vocabulary and 128 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-07-10 09:40:54,147 : INFO : PROGRESS: at 1.54% examples, 702277 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:40:55,148 : INFO : PROGRESS: at 3.26% examples, 747209 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:40:56,161 : INFO : PROGRESS: at 5.05% examples, 769547 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:40:57,166 : INFO : PROGRESS: at 6.79% examples, 772770 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:40:58,174 : INFO : PROGRESS: at 8.54% examples, 775878 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:40:59,181 : INFO : PROGRESS: at 10.34% examples, 780577 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:00,193 : INFO : PROGRESS: at 12.11% examples, 784034 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:01,198 : INFO : PROGRESS: at 13.47% examples, 763704 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:02,208 : INFO : PROGRESS: at 14.43% examples, 727703 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-10 09:41:03,211 : INFO : PROGRESS: at 15.20% examples, 690763 words/s, in_qsize 7, out_qsize 1\n",
      "2018-07-10 09:41:04,213 : INFO : PROGRESS: at 16.53% examples, 681857 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:05,216 : INFO : PROGRESS: at 18.25% examples, 690497 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:06,224 : INFO : PROGRESS: at 20.01% examples, 697461 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:07,228 : INFO : PROGRESS: at 21.71% examples, 703456 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:08,240 : INFO : PROGRESS: at 23.46% examples, 709092 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-10 09:41:09,252 : INFO : PROGRESS: at 25.17% examples, 713899 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:10,256 : INFO : PROGRESS: at 26.86% examples, 716761 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:11,263 : INFO : PROGRESS: at 28.65% examples, 721546 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:12,270 : INFO : PROGRESS: at 30.46% examples, 726245 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:13,271 : INFO : PROGRESS: at 32.16% examples, 728806 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:14,284 : INFO : PROGRESS: at 33.92% examples, 732232 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-10 09:41:15,352 : INFO : PROGRESS: at 35.09% examples, 721224 words/s, in_qsize 6, out_qsize 2\n",
      "2018-07-10 09:41:16,352 : INFO : PROGRESS: at 36.62% examples, 720338 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:17,364 : INFO : PROGRESS: at 38.05% examples, 716869 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-10 09:41:18,365 : INFO : PROGRESS: at 39.83% examples, 720224 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:19,368 : INFO : PROGRESS: at 41.44% examples, 720720 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:20,373 : INFO : PROGRESS: at 43.17% examples, 723351 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:21,376 : INFO : PROGRESS: at 44.62% examples, 721526 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:22,436 : INFO : PROGRESS: at 45.44% examples, 708115 words/s, in_qsize 7, out_qsize 1\n",
      "2018-07-10 09:41:23,448 : INFO : PROGRESS: at 47.17% examples, 710200 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:24,460 : INFO : PROGRESS: at 48.42% examples, 705548 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-10 09:41:25,490 : INFO : PROGRESS: at 49.02% examples, 691269 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:26,516 : INFO : PROGRESS: at 50.28% examples, 686952 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-10 09:41:27,549 : INFO : PROGRESS: at 51.26% examples, 679628 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:28,550 : INFO : PROGRESS: at 52.09% examples, 671098 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:29,555 : INFO : PROGRESS: at 53.80% examples, 674051 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:30,559 : INFO : PROGRESS: at 55.45% examples, 676458 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-10 09:41:31,577 : INFO : PROGRESS: at 57.21% examples, 679026 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:32,587 : INFO : PROGRESS: at 58.95% examples, 681793 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:33,613 : INFO : PROGRESS: at 60.20% examples, 678301 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:34,620 : INFO : PROGRESS: at 61.79% examples, 679561 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:35,654 : INFO : PROGRESS: at 63.26% examples, 678904 words/s, in_qsize 5, out_qsize 2\n",
      "2018-07-10 09:41:36,659 : INFO : PROGRESS: at 64.03% examples, 671315 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:37,660 : INFO : PROGRESS: at 65.58% examples, 672294 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:38,662 : INFO : PROGRESS: at 67.27% examples, 674158 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:39,670 : INFO : PROGRESS: at 68.98% examples, 676306 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-10 09:41:40,674 : INFO : PROGRESS: at 70.67% examples, 678119 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:41,677 : INFO : PROGRESS: at 72.44% examples, 680766 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:42,682 : INFO : PROGRESS: at 74.13% examples, 682752 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:43,688 : INFO : PROGRESS: at 75.83% examples, 684566 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:44,696 : INFO : PROGRESS: at 77.04% examples, 681734 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-10 09:41:45,698 : INFO : PROGRESS: at 77.70% examples, 674347 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:46,707 : INFO : PROGRESS: at 79.13% examples, 673804 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:47,712 : INFO : PROGRESS: at 80.79% examples, 675261 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:48,719 : INFO : PROGRESS: at 82.46% examples, 676685 words/s, in_qsize 7, out_qsize 1\n",
      "2018-07-10 09:41:49,731 : INFO : PROGRESS: at 84.08% examples, 677763 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:50,733 : INFO : PROGRESS: at 85.81% examples, 679863 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:51,744 : INFO : PROGRESS: at 87.54% examples, 681362 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:52,789 : INFO : PROGRESS: at 88.88% examples, 679710 words/s, in_qsize 8, out_qsize 1\n",
      "2018-07-10 09:41:53,794 : INFO : PROGRESS: at 89.94% examples, 676247 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-10 09:41:54,798 : INFO : PROGRESS: at 91.50% examples, 676829 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:55,801 : INFO : PROGRESS: at 93.19% examples, 678344 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:56,804 : INFO : PROGRESS: at 94.91% examples, 680150 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:57,806 : INFO : PROGRESS: at 96.61% examples, 681561 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:58,810 : INFO : PROGRESS: at 98.36% examples, 683241 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:41:59,731 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-10 09:41:59,738 : INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-10 09:41:59,744 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-10 09:41:59,746 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-10 09:41:59,747 : INFO : training on 62511700 raw words (45598925 effective words) took 66.6s, 684642 effective words/s\n"
     ]
    }
   ],
   "source": [
    "result = build_dict(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents_pos = result['tok_sents_pos']\n",
    "model = result['model']\n",
    "train_x_pos = sentence2idx(sents_pos, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-10 09:42:10,793 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('decent', 0.7672065496444702),\n",
       " ('great', 0.7299867868423462),\n",
       " ('bad', 0.6878710389137268),\n",
       " ('nice', 0.6379164457321167),\n",
       " ('cool', 0.630168616771698),\n",
       " ('terrific', 0.6098483800888062),\n",
       " ('fine', 0.6026772856712341),\n",
       " ('solid', 0.6011196970939636),\n",
       " ('mediocre', 0.5951859951019287),\n",
       " ('funny', 0.5892332196235657)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  792,    16,    29,     4,     0,   118,  1764,  7121,    10,\n",
      "          19,   983,     5,    28, 20422,     5,     7,    12,  2375,\n",
      "        1807,   128,  2230,     5,     3,  6987,   300,     1,  2584,\n",
      "        2314,     0,    19,    36,   484,  5942,    12,  3371,     2,\n",
      "          39,    12,     3,  1003,   175,    21,    50,   782,     1]), array([   78,     1,     1, 49001,     9,     0,   197,   627,   131,\n",
      "           8,  3345,     2,     9,   250,     0,    17,   179,   710,\n",
      "           5,   107,     1,   190,    32, 12560,     5,    99, 10283,\n",
      "        1478,     2,  3369,     1,  1642,    71,   147,   103,   627,\n",
      "         168,  4411,   204,   102,    32,  1747,     0,    90,    29,\n",
      "           9,   375,  2383, 26621,   151,    63,   537,   305,     6,\n",
      "        2200,     4,  9892,     0,   861,   137,     8,  6074,   397,\n",
      "          55,    32,   955,   155,    29,     4,   132, 19296, 12672,\n",
      "           4,     3,    17, 10283,  5509,    61,    23,    28,  1148,\n",
      "           3,    86,    17,     8,   189,     7,    61,    23,    28,\n",
      "          56,  1148,    34,  8097,     4,     3,    86,    17,    15,\n",
      "           7,   445,    59,     5,    51,    75,    11,     1,    14,\n",
      "           9,  3301,     5,   850,   150,     7,     9,  1901,    11,\n",
      "          57,   502,   627,     5,  7401,     2,     9,  3195, 29310,\n",
      "          11,     0,   115,   240,    36,     0,  1732,    55,  9426,\n",
      "         144,     0,   251,     2,   499,    71,     1,    72,    32,\n",
      "          65,   104,    11,  5801,     0,    81,    35,    66, 16752,\n",
      "           8,     0,    84,   103,    59,    14,   150,    32,    70,\n",
      "         177,    36,     0,  1313,  1210,    55,    99,     0,    17,\n",
      "          16,     0,  2532,   169, 12557,   101,   125,  1209,     9,\n",
      "          55,   113,  1414,  2636,    10,    17,     9,   375,    39,\n",
      "          24,  9625,   734,     5,   163,   182,    11,  3560,    78,     1])]\n"
     ]
    }
   ],
   "source": [
    "sents_neg = result['tok_sents_neg']\n",
    "train_x_neg = sentence2idx(sents_neg, model)\n",
    "print(train_x_neg[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = train_x_pos + train_x_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = [1] * len(train_x_pos) + [0] * len(train_x_neg)\n",
    "# print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_path = '../data/Method3/aclImdb/test'\n",
    "test_sents_pos = read_dataset(test_path+'/pos/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok_test_sents_pos = [nltk.word_tokenize(sent) for sent in test_sents_pos]\n",
    "test_x_pos = sentence2idx(tok_test_sents_pos, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sents_neg = read_dataset(test_path+'/neg/')\n",
    "tok_test_sents_neg = [nltk.word_tokenize(sent) for sent in test_sents_neg]\n",
    "test_x_neg = sentence2idx(tok_test_sents_neg, model)\n",
    "test_x = test_x_pos + test_x_neg\n",
    "test_y = [1] * len(test_x_pos) + [0] * len(test_x_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save converted sentence indices to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('../data/Method1_and_2/gensim/gensim_imdb.pkl', 'wb')\n",
    "pkl.dump((train_x, train_y), f, -1)\n",
    "pkl.dump((test_x, test_y), f, -1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('../data/Method1_and_2/gensim/gensim_imdb.pkl', 'rb')\n",
    "train_set = pkl.load(f)\n",
    "test_set = pkl.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save word embeddings to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Wemb = create_Wemb(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('../data/Method1_and_2/gensim/gensim_imdb_Wemb.pkl', 'wb')\n",
    "pkl.dump(Wemb, f, -1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('../data/Method1_and_2/gensim/gensim_imdb_Wemb.pkl', 'rb')\n",
    "Wemb_file = pkl.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -1.92171264e+00  -3.81147116e-01  -1.29860923e-01 ...,  -2.91345835e-01\n",
      "   -1.78681016e+00  -7.31336296e-01]\n",
      " [ -8.08200777e-01   4.21601906e-02   3.02510470e-01 ...,   2.95328256e-03\n",
      "   -6.02169633e-01  -4.39393252e-01]\n",
      " [ -8.35283518e-01  -1.01795304e+00  -7.26746678e-01 ...,   5.62283099e-01\n",
      "   -2.69957393e-01   4.16467078e-02]\n",
      " ..., \n",
      " [  3.78807150e-02  -1.21285790e-03  -3.67050283e-02 ...,   8.16808548e-03\n",
      "   -3.65836434e-02   7.64383702e-03]\n",
      " [  5.21445349e-02  -1.07249478e-02  -6.36772141e-02 ...,  -4.69883308e-02\n",
      "   -2.36288421e-02   1.54223349e-02]\n",
      " [ -3.80863249e-02   4.27303603e-03  -6.16931077e-03 ...,  -2.31780261e-02\n",
      "   -3.32884826e-02   3.24734516e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(Wemb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -1.92171264e+00  -3.81147116e-01  -1.29860923e-01 ...,  -2.91345835e-01\n",
      "   -1.78681016e+00  -7.31336296e-01]\n",
      " [ -8.08200777e-01   4.21601906e-02   3.02510470e-01 ...,   2.95328256e-03\n",
      "   -6.02169633e-01  -4.39393252e-01]\n",
      " [ -8.35283518e-01  -1.01795304e+00  -7.26746678e-01 ...,   5.62283099e-01\n",
      "   -2.69957393e-01   4.16467078e-02]\n",
      " ..., \n",
      " [  3.78807150e-02  -1.21285790e-03  -3.67050283e-02 ...,   8.16808548e-03\n",
      "   -3.65836434e-02   7.64383702e-03]\n",
      " [  5.21445349e-02  -1.07249478e-02  -6.36772141e-02 ...,  -4.69883308e-02\n",
      "   -2.36288421e-02   1.54223349e-02]\n",
      " [ -3.80863249e-02   4.27303603e-03  -6.16931077e-03 ...,  -2.31780261e-02\n",
      "   -3.32884826e-02   3.24734516e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(Wemb_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include all above into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_gensim_w2vec(path):\n",
    "    result = build_dict(path+'train')\n",
    "    model = result['model']\n",
    "\n",
    "    sents_pos = result['tok_sents_pos']\n",
    "    train_x_pos = sentence2idx(sents_pos, model)\n",
    "\n",
    "    sents_neg = result['tok_sents_neg']\n",
    "    train_x_neg = sentence2idx(sents_neg, model)\n",
    "    train_x = train_x_pos + train_x_neg\n",
    "    train_y = [1] * len(train_x_pos) + [0] * len(train_x_neg)\n",
    "\n",
    "    test_sents_pos = read_dataset(path+'test/pos/')\n",
    "    tok_test_sents_pos = [nltk.word_tokenize(sent) for sent in test_sents_pos]\n",
    "    test_x_pos = sentence2idx(tok_test_sents_pos, model)\n",
    "\n",
    "    test_sents_neg = read_dataset(path+'test/neg/')\n",
    "    tok_test_sents_neg = [nltk.word_tokenize(sent) for sent in test_sents_neg]\n",
    "    test_x_neg = sentence2idx(tok_test_sents_neg, model)\n",
    "    test_x = test_x_pos + test_x_neg\n",
    "    test_y = [1] * len(test_x_pos) + [0] * len(test_x_neg)\n",
    "\n",
    "    f = open('../data/Method1_and_2/gensim/gensim_imdb.pkl', 'wb')\n",
    "    pkl.dump((train_x, train_y), f, -1)\n",
    "    pkl.dump((test_x, test_y), f, -1)\n",
    "    f.close()\n",
    "\n",
    "    Wemb = create_Wemb(model)\n",
    "    f = open('../data/Method1_and_2/gensim/gensim_imdb_Wemb.pkl', 'wb')\n",
    "    pkl.dump(Wemb, f, -1)\n",
    "    f.close()\n",
    "\n",
    "    model.save('../data/Method1_and_2/gensim/imdb_gensim_w2vmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-10 09:44:59,609 : INFO : collecting all words and their counts\n",
      "2018-07-10 09:44:59,611 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-07-10 09:45:00,369 : INFO : PROGRESS: at sentence #10000, processed 2518483 words, keeping 63047 word types\n",
      "2018-07-10 09:45:01,003 : INFO : PROGRESS: at sentence #20000, processed 5010758 words, keeping 90480 word types\n",
      "2018-07-10 09:45:01,312 : INFO : collected 101119 word types from a corpus of 6251170 raw words and 25000 sentences\n",
      "2018-07-10 09:45:01,314 : INFO : Loading a fresh vocabulary\n",
      "2018-07-10 09:45:02,048 : INFO : min_count=1 retains 101119 unique words (100% of original 101119, drops 0)\n",
      "2018-07-10 09:45:02,049 : INFO : min_count=1 leaves 6251170 word corpus (100% of original 6251170, drops 0)\n",
      "2018-07-10 09:45:02,534 : INFO : deleting the raw counts dictionary of 101119 items\n",
      "2018-07-10 09:45:02,540 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2018-07-10 09:45:02,544 : INFO : downsampling leaves estimated 4559750 word corpus (72.9% of prior 6251170)\n",
      "2018-07-10 09:45:02,566 : INFO : estimated required memory for 101119 words and 128 dimensions: 154105356 bytes\n",
      "2018-07-10 09:45:03,261 : INFO : resetting layer weights\n",
      "2018-07-10 09:45:04,894 : INFO : training model with 4 workers on 101119 vocabulary and 128 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-07-10 09:45:05,902 : INFO : PROGRESS: at 1.63% examples, 745413 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:06,951 : INFO : PROGRESS: at 2.64% examples, 593637 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-10 09:45:07,955 : INFO : PROGRESS: at 3.03% examples, 457225 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:08,970 : INFO : PROGRESS: at 3.37% examples, 380297 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:09,981 : INFO : PROGRESS: at 4.44% examples, 402025 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:10,992 : INFO : PROGRESS: at 5.60% examples, 422168 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:12,023 : INFO : PROGRESS: at 6.61% examples, 425321 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:13,032 : INFO : PROGRESS: at 7.40% examples, 415796 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:14,047 : INFO : PROGRESS: at 8.34% examples, 417331 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:15,049 : INFO : PROGRESS: at 9.86% examples, 443088 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:16,066 : INFO : PROGRESS: at 11.36% examples, 464196 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:17,076 : INFO : PROGRESS: at 12.88% examples, 483942 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:18,077 : INFO : PROGRESS: at 14.53% examples, 504306 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:19,085 : INFO : PROGRESS: at 16.00% examples, 515741 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:20,090 : INFO : PROGRESS: at 17.29% examples, 519380 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:21,094 : INFO : PROGRESS: at 18.22% examples, 513724 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:22,098 : INFO : PROGRESS: at 19.67% examples, 521572 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:23,107 : INFO : PROGRESS: at 21.22% examples, 531513 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:24,113 : INFO : PROGRESS: at 22.34% examples, 530498 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-10 09:45:25,118 : INFO : PROGRESS: at 23.25% examples, 524737 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:26,124 : INFO : PROGRESS: at 24.44% examples, 525892 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:27,125 : INFO : PROGRESS: at 26.05% examples, 535338 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:28,126 : INFO : PROGRESS: at 27.49% examples, 540086 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:29,134 : INFO : PROGRESS: at 29.11% examples, 547985 words/s, in_qsize 8, out_qsize 1\n",
      "2018-07-10 09:45:30,135 : INFO : PROGRESS: at 30.85% examples, 557713 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:31,136 : INFO : PROGRESS: at 31.94% examples, 555228 words/s, in_qsize 8, out_qsize 1\n",
      "2018-07-10 09:45:32,167 : INFO : PROGRESS: at 33.15% examples, 554771 words/s, in_qsize 5, out_qsize 2\n",
      "2018-07-10 09:45:33,171 : INFO : PROGRESS: at 34.56% examples, 558174 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:34,178 : INFO : PROGRESS: at 36.14% examples, 563356 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:35,195 : INFO : PROGRESS: at 37.70% examples, 567651 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-10 09:45:36,211 : INFO : PROGRESS: at 38.81% examples, 565430 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-10 09:45:37,211 : INFO : PROGRESS: at 39.88% examples, 562758 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:38,214 : INFO : PROGRESS: at 41.22% examples, 564258 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:39,220 : INFO : PROGRESS: at 42.22% examples, 561044 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:40,239 : INFO : PROGRESS: at 43.66% examples, 563547 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:41,240 : INFO : PROGRESS: at 44.98% examples, 564994 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:42,253 : INFO : PROGRESS: at 46.48% examples, 567687 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-10 09:45:43,268 : INFO : PROGRESS: at 48.07% examples, 571543 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:44,279 : INFO : PROGRESS: at 49.33% examples, 571217 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:45,289 : INFO : PROGRESS: at 50.76% examples, 573209 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:46,294 : INFO : PROGRESS: at 52.25% examples, 575700 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:47,299 : INFO : PROGRESS: at 53.56% examples, 576286 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:48,299 : INFO : PROGRESS: at 55.09% examples, 579183 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:49,316 : INFO : PROGRESS: at 56.20% examples, 577325 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:50,319 : INFO : PROGRESS: at 57.92% examples, 581619 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:51,328 : INFO : PROGRESS: at 59.44% examples, 583760 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-10 09:45:52,336 : INFO : PROGRESS: at 60.84% examples, 584913 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-10 09:45:53,336 : INFO : PROGRESS: at 62.26% examples, 586270 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-10 09:45:54,341 : INFO : PROGRESS: at 63.78% examples, 588441 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:55,342 : INFO : PROGRESS: at 65.25% examples, 590257 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:56,354 : INFO : PROGRESS: at 66.85% examples, 592686 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:57,356 : INFO : PROGRESS: at 68.39% examples, 594748 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-10 09:45:58,367 : INFO : PROGRESS: at 70.09% examples, 597695 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:45:59,369 : INFO : PROGRESS: at 71.80% examples, 601250 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:46:00,375 : INFO : PROGRESS: at 73.58% examples, 604958 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:46:01,381 : INFO : PROGRESS: at 75.38% examples, 608920 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:46:02,398 : INFO : PROGRESS: at 77.16% examples, 612076 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:46:03,399 : INFO : PROGRESS: at 78.84% examples, 614697 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-10 09:46:04,402 : INFO : PROGRESS: at 80.56% examples, 617438 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-10 09:46:05,409 : INFO : PROGRESS: at 82.27% examples, 620034 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:46:06,421 : INFO : PROGRESS: at 83.97% examples, 622555 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:46:07,423 : INFO : PROGRESS: at 85.74% examples, 625622 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:46:08,428 : INFO : PROGRESS: at 87.38% examples, 627348 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:46:09,439 : INFO : PROGRESS: at 88.31% examples, 624165 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-10 09:46:10,447 : INFO : PROGRESS: at 89.58% examples, 623192 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:46:11,468 : INFO : PROGRESS: at 91.04% examples, 623724 words/s, in_qsize 7, out_qsize 1\n",
      "2018-07-10 09:46:12,477 : INFO : PROGRESS: at 92.16% examples, 621927 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-10 09:46:13,488 : INFO : PROGRESS: at 93.25% examples, 620093 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:46:14,489 : INFO : PROGRESS: at 94.38% examples, 618694 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:46:15,495 : INFO : PROGRESS: at 95.76% examples, 618879 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:46:16,511 : INFO : PROGRESS: at 97.32% examples, 619810 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-10 09:46:17,514 : INFO : PROGRESS: at 98.19% examples, 616762 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-10 09:46:18,518 : INFO : PROGRESS: at 99.64% examples, 617200 words/s, in_qsize 5, out_qsize 2\n",
      "2018-07-10 09:46:18,784 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-10 09:46:18,800 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-10 09:46:18,802 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-10 09:46:18,804 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-10 09:46:18,806 : INFO : training on 62511700 raw words (45598623 effective words) took 73.9s, 616975 effective words/s\n",
      "2018-07-10 09:47:30,526 : INFO : saving Word2Vec object under ../data/Method1_and_2/gensim/imdb_gensim_w2vmodel, separately None\n",
      "2018-07-10 09:47:30,530 : INFO : storing np array 'syn0' to ../data/Method1_and_2/gensim/imdb_gensim_w2vmodel.wv.syn0.npy\n",
      "2018-07-10 09:47:30,854 : INFO : not storing attribute syn0norm\n",
      "2018-07-10 09:47:30,860 : INFO : storing np array 'syn1neg' to ../data/Method1_and_2/gensim/imdb_gensim_w2vmodel.syn1neg.npy\n",
      "2018-07-10 09:47:31,170 : INFO : not storing attribute cum_table\n",
      "2018-07-10 09:47:31,479 : INFO : saved ../data/Method1_and_2/gensim/imdb_gensim_w2vmodel\n"
     ]
    }
   ],
   "source": [
    "path = '../data/Method3/aclImdb/'\n",
    "train_gensim_w2vec(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if embeddings from gensim model equal to the word embeddings stored in the file to make sure the correctness of storing word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_idxwemb_wordemb(sentence_idx):\n",
    "    model_path = '../data/Method1_and_2/gensim/imdb_gensim_w2vmodel'\n",
    "    model = gensim.models.Word2Vec.load(model_path)\n",
    "    sentence_words = []\n",
    "    for idx, x in enumerate(sentence_idx):\n",
    "        sentence_words.append(model.wv[model.wv.index2word[x]])\n",
    "\n",
    "    f = open('../data/Method1_and_2/gensim/gensim_imdb_Wemb.pkl', 'rb')\n",
    "    Wemb = pkl.load(f)\n",
    "    f.close()\n",
    "    emb_x = Wemb[sentence_idx]\n",
    "\n",
    "    return (numpy.matrix(sentence_words)==numpy.matrix(emb_x)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-10 09:49:00,239 : INFO : loading Word2Vec object from ../data/Method1_and_2/gensim/imdb_gensim_w2vmodel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  792    16    29     4     0   118  1764  7121    10    19   983     5\n",
      "    28 20422     5     7    12  2375  1807   128  2230     5     3  6987\n",
      "   300     1  2584  2314     0    19    36   484  5942    12  3371     2\n",
      "    39    12     3  1003   175    21    50   782     1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-10 09:49:01,312 : INFO : loading wv recursively from ../data/Method1_and_2/gensim/imdb_gensim_w2vmodel.wv.* with mmap=None\n",
      "2018-07-10 09:49:01,314 : INFO : loading syn0 from ../data/Method1_and_2/gensim/imdb_gensim_w2vmodel.wv.syn0.npy with mmap=None\n",
      "2018-07-10 09:49:01,367 : INFO : setting ignored attribute syn0norm to None\n",
      "2018-07-10 09:49:01,371 : INFO : loading syn1neg from ../data/Method1_and_2/gensim/imdb_gensim_w2vmodel.syn1neg.npy with mmap=None\n",
      "2018-07-10 09:49:01,442 : INFO : setting ignored attribute cum_table to None\n",
      "2018-07-10 09:49:01,445 : INFO : loaded ../data/Method1_and_2/gensim/imdb_gensim_w2vmodel\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_x_neg[0])\n",
    "compare_idxwemb_wordemb(train_x_neg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
