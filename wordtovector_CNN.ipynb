{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import word2vec\n",
    "import numpy\n",
    "import pickle\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from subprocess import Popen, PIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences):\n",
    "    #print('Tokenizing sentences..')\n",
    "    text = \"\".join(sentences)\n",
    "    \n",
    "    tokenizer = Popen(tokenizer_cmd, stdin=PIPE, stdout=PIPE)\n",
    "    tok_text, _ = tokenizer.communicate(bytes(text,'utf-8'))\n",
    "    \n",
    "    \n",
    "    finalSentences = tok_text.decode('utf-8').split('\\n')[:-1]\n",
    "    \n",
    "    #sentencesArray = tok_text.decode('utf-8').split('.')[:-1]\n",
    "    #finalSentences = []\n",
    "    #for sentence in sentencesArray:\n",
    "    #    finalSentences.append(sentence.split('\\n'))\n",
    "    return finalSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_dictionary(sentences):\n",
    "    #print('Tokenizing dictionary..')\n",
    "    text = \"\".join(sentences)\n",
    "    \n",
    "    tokenizer = Popen(tokenizer_cmd, stdin=PIPE, stdout=PIPE)\n",
    "    tok_text, _ = tokenizer.communicate(bytes(text,'utf-8'))\n",
    "    toks = tok_text.decode('utf-8').split('\\n')[:-1]\n",
    "    return toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dict(path):\n",
    "    texts = []\n",
    "    currdir = os.getcwd()\n",
    "    os.chdir('%s/pos/' % path)\n",
    "    for ff in glob.glob(\"*.txt\"):\n",
    "        with open(ff, 'r') as f:\n",
    "            texts.append(f.readline().strip())\n",
    "    os.chdir('%s/neg/' % path)\n",
    "    for ff in glob.glob(\"*.txt\"):\n",
    "        with open(ff, 'r') as f:\n",
    "            texts.append(f.readline().strip())\n",
    "    os.chdir(currdir)\n",
    "\n",
    "    \n",
    "    sentences = tokenize_dictionary(texts)\n",
    "    \n",
    "    print('Building dictionary..')\n",
    "    wordcount = dict()\n",
    "    for ss in sentences:\n",
    "        words = ss.strip().lower().split()\n",
    "        for w in words:\n",
    "            if w not in wordcount:\n",
    "                wordcount[w] = 1\n",
    "            else:\n",
    "                wordcount[w] += 1\n",
    "\n",
    "    counts = list(wordcount.values())\n",
    "    keys = list(wordcount.keys())\n",
    "\n",
    "    sorted_idx = numpy.argsort(counts)[::-1]\n",
    "\n",
    "    worddict = dict()\n",
    "\n",
    "    for idx, ss in enumerate(sorted_idx):\n",
    "        worddict[keys[ss]] = idx+2  # leave 0 and 1 (UNK)\n",
    " \n",
    "    print(numpy.sum(counts), ' total words ', len(keys), ' unique words')\n",
    "\n",
    "    return worddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grab_data(path, dictionary):\n",
    "    texts = []\n",
    "    currdir = os.getcwd()\n",
    "    os.chdir(path)\n",
    "    for ff in glob.glob(\"*.txt\"):\n",
    "        with open(ff, 'r') as f:\n",
    "            texts.append(f.readline().strip())\n",
    "    os.chdir(currdir)\n",
    "    \n",
    "    seqs_text = []\n",
    "    sentenceCount = 0\n",
    "    textCount = 0\n",
    "    for text in texts:\n",
    "        #print(text)\n",
    "        words = tokenize_sentences(text)\n",
    "        seqs_sentence = []\n",
    "        for idx, ss in enumerate(words):\n",
    "            word = ss.strip().lower()\n",
    "            wordfound = False\n",
    "            for (k, v) in dictionary.items():\n",
    "                if(k == word):\n",
    "                    seqs_sentence.append(v)  \n",
    "                    wordfound = True\n",
    "                    break\n",
    "            if(wordfound == False):\n",
    "                seqs_sentence.append(1) \n",
    "            #sentenceCount += 1\n",
    "        seqs_text.append(seqs_sentence)\n",
    "        #textCount +=1\n",
    "        #if(textCount >= 2): break\n",
    "    print(seqs_text)\n",
    "    return seqs_text\n",
    "    \n",
    "    return\n",
    "    # sentences = tokenize(sentences)\n",
    "\n",
    "    seqs = [None] * len(sentences)\n",
    "    for idx, ss in enumerate(sentences):\n",
    "        words = ss.strip().lower().split()\n",
    "        seqs[idx] = [dictionary[w] if w in dictionary else 1 for w in words]\n",
    "\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['For', 'a', 'movie', 'that', 'gets', 'no', 'respect', 'there', 'sure', 'are', 'a', 'lot', 'of', 'memorable', 'quotes', 'listed', 'for', 'this', 'gem .', 'Imagine', 'a', 'movie', 'where', 'Joe', 'Piscopo', 'is', 'actually', 'funny', '!', '', 'Maureen', 'Stapleton', 'is', 'a', 'scene', 'stealer .', 'The', 'Moroni', 'character', 'is', 'an', 'absolute', 'scream .', 'Watch', 'for', 'Alan', '', '&quot;', 'The', 'Skipper', '&quot;', '', 'Hale', 'jr.', 'as', 'a', 'police', 'Sgt.']\n"
     ]
    }
   ],
   "source": [
    "data_path = '/Users/lifa08/Documents/Lifa/Machine_Learning/Miniproject/aclImdb/'\n",
    "tokenizer_cmd = ['/Users/lifa08/Documents/Lifa/Machine_Learning/Miniproject/mosesdecoder-master/scripts/tokenizer/tokenizer.perl', '-l', 'en', '-q', '-' ] \n",
    "text = ['For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt.']\n",
    "testOutput = tokenize_dictionary(text)\n",
    "print(testOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary..\n",
      "7078735  total words  114053  unique words\n"
     ]
    }
   ],
   "source": [
    "dictionary = build_dict(os.path.join(data_path, 'train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "f = open('imdb_own.dict.pkl', 'wb')\n",
    "pkl.dump(dictionary, f, -1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "train_x_pos = grab_data(data_path+'train/pos', dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12500,)\n"
     ]
    }
   ],
   "source": [
    "print(numpy.array(train_x_pos).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('train_x_pos_own.pkl', 'wb')\n",
    "pkl.dump((train_x_pos), f, -1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "train_x_neg = grab_data(data_path+'train/neg', dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12500,)\n"
     ]
    }
   ],
   "source": [
    "print(numpy.array(train_x_neg).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = [1] * len(train_x_pos) + [0] * len(train_x_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = train_x_pos + train_x_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('train_x_own.pkl', 'wb')\n",
    "pkl.dump((train_x, train_y), f, -1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "test_x_pos = grab_data(data_path+'test/pos', dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "test_x_neg = grab_data(data_path+'test/neg', dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_y = [1] * len(test_x_pos) + [0] * len(test_x_neg)\n",
    "test_x = test_x_pos + test_x_neg\n",
    "\n",
    "f = open('test_x_own.pkl', 'wb')\n",
    "pkl.dump((test_x, test_y), f, -1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Get the dataset from http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "    path = 'aclImdb'\n",
    "    dictionary = build_dict(os.path.join(path, 'train'))\n",
    "\n",
    "    train_x_pos = grab_data(path+'train/pos', dictionary)\n",
    "    train_x_neg = grab_data(path+'train/neg', dictionary)\n",
    "    train_x = train_x_pos + train_x_neg\n",
    "    train_y = [1] * len(train_x_pos) + [0] * len(train_x_neg)\n",
    "\n",
    "    test_x_pos = grab_data(path+'test/pos', dictionary)\n",
    "    test_x_neg = grab_data(path+'test/neg', dictionary)\n",
    "    test_x = test_x_pos + test_x_neg\n",
    "    test_y = [1] * len(test_x_pos) + [0] * len(test_x_neg)\n",
    "\n",
    "    f = open('imdb.pkl', 'wb')\n",
    "    pkl.dump((train_x, train_y), f, -1)\n",
    "    pkl.dump((test_x, test_y), f, -1)\n",
    "    f.close()\n",
    "\n",
    "    f = open('imdb.dict.pkl', 'wb')\n",
    "    pkl.dump(dictionary, f, -1)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_dataset(path):\n",
    "    \"\"\"Read a dataset, where the first column contains a real-valued score,\n",
    "    followed by a tab and a string of words.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line_parts = line.strip().split(\"\\t\")\n",
    "            dataset.append((float(line_parts[0]), line_parts[1].lower()))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences_train = read_dataset('train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('sentence_forword2vector.txt', 'w')\n",
    "for label, sentence in sentences_train:\n",
    "    f.write(sentence)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file sentence_forword2vector.txt\n",
      "Words processed: 100K     Vocab size: 76K  \n",
      "Vocab size (unigrams + bigrams): 51508\n",
      "Words in train file: 133711\n",
      "Words written: 100K\r"
     ]
    }
   ],
   "source": [
    "word2vec.word2phrase('sentence_forword2vector.txt', 'sentence_phrases.txt', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file sentence_phrases.txt\n",
      "Vocab size: 3062\n",
      "Words in train file: 110763\n"
     ]
    }
   ],
   "source": [
    "word2vec.word2vec('sentence_phrases.txt', 'sentence_phrases_bin.bin', size=100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file sentence_phrases_bin.bin\n",
      "Vocab size: 9\n",
      "Words in train file: 7240\n"
     ]
    }
   ],
   "source": [
    "word2vec.word2clusters('sentence_phrases_bin.bin', 'sentence_phrases_clusters.txt', 100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lifa08/anaconda3/lib/python3.6/site-packages/word2vec/utils.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return (1.0 / LA.norm(vec, ord=2)) * vec\n",
      "/Users/lifa08/anaconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py:2168: RuntimeWarning: invalid value encountered in sqrt\n",
      "  ret = sqrt(sqnorm)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['</s>', ',', 'the', ..., 'ethnic', 'nonsense', 'earth'],\n",
       "      dtype='<U78')"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = word2vec.load('sentence_phrases_bin.bin')\n",
    "model.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3062, 100)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.58986646e-13,   7.27593387e-13,  -6.30599360e-13, ...,\n",
       "          2.50888205e-13,   5.03685635e-13,   3.18791556e-14],\n",
       "       [             inf,             -inf,              inf, ...,\n",
       "                     inf,             -inf,             -inf],\n",
       "       [            -inf,             -inf,              inf, ...,\n",
       "                    -inf,             -inf,             -inf],\n",
       "       ..., \n",
       "       [             inf,             -inf,              inf, ...,\n",
       "                     inf,             -inf,             -inf],\n",
       "       [             inf,             -inf,              inf, ...,\n",
       "                     inf,             -inf,             -inf],\n",
       "       [             inf,             -inf,              inf, ...,\n",
       "                     inf,             -inf,             -inf]])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['nonsense'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ inf, -inf,  inf,  inf,  inf,  inf, -inf, -inf, -inf,  inf])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[','][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1016, 1025, 1024, 1023, 1022, 1021, 1020, 1019, 1018, 1017]),\n",
       " array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan]))"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes, metrics = model.cosine('earth')\n",
    "indexes, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['faith', 'writer', 'class', 'ultimate', 'cute', 'fears', 'fit',\n",
       "       '.nothing', 'steven', 'several'],\n",
       "      dtype='<U78')"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocab[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rec.array([('faith',  nan), ('writer',  nan), ('class',  nan),\n",
       "           ('ultimate',  nan), ('cute',  nan), ('fears',  nan),\n",
       "           ('fit',  nan), ('.nothing',  nan), ('steven',  nan),\n",
       "           ('several',  nan)], \n",
       "          dtype=[('word', '<U78'), ('metric', '<f8')])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_response(indexes, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('faith', nan),\n",
       " ('writer', nan),\n",
       " ('class', nan),\n",
       " ('ultimate', nan),\n",
       " ('cute', nan),\n",
       " ('fears', nan),\n",
       " ('fit', nan),\n",
       " ('.nothing', nan),\n",
       " ('steven', nan),\n",
       " ('several', nan)]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_response(indexes, metrics).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('faith', nan),\n",
       " ('writer', nan),\n",
       " ('class', nan),\n",
       " ('ultimate', nan),\n",
       " ('cute', nan),\n",
       " ('fears', nan),\n",
       " ('fit', nan),\n",
       " ('.nothing', nan),\n",
       " ('steven', nan),\n",
       " ('several', nan)]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes, metrics = model.cosine('good')\n",
    "model.generate_response(indexes, metrics).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://radimrehurek.com/gensim/models/word2vec.html\n",
    "# https://rare-technologies.com/word2vec-tutorial/\n",
    "\n",
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences_train = read_dataset('train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.69444, \"the rock is destined to be the 21st century 's new `` conan '' and that he 's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\"), (0.83333, \"the gorgeously elaborate continuation of `` the lord of the rings '' trilogy is so huge that a column of words can not adequately describe co-writer\\\\/director peter jackson 's expanded vision of j.r.r. tolkien 's middle-earth .\"), (0.625, 'singer\\\\/composer bryan adams contributes a slew of songs -- a few potential hits , a few more simply intrusive to the story -- but the whole package certainly captures the intended , er , spirit of the piece .'), (0.5, \"you 'd think by now america would have had enough of plucky british eccentrics with hearts of gold .\"), (0.72222, 'yet the act is still charming here .'), (0.83333, \"whether or not you 're enlightened by any of derrida 's lectures on `` the other '' and `` the self , '' derrida is an undeniably fascinating and playful fellow .\"), (0.875, 'just the labour involved in creating the layered richness of the imagery in this chiaroscuro of madness and light is astonishing .'), (0.72222, 'part of the charm of satin rouge is that it avoids the obvious with humour and lightness .')]\n"
     ]
    }
   ],
   "source": [
    "print(sentences_train[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the gorgeously elaborate continuation of `` the lord of the rings '' trilogy is so huge that a column of words can not adequately describe co-writer\\/director peter jackson 's expanded vision of j.r.r. tolkien 's middle-earth .\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "i = 0\n",
    "for label, sentence in sentences_train:\n",
    "    sentences.append(sentence)\n",
    "\n",
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/lifa08/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/lifa08/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/lifa08/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/lifa08/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/lifa08/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/lifa08/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/lifa08/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/lifa08/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/lifa08/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/lifa08/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/lifa08/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /Users/lifa08/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/lifa08/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/lifa08/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/lifa08/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/lifa08/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/lifa08/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/lifa08/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/lifa08/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('popular')\n",
    "# Tokenize the sentences into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'century', \"'s\", 'new', '``', 'conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarzenegger', ',', 'jean-claud', 'van', 'damme', 'or', 'steven', 'segal', '.'], ['the', 'gorgeously', 'elaborate', 'continuation', 'of', '``', 'the', 'lord', 'of', 'the', 'rings', \"''\", 'trilogy', 'is', 'so', 'huge', 'that', 'a', 'column', 'of', 'words', 'can', 'not', 'adequately', 'describe', 'co-writer\\\\/director', 'peter', 'jackson', \"'s\", 'expanded', 'vision', 'of', 'j.r.r', '.', 'tolkien', \"'s\", 'middle-earth', '.'], ['singer\\\\/composer', 'bryan', 'adams', 'contributes', 'a', 'slew', 'of', 'songs', '--', 'a', 'few', 'potential', 'hits', ',', 'a', 'few', 'more', 'simply', 'intrusive', 'to', 'the', 'story', '--', 'but', 'the', 'whole', 'package', 'certainly', 'captures', 'the', 'intended', ',', 'er', ',', 'spirit', 'of', 'the', 'piece', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_sentences[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-17 21:56:00,583 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2018-01-17 21:56:00,586 : INFO : collecting all words and their counts\n",
      "2018-01-17 21:56:00,587 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-01-17 21:56:00,589 : INFO : collected 100 word types from a corpus of 140 raw words and 5 sentences\n",
      "2018-01-17 21:56:00,591 : INFO : Loading a fresh vocabulary\n",
      "2018-01-17 21:56:00,593 : INFO : min_count=1 retains 100 unique words (100% of original 100, drops 0)\n",
      "2018-01-17 21:56:00,596 : INFO : min_count=1 leaves 140 word corpus (100% of original 140, drops 0)\n",
      "2018-01-17 21:56:00,598 : INFO : deleting the raw counts dictionary of 100 items\n",
      "2018-01-17 21:56:00,602 : INFO : sample=0.001 downsamples 100 most-common words\n",
      "2018-01-17 21:56:00,603 : INFO : downsampling leaves estimated 55 word corpus (40.0% of prior 140)\n",
      "2018-01-17 21:56:00,605 : INFO : estimated required memory for 100 words and 10 dimensions: 58000 bytes\n",
      "2018-01-17 21:56:00,608 : INFO : resetting layer weights\n",
      "2018-01-17 21:56:00,614 : INFO : training model with 4 workers on 100 vocabulary and 10 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-01-17 21:56:00,619 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-01-17 21:56:00,621 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-17 21:56:00,624 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-17 21:56:00,626 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-17 21:56:00,628 : INFO : training on 700 raw words (268 effective words) took 0.0s, 26476 effective words/s\n",
      "2018-01-17 21:56:00,630 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(tokenized_sentences[0:5], min_count=1, size=10, window=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0302295 ,  0.03751555, -0.0090684 ,  0.03493763, -0.02394874,\n",
       "        0.00456192, -0.03647238,  0.0383511 ,  0.03158452, -0.03736592], dtype=float32)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['but']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-17 21:56:08,364 : INFO : saving Word2Vec object under mygword2vmodel, separately None\n",
      "2018-01-17 21:56:08,367 : INFO : not storing attribute syn0norm\n",
      "2018-01-17 21:56:08,370 : INFO : not storing attribute cum_table\n",
      "2018-01-17 21:56:08,374 : INFO : saved mygword2vmodel\n"
     ]
    }
   ],
   "source": [
    "model.save('mygword2vmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-17 21:56:11,198 : INFO : loading Word2Vec object from mygword2vmodel\n",
      "2018-01-17 21:56:11,202 : INFO : loading wv recursively from mygword2vmodel.wv.* with mmap=None\n",
      "2018-01-17 21:56:11,205 : INFO : setting ignored attribute syn0norm to None\n",
      "2018-01-17 21:56:11,207 : INFO : setting ignored attribute cum_table to None\n",
      "2018-01-17 21:56:11,209 : INFO : loaded mygword2vmodel\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load('mygword2vmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0302295 ,  0.03751555, -0.0090684 ,  0.03493763, -0.02394874,\n",
       "        0.00456192, -0.03647238,  0.0383511 ,  0.03158452, -0.03736592], dtype=float32)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['but']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-17 21:56:15,443 : INFO : collecting all words and their counts\n",
      "2018-01-17 21:56:15,447 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2018-01-17 21:56:15,451 : INFO : collected 233 word types from a corpus of 140 words (unigram + bigrams) and 5 sentences\n",
      "2018-01-17 21:56:15,453 : INFO : using 233 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "bigram_transformer = gensim.models.Phrases(tokenized_sentences[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lifa08/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "2018-01-17 21:56:19,342 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2018-01-17 21:56:19,344 : INFO : collecting all words and their counts\n",
      "2018-01-17 21:56:19,347 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-01-17 21:56:19,350 : INFO : collected 100 word types from a corpus of 140 raw words and 5 sentences\n",
      "2018-01-17 21:56:19,352 : INFO : Loading a fresh vocabulary\n",
      "2018-01-17 21:56:19,355 : INFO : min_count=1 retains 100 unique words (100% of original 100, drops 0)\n",
      "2018-01-17 21:56:19,357 : INFO : min_count=1 leaves 140 word corpus (100% of original 140, drops 0)\n",
      "2018-01-17 21:56:19,360 : INFO : deleting the raw counts dictionary of 100 items\n",
      "2018-01-17 21:56:19,362 : INFO : sample=0.001 downsamples 100 most-common words\n",
      "2018-01-17 21:56:19,364 : INFO : downsampling leaves estimated 55 word corpus (40.0% of prior 140)\n",
      "2018-01-17 21:56:19,366 : INFO : estimated required memory for 100 words and 10 dimensions: 58000 bytes\n",
      "2018-01-17 21:56:19,371 : INFO : resetting layer weights\n",
      "2018-01-17 21:56:19,376 : INFO : training model with 3 workers on 100 vocabulary and 10 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-01-17 21:56:19,380 : WARNING : train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable).\n",
      "2018-01-17 21:56:19,383 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-17 21:56:19,384 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-17 21:56:19,386 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-17 21:56:19,388 : INFO : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-01-17 21:56:19,393 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-01-17 21:56:19,401 : WARNING : supplied example count (0) did not equal expected count (25)\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(bigram_transformer[tokenized_sentences[0:5]], size=10, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03017185,  0.03753857, -0.00916746,  0.03481733, -0.02395921,\n",
       "        0.00431288, -0.03660684,  0.03821143,  0.03154584, -0.0372339 ], dtype=float32)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['but']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=100, size=10, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "model.wv[tokenized_sentences[0]].shape\n",
    "print(len(tokenized_sentences[0]))\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-17 21:56:29,993 : INFO : collecting all words and their counts\n",
      "2018-01-17 21:56:29,996 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2018-01-17 21:56:29,998 : INFO : collected 233 word types from a corpus of 140 words (unigram + bigrams) and 5 sentences\n",
      "2018-01-17 21:56:30,000 : INFO : using 233 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "/Users/lifa08/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "2018-01-17 21:56:30,002 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2018-01-17 21:56:30,005 : INFO : collecting all words and their counts\n",
      "2018-01-17 21:56:30,007 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-01-17 21:56:30,010 : INFO : collected 100 word types from a corpus of 140 raw words and 5 sentences\n",
      "2018-01-17 21:56:30,012 : INFO : Loading a fresh vocabulary\n",
      "2018-01-17 21:56:30,014 : INFO : min_count=1 retains 100 unique words (100% of original 100, drops 0)\n",
      "2018-01-17 21:56:30,016 : INFO : min_count=1 leaves 140 word corpus (100% of original 140, drops 0)\n",
      "2018-01-17 21:56:30,019 : INFO : deleting the raw counts dictionary of 100 items\n",
      "2018-01-17 21:56:30,021 : INFO : sample=0.001 downsamples 100 most-common words\n",
      "2018-01-17 21:56:30,023 : INFO : downsampling leaves estimated 55 word corpus (40.0% of prior 140)\n",
      "2018-01-17 21:56:30,025 : INFO : estimated required memory for 100 words and 5 dimensions: 54000 bytes\n",
      "2018-01-17 21:56:30,027 : INFO : resetting layer weights\n",
      "2018-01-17 21:56:30,032 : INFO : training model with 4 workers on 100 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-01-17 21:56:30,038 : WARNING : train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable).\n",
      "2018-01-17 21:56:30,040 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-01-17 21:56:30,043 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-17 21:56:30,046 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-17 21:56:30,048 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-17 21:56:30,050 : INFO : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-01-17 21:56:30,052 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-01-17 21:56:30,054 : WARNING : supplied example count (0) did not equal expected count (25)\n"
     ]
    }
   ],
   "source": [
    "bigram_transformer = gensim.models.Phrases(tokenized_sentences[0:5])\n",
    "model = gensim.models.Word2Vec(bigram_transformer[tokenized_sentences[0:5]], size=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))\n",
    "print(len(w2v['the']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.02208557  0.09837363  0.05117133  0.02832818  0.05325192]\n",
      " [-0.08818197  0.06067047 -0.05662125 -0.08450653  0.04170386]\n",
      " [-0.03119097  0.02130773 -0.0160614  -0.02544381  0.09212921]\n",
      " [-0.05740406 -0.09070331 -0.08396011  0.02058745 -0.06510007]\n",
      " [-0.04710092 -0.00876878  0.06991214  0.02935793  0.02002611]\n",
      " [ 0.06656373  0.0385834  -0.01902515  0.09457456 -0.02702475]\n",
      " [-0.08470868  0.00165001 -0.06081249  0.02858844  0.04205706]\n",
      " [ 0.06066915  0.03912574  0.06659566 -0.01201316  0.03416824]\n",
      " [-0.07840199  0.08288328 -0.06001099  0.02987055  0.08084781]\n",
      " [-0.01090996 -0.09316173 -0.01690514 -0.04118293 -0.05039416]\n",
      " [-0.07112965 -0.08161124 -0.00730399  0.09642613  0.05540927]\n",
      " [ 0.08947527 -0.03912371  0.06929317  0.02684684  0.02780831]\n",
      " [-0.01014586 -0.02885853  0.07568201  0.04318088 -0.00745011]\n",
      " [ 0.02499827  0.06154311 -0.08528498  0.02327873  0.09503242]\n",
      " [-0.09768962 -0.02994036 -0.05794133 -0.0089996   0.03667355]\n",
      " [-0.04113455 -0.04574016  0.02809235  0.0973269   0.02184772]\n",
      " [ 0.01312863  0.02395977 -0.03681421  0.03054649  0.00212549]\n",
      " [ 0.0413372   0.00682952 -0.09489356 -0.06137311 -0.07631552]\n",
      " [ 0.03129071  0.01224343 -0.05468677  0.00943563 -0.0781847 ]\n",
      " [ 0.07280798 -0.00517613  0.04541467 -0.09641068  0.02611982]\n",
      " [ 0.06551424 -0.02351311 -0.09838882 -0.03178639  0.09659231]\n",
      " [ 0.01877617 -0.07021765  0.01556099  0.0808677  -0.01921784]\n",
      " [-0.02675786  0.07933032  0.06348857 -0.06435207 -0.09183709]\n",
      " [ 0.07568843  0.02404802 -0.09938408 -0.0100976  -0.06329944]\n",
      " [-0.07619162  0.01685455 -0.05385781 -0.0847052  -0.02897531]\n",
      " [-0.0248951  -0.07991102  0.03481456 -0.020491    0.08184424]\n",
      " [ 0.08371945  0.06096968  0.02284763  0.05303666  0.01691001]\n",
      " [ 0.02069098  0.07697137  0.05808778 -0.09156392 -0.06044998]\n",
      " [-0.0893174   0.06647513  0.03608273  0.03505246 -0.00021999]\n",
      " [-0.05139957  0.03789569 -0.003895   -0.08582972 -0.00206084]\n",
      " [ 0.05447593 -0.08484296  0.05157855 -0.098056    0.06866596]\n",
      " [ 0.02053143  0.02440531  0.01443942 -0.02292933  0.0657156 ]\n",
      " [-0.05450443  0.08633521  0.03522423 -0.0625464  -0.00868871]\n",
      " [-0.04607688  0.00113363 -0.02997021  0.08546974  0.04982941]\n",
      " [ 0.06322306  0.07989168  0.03981764 -0.05280844 -0.09425716]\n",
      " [ 0.0626345  -0.08885864 -0.08391272  0.08163058  0.01041862]\n",
      " [ 0.09180424 -0.00692388  0.0898909   0.02154546 -0.02691581]\n",
      " [-0.01659911 -0.09556786  0.00528306 -0.08608948  0.01826818]\n",
      " [-0.02677443  0.06463192 -0.0659262  -0.09810574  0.04052832]\n",
      " [-0.07431163  0.02398859  0.0987974  -0.02411952 -0.02399001]\n",
      " [-0.03113262 -0.09608384  0.09224563  0.03885103  0.0694783 ]\n",
      " [-0.00542978 -0.05061887  0.03570236 -0.0247163   0.00200419]\n",
      " [-0.01280779 -0.04258131 -0.0719157   0.03673592  0.06821556]\n",
      " [ 0.03762987 -0.08613942  0.05890341 -0.00062804 -0.0427044 ]\n",
      " [ 0.03466366 -0.05412368 -0.08241312  0.05064562 -0.08257569]\n",
      " [ 0.02729239  0.08748413 -0.09256148  0.00730945  0.08834153]\n",
      " [-0.02012121 -0.07248519  0.04512032  0.00605386  0.01545877]\n",
      " [ 0.0765168  -0.03539943  0.01920795 -0.03283427  0.08589461]\n",
      " [-0.08624179  0.08878215  0.03495431 -0.07917871  0.00401828]\n",
      " [ 0.01904407  0.06539171  0.08972013  0.08030578  0.02730242]\n",
      " [ 0.0834824  -0.03628479  0.07849384 -0.09896687 -0.07808561]\n",
      " [-0.05162851 -0.03696059  0.08358098 -0.01893236  0.09396729]\n",
      " [ 0.05987629 -0.08214573  0.03864621 -0.05123932 -0.06838399]\n",
      " [-0.00368483 -0.04680032 -0.06548148  0.07438215 -0.07134576]\n",
      " [-0.02305096 -0.05321802  0.00191134 -0.01642684  0.05346838]\n",
      " [-0.04198635 -0.04155859  0.06923201 -0.09650694 -0.05333848]\n",
      " [ 0.08388449 -0.07634359  0.06187332 -0.0021375  -0.0686513 ]\n",
      " [ 0.09327758  0.02116442  0.00519353  0.02722007  0.06689903]\n",
      " [ 0.03251065 -0.07932073 -0.05402292 -0.01245517 -0.05359463]\n",
      " [-0.0085272   0.08849396  0.09781183 -0.07558964 -0.05050895]\n",
      " [ 0.05116026 -0.02013152  0.05111307  0.02622244 -0.06574715]\n",
      " [ 0.07442442  0.09291618 -0.01889432 -0.07443026  0.07240096]\n",
      " [ 0.08095247  0.08167747 -0.03458137  0.06490264 -0.04611895]\n",
      " [-0.07989555  0.00524401 -0.0518267   0.04666588 -0.06019726]\n",
      " [ 0.08543577  0.07618818  0.02475538  0.0145753  -0.05258996]\n",
      " [-0.09608836 -0.01184369  0.04557321 -0.08019754 -0.028711  ]\n",
      " [-0.03088308 -0.09051377  0.01881348  0.03382015  0.00836416]\n",
      " [ 0.02968406 -0.06696992 -0.06166176  0.06892245  0.04496993]\n",
      " [ 0.05204569 -0.05080719  0.06684706  0.08499313  0.08597356]\n",
      " [ 0.04624145 -0.04347841  0.05301302  0.033104   -0.03760967]\n",
      " [ 0.06034371  0.07507715 -0.01833491  0.06963466 -0.04791842]\n",
      " [-0.01127287 -0.06324352 -0.03084805 -0.06167804  0.08821692]\n",
      " [ 0.09312951  0.00928626  0.06876919 -0.08506425  0.06069516]\n",
      " [ 0.08787874  0.0205936   0.0948018   0.04532849 -0.01488674]\n",
      " [-0.03325853 -0.08622959 -0.0134991  -0.03872734 -0.09511271]\n",
      " [-0.0680985  -0.09666372  0.05424742 -0.04578028 -0.03888563]\n",
      " [ 0.04834791  0.03663566 -0.00622357 -0.04070731 -0.04292046]\n",
      " [ 0.0653737   0.00956985 -0.03473657  0.0349347   0.03342534]\n",
      " [ 0.00670868 -0.06652486  0.0129615   0.0226926   0.0128647 ]\n",
      " [-0.09015199  0.08984455 -0.06030235 -0.01507311 -0.02846667]\n",
      " [-0.02322846  0.06090744 -0.04500072  0.07820896 -0.07967971]\n",
      " [ 0.01222918  0.02834683  0.06874315  0.09873541 -0.01471821]\n",
      " [-0.01560786  0.09304931 -0.045956    0.0520987   0.0001565 ]\n",
      " [ 0.09452294 -0.00753749 -0.08348075  0.05650357  0.0009572 ]\n",
      " [ 0.05601021  0.05026804  0.00700066  0.04024335 -0.05607857]\n",
      " [-0.02265737 -0.02170283 -0.07848036 -0.08169574 -0.05159614]\n",
      " [ 0.07695433  0.05707718  0.06511458 -0.09061955  0.03065578]\n",
      " [-0.0552653   0.06564931  0.09939339 -0.00037911  0.04107273]\n",
      " [-0.07845537 -0.00319906  0.06448117  0.04807265  0.00828258]\n",
      " [-0.04025855  0.09683561  0.07770777 -0.0225393   0.06943382]\n",
      " [ 0.07214555  0.03760307  0.05261369  0.02739615  0.05882797]\n",
      " [ 0.03517679 -0.06071718  0.09236851  0.03232721  0.03116744]\n",
      " [ 0.04626998 -0.06110999 -0.02364745  0.01205596 -0.07543983]\n",
      " [-0.09822867  0.00044234  0.05026206 -0.07711133 -0.09255776]\n",
      " [ 0.00169718  0.03399238 -0.04584499  0.03713773  0.06998029]\n",
      " [ 0.03454699 -0.0310432   0.04206235  0.06974338 -0.04868919]\n",
      " [ 0.02667446 -0.07592399 -0.094249    0.07732895 -0.08405826]\n",
      " [ 0.03697839 -0.08455934 -0.00760737 -0.01818506 -0.08633633]\n",
      " [-0.08733574 -0.00236382  0.00096881  0.07013037 -0.03732081]\n",
      " [-0.07851011 -0.09754091  0.03012871  0.06418077  0.09636804]]\n"
     ]
    }
   ],
   "source": [
    "# http://adventuresinmachinelearning.com/gensim-word2vec-tutorial/\n",
    "import numpy\n",
    "embedding_matrix = numpy.zeros((len(model.wv.vocab), 5))\n",
    "for i in range(len(model.wv.vocab)):\n",
    "    embedding_vector = model.wv[model.wv.index2word[i]]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EuclideanKeyedVectors' object has no attribute 'word2index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-231-38ba2f8e5761>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msentence_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msentence_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EuclideanKeyedVectors' object has no attribute 'word2index'"
     ]
    }
   ],
   "source": [
    "sentence_matrix = numpy.zeros((len(tokenized_sentences[0]), 5))\n",
    "for word in tokenized_sentences[0]:\n",
    "    sentence_matrix[i] = model.wv.word2index[word]\n",
    "    i = i + 1\n",
    "print(sentence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09923361,  0.06483728, -0.06977377, -0.06283475, -0.09162995],\n",
       "       [ 0.06060733, -0.00900702, -0.07504471,  0.05474842, -0.03298104],\n",
       "       [-0.01017982, -0.00342942, -0.02423411,  0.01104012,  0.08833295],\n",
       "       [ 0.09854638,  0.09075827,  0.03262121, -0.05288594, -0.08344082],\n",
       "       [ 0.05605929, -0.04426245, -0.01464995,  0.08522175, -0.00209202],\n",
       "       [-0.06084878, -0.01537897, -0.0671598 ,  0.06267373, -0.09987913],\n",
       "       [-0.09923361,  0.06483728, -0.06977377, -0.06283475, -0.09162995],\n",
       "       [ 0.01645874,  0.03998483, -0.03984243,  0.01195503, -0.09593436],\n",
       "       [-0.05285584,  0.01166624, -0.04143319,  0.03806416,  0.07841747],\n",
       "       [ 0.01958995,  0.00531222,  0.05089951,  0.08859974,  0.00560018],\n",
       "       [ 0.02172123, -0.06589881,  0.00193517,  0.02630652, -0.03498821],\n",
       "       [ 0.01906343, -0.04771835,  0.04810028,  0.05033817,  0.00966696],\n",
       "       [ 0.04541571,  0.07379074, -0.09286577,  0.06168452, -0.02174261],\n",
       "       [ 0.0807071 ,  0.07747361,  0.03114487,  0.08429039,  0.00411573],\n",
       "       [-0.01840948, -0.06675464,  0.02041076, -0.04131516,  0.08144502],\n",
       "       [-0.01486986, -0.09508315, -0.04994346,  0.03116045,  0.04777059],\n",
       "       [-0.04285311,  0.07851557, -0.02305727,  0.09014656,  0.00753869],\n",
       "       [ 0.01958995,  0.00531222,  0.05089951,  0.08859974,  0.00560018],\n",
       "       [ 0.07724369,  0.024476  ,  0.07140469, -0.0679525 ,  0.03048294],\n",
       "       [ 0.05605929, -0.04426245, -0.01464995,  0.08522175, -0.00209202],\n",
       "       [ 0.07739772, -0.03347112,  0.06010308,  0.08778124, -0.08893488],\n",
       "       [-0.03772506,  0.0996588 , -0.09982965, -0.02742348, -0.00129362],\n",
       "       [-0.05432634,  0.03884624,  0.07749817, -0.06356236,  0.06455674],\n",
       "       [-0.02605175,  0.05364309, -0.06292524,  0.02411835,  0.0935306 ],\n",
       "       [ 0.09847988, -0.0227323 ,  0.09269381, -0.02902534, -0.01298453],\n",
       "       [ 0.04451498,  0.07949726, -0.08244707, -0.03271058,  0.0575508 ],\n",
       "       [-0.09797613,  0.07278931, -0.07629403, -0.00116221,  0.01048501],\n",
       "       [-0.03725352,  0.01852884,  0.01375999,  0.07192751,  0.03508877],\n",
       "       [ 0.00145268, -0.03099545,  0.05368621,  0.01096387, -0.01429736],\n",
       "       [ 0.05952052, -0.09008217,  0.03319135, -0.00733409,  0.0420899 ],\n",
       "       [-0.00874011, -0.09565205, -0.03837397, -0.029537  ,  0.09283746],\n",
       "       [-0.09224488, -0.03248958,  0.00719275,  0.04691073, -0.0874926 ],\n",
       "       [-0.0789341 ,  0.0014308 ,  0.0226523 ,  0.09537727,  0.04658386],\n",
       "       [-0.09431996, -0.02370959, -0.03100465, -0.05839001,  0.06874202],\n",
       "       [ 0.07591475, -0.07993622, -0.05515864, -0.01907152, -0.07367617],\n",
       "       [-0.09837995, -0.08038234,  0.04700774,  0.07812591,  0.035399  ]], dtype=float32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.array(model.wv[tokenized_sentences[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "[  0.  13.   6.  14.   7.  15.   0.  16.  17.   4.  18.   8.  19.   9.  20.\n",
      "  10.  21.   4.  22.   7.  23.   3.  24.  25.  26.  27.  28.  29.   5.  30.\n",
      "  31.  32.  33.  34.  35.   2.]\n"
     ]
    }
   ],
   "source": [
    "print(len(model.wv.vocab))\n",
    "\n",
    "idx_sentence = numpy.zeros(len(tokenized_sentences[0]))\n",
    "i = 0\n",
    "for word in tokenized_sentences[0]\n",
    "    idx_sentence[i] = model.wv.vocab[word].index\n",
    "    i = i + 1\n",
    "print(idx_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07591475, -0.07993622, -0.05515864, -0.01907152, -0.07367617])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-ba2447bfcf42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(tokenized_sentences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_idx(tokenized_sentences):\n",
    "    idx_sentences = []\n",
    "    for tokenized_sentence in tokenized_sentences:\n",
    "        idx_one_sentence = numpy.zeros(len(tokenized_sentence))\n",
    "        idx = 0\n",
    "        for word in tokenized_sentence:\n",
    "            idx_one_sentence[idx] = model.wv.vocab[word].index\n",
    "            idx = idx + 1\n",
    "        idx_sentences.append(idx_one_sentence) \n",
    "    return idx_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  0.,  13.,   6.,  14.,   7.,  15.,   0.,  16.,  17.,   4.,  18.,\n",
      "         8.,  19.,   9.,  20.,  10.,  21.,   4.,  22.,   7.,  23.,   3.,\n",
      "        24.,  25.,  26.,  27.,  28.,  29.,   5.,  30.,  31.,  32.,  33.,\n",
      "        34.,  35.,   2.]), array([  0.,  36.,  37.,  38.,   1.,   8.,   0.,  39.,   1.,   0.,  40.,\n",
      "         9.,  41.,   6.,  42.,  43.,  10.,   3.,  44.,   1.,  45.,  46.,\n",
      "        47.,  48.,  49.,  50.,  51.,  52.,   4.,  53.,  54.,   1.,  55.,\n",
      "         2.,  56.,   4.,  57.,   2.]), array([ 58.,  59.,  60.,  61.,   3.,  62.,   1.,  63.,  11.,   3.,  12.,\n",
      "        64.,  65.,   5.,   3.,  12.,  66.,  67.,  68.,   7.,   0.,  69.,\n",
      "        11.,  70.,   0.,  71.,  72.,  73.,  74.,   0.,  75.,   5.,  76.,\n",
      "         5.,  77.,   1.,   0.,  78.,   2.]), array([ 79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,  88.,   1.,\n",
      "        89.,  90.,  91.,  92.,  93.,   1.,  94.,   2.]), array([ 95.,   0.,  96.,   6.,  97.,  98.,  99.,   2.])]\n"
     ]
    }
   ],
   "source": [
    "idx_sentences = sentence_to_idx(tokenized_sentences[0:5])\n",
    "print(idx_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for debugging the tokenizer script\n",
    "test = b'For a movie that gets no respect there sure are a lot of memorable quotes \\\n",
    "listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. \\\n",
    "The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt.'\n",
    "tokenizer_cmd = ['/Users/lifa08/Documents/Lifa/Machine_Learning/Miniproject/mosesdecoder-master/scripts/tokenizer/tokenizer.perl', '-l', 'en', '-q', '-']\n",
    "tokenizer = Popen(tokenizer_cmd, stdin=PIPE, stdout=PIPE)\n",
    "tok_text, _ = tokenizer.communicate(test)\n",
    "toks = tok_text.decode().split('\\n')[:-1]\n",
    "toks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
